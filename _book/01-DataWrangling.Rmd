# Everyday data wrangling

Suppose there exists a test dataset and the user is tasked with proving their proficiency in exploratory data analysis (EDA). Alternatively, the user may be handed a piece of data (e.g., maybe an Excel sheet) and is asked to run some preliminary analysis. Prior to jumping straight into EDA and statistical modeling, it's a good idea to transform the original data into a desirable format. This includes, but is not limited to, missing data imputation, scalar transformation, feature selection, group aggregation, and data filtering. The following notebook outlines the fundamentals of data wrangling, using practical base R and `tidyverse` solutions interchangeably. 

```{r, warning=FALSE, message=FALSE}
library(tidyverse)
```


## Renaming column headers

The first few steps in data analysis is to gain intuition around the data at hand - typically, a `m x n` dataset is structured in a way such that there are `m` records (also referred to as instances or samples) and `m` features (also referred to as predictors). Getting a good sense and understanding of the features we're working with often requires cleaning the data before jumping into things like feature selection and feature engineering. 

Suppose we have a `32 x 11` dataset `mtcars` where `rownames` correspond to the car model: 

```{r}
data(mtcars)
mtcars <- mtcars %>% as_tibble(rownames = 'CAR')
```

There are 11 columns here and since this is a toy dataset, it is already neat and tidy. However, sometimes we may need to rename the columns to trim specific strings or to make downstream analysis easier. Renaming column headers with `dplyr::rename()` is as simple as `rename(new_name = old_name)`, but with `rename_with()` there's more flexibility:

```{r}
mtcars %>% rename_with(toupper)
```

You can pass any function within `rename_with()` to alter the column headers and use the `.cols = ` argument to define which columns you'd apply the function to:

```{r}
mtcars %>% rename_with(function(x) paste0(x, "_1"), .cols = -1)
```

Alternatively with base R, you can call the `names` attribute and alter that directly:

```{r, eval=FALSE}
names(mtcars)[-1] <- paste0(names(mtcars[-1]), '_1') # not run
```


## Grouped operations

Grouped operations are most useful when you're working with data where there is a categorical variable. Grouped summaries are powerful tools easily done with `dplyr::summarise()`:

```{r}
mtcars %>% group_by(cyl) %>% summarise(avg_mpg = mean(mpg))
```

Using multiple columns at once and renaming the output columns:

```{r}
mtcars %>% group_by(cyl) %>% 
  summarise(across(c(mpg, disp, hp), mean, .names = "mean_{col}"))
```

```{r}
mtcars %>% group_by(cyl) %>% 
  summarise(across(where(is.numeric), mean, .names = "mean_{col}"))
```

Alternatively, in base R, you can use `aggregate()` with the `~`; you can read this as *grouping mpg by cyl* where *by* corresponds to the `~`:

```{r}
aggregate(mpg ~ cyl, data = mtcars, FUN = mean)
```

Since `aggregate()` uses the formula syntax (`~`), using the `.` placeholder on the LHS calls all variables not called in the RHS:

```{r, eval=FALSE}
aggregate(. ~ cyl, data = mtcars, FUN = mean)
```

When calculating the size of the groups, you can use `n()`:

```{r}
mtcars %>% group_by(cyl) %>% summarise(count = n())
```

The `n()` function is useful when filtering by groups of size greater than a certain value:

```{r}
mtcars %>% group_by(cyl) %>% filter(n() > 7) %>% ungroup()
```

Using `n()` gives us access to finding the group sizes using `tally()` or `count()`:

```{r}
mtcars %>% group_by(cyl) %>% tally()
```

We can label encode (i.e., give a unique integer identifier for the current groups) using `cur_group_id()`:

```{r}
mtcars %>% group_by(cyl) %>%
  mutate(ID = cur_group_id()) %>% select(ID, cyl)
```


## Data transformation

Sometimes it's useful to transform numeric columns; this is particularly helpful when performing feature transformations (e.g., logarithmic, absolute value) or creating new columns entirely from preexisting columns.

```{r}
mtcars %>% mutate(mpg = mpg*2)
```

Using `across()` allows you to specifically select which columns you'd like to apply a given function on. The first part of the `across()` argument should therefore specify the columns of interest. Here, `where()` is particularly useful as it returns indices (kind of like `grep()`) after evaluating a logical expression, such as `is.numeric()`. The second part of `across()` takes the function you'd like to apply, while the optional `.names` argument allows you to create new columns rather than overwrite the old ones.

```{r}
mtcars %>% 
  mutate(across(where(is.numeric), function(x) x*2, .names = "double_{col}"))
```

```{r}
mtcars %>%
  mutate(across(where(is.character),
                function(x) gsub('[a-z]$', '', x))) %>%
  select(CAR)
```

Conditional `mutate()` can also be done using `mutate_if()`, though the `_if()` functions, along with variants `_at()` and `_all()` have been effectively replaced by `across()`, as seen above. 

You can overwrite the existing columns completely; also keep in mind that you can pass any function within `across()` as such:

```{r}
mtcars %>% mutate(across(c(mpg, disp), function(x) x*3))
```

Iterating across the columns using `purrr::modify()` instead:

```{r}
mtcars %>% modify_if(is.numeric, ~ .x*3)
```

Mutating columns of characters is also straightforward; this is helpful when working with messy columns that require cleaning.

```{r}
mtcars %>% mutate(CAR = gsub('Merc', 'Mercedes', CAR))
```

```{r}
mtcars %>% mutate(CAR = gsub(' ', '_', CAR)) %>% mutate(across(CAR, tolower))
```


## Joining and separating character columns

For the `CAR` column comprised of strings, you can separate the individual strings into multiple columns; the `extra = 'merge'` argument tells the function to separate the string based on the first instance of " " and merge the rest:

```{r, warning=FALSE}
mtcars %>% separate(CAR, c('Brand', 'Model'), sep = " ", extra = 'merge')
```

Chaining multiple `tidyverse` verbs can be done using the pipe `%>%`; this `%>%` symbol is part of the `magrittr` package, which is loaded with `tidyverse`. Recently as of R version 4.1.0, the native pipe `|>` was introduced, which behaves in mostly a similar manner as the `magrittr` pipe (though there are a few important differences). Here, I will stick to the `%>%` notation. 

```{r, warning=FALSE}
mtcars %>% separate(CAR, c('Brand', 'Model'), sep = " ", extra = 'merge') %>%
  group_by(Brand) %>% summarise(count = n(), mean_mpg = mean(mpg)) %>%
  arrange(desc(count))
```

The pipe notation is frequently used between `tidyverse` verbs but you can also use it in some base R operations as well.

Separation of columns by strings, using base R instead involves using `strsplit()` which outputs a list and therefore we need to unpack it afterwards:

```{r, eval=FALSE}
mtcars$Brand <- unlist(
  lapply(strsplit(mtcars$CAR, split = ' '), function(x) x[1])
) # not run
mtcars$Model <- unlist(
  lapply(strsplit(mtcars$CAR, split = ' '), function(x) x[2])
) # not run
```

The opposite of `separate()` is `unite()` which combines columns into a single column:

```{r}
data(iris)
iris <- as_tibble(iris)
iris %>% unite('Petal.Dimensions', c(`Petal.Length`, `Petal.Width`),
               sep = " x ", remove = FALSE)
```

In base R, we create a new column using the `$` notation:

```{r, eval=FALSE}
iris$Petal.Dimensions <- paste(iris$Petal.Length, iris$Petal.Width, sep = " x ") # not run
```


## Filtering rows

Typically, filtering involves a condition based on a column to select the corresponding rows. Here I am combining `stringr::str_detect()` with `dplyr::filter()` since the column I am filtering on is a character vector.

```{r}
mtcars %>% filter(str_detect(CAR, 'Mazda'))
```

This function accepts logical operators and regex as well:

```{r}
mtcars %>% filter(str_detect(CAR, 'Mazda | Merc'))
```

Ignoring upper/lower case distinction using `regex(..., ignore_case = TRUE)`:

```{r}
mtcars %>% filter(str_detect(CAR, regex('mazda', ignore_case = TRUE)))
```

Alternatively, using base R using `grepl()`:

```{r, eval=FALSE}
mtcars[grepl('Mazda', mtcars$CAR),] # not run
mtcars[grepl('Mazda|Merc', mtcars$CAR),] # not run
```

Using `tolower()` to make sure we're on the same page in regards to case:

```{r}
mtcars[grepl(tolower('Mazda'), tolower(mtcars$CAR)),]
```

Filtering rows based on a numeric column is also trivial:

```{r}
mtcars %>% filter(between(mpg, 18, 20))
```

```{r}
mtcars %>% filter(cyl %in% c(6, 8))
```

A useful function is the negation of `%in%`, which we can define manually:

```{r}
`%nin%` <- Negate(`%in%`)

mtcars %>% filter(cyl %nin% c(6, 8))
```

In base R we can use `which()` instead to subset:

```{r, eval=FALSE}
mtcars[which(mtcars$mpg > 18 & mtcars$mpg < 20),] # not run
mtcars[which(mtcars$cyl %in% c(6,8)),] # not run
```

Base R function `subset()` also works:

```{r}
subset(mtcars, cyl == 6 & vs == 0)
```


## Subsetting columns based on strings

Subsetting columns typically involves using square brackets `[ ]` in base R, but can be done easily with `dplyr::select()`; `select()` supports multiple selector functions such as `starts_with()`, `ends_with()`, `contains()`, and `match()`. The selector `everything()` also exists, which is used to - you guessed it - select every column. 

As a word of caution: sometimes it's good practice to explicitly state `select()` we're calling is from `dplyr` (i.e., `dplyr::select()`) to avoid confusion when working with many different packages at once that could potentially introduce conflicts.

```{r}
mtcars %>% select(contains('m'))
```

You can use multiple helper functions at once:

```{r}
mtcars %>% select(starts_with('m'), ends_with('c'))
```

Using regex allows more flexibility in terms of pattern matching, as well as with anchors `^` and `$`:

```{r}
mtcars %>% select(matches('^m'), matches('c$'))
```

Alternatively, in base R and regex:

```{r}
mtcars[,grepl('^m|c$', names(mtcars))]
```

We can also define a vector containing the column names and use `select()` using the helper functions such as `any_of()` and `all_of()`. 

```{r}
my_cols <- c('mpg', 'cyl', 'am')
mtcars %>% select(any_of(my_cols))
```

Standard slicing and dicing works just as well:

```{r}
mtcars[,my_cols]
```

`any_of()` is particularly useful when the vector of strings contains columns that don't exist in the data. For example, if `my_cols` contained elements called `banana` and `apple`, running `mtcars[,my_cols]` will throw an error - you can't subset columns that don't exist. Using `any_of()` safely finds columns that do exist in `my_cols` and returns them safely.


## Long and wide data formats

Transforming datasets into long/wide formats is a typical task in exploratory data analysis and this can be done using `tidyr`:

```{r}
mtcars_long <- mtcars %>% pivot_longer(-1, names_to = 'Metric', values_to = 'Values')
mtcars_long
```

Having the dataset in this long format allows us to create visualizations such as the boxplot much easier. 

Pivoting the long format back to a wide format as also straightforward with `tidyr`:

```{r}
mtcars_wide <- mtcars_long %>% 
  pivot_wider(names_from = 'Metric', values_from = 'Values')
mtcars_wide
```

Having the data in a wide format may be useful when plotting visualizations such as a heatmap, where row and column annotations correspond to the heatmap perimeter. 

In base R, you can use `reshape()` with the argument `direction = ` but I don't find its syntax very intuitive. Alternatively, using `data.table` gets us the solution much easier:

```{r, warning=FALSE, message=FALSE}
library(data.table)
melt(setDT(mtcars), id.vars = c('CAR'), variable.name = 'Metric')
```

Note that using `data.table` returns a `data.table` object (due to the `setDT()` function), which differs from the `tibble` we've been using for packages within `tidyverse`. There are some advantages to using `data.table` instead, especially when working with very large datasets. For the sake of this book I will mostly use base R and `tidyverse`, other than exceptional cases. However, for a brief discourse on `data.table`, refer to Chapter 4. 


## Trimming strings

Trimming character columns, then re-encoding them as factors:

```{r}
iris %>% mutate(Species = strtrim(Species, 3)) %>%
  mutate(Species = factor(Species, levels = c('set', 'ver', 'vir')))
```

Sometimes when you're importing datasets from external and/or untrustworthy sources (e.g., an Excel sheet) it's worth checking for any whitespaces. In that case you can use `stringr::str_trim()` to remove all whitespaces prior to data analysis.

```{r}
iris %>% mutate(Species = str_trim(Species))
```


## Iterating over list of dataframes

Analogously to base R's `split()`, using `group_split()` allows us to convert the dataset to a list based on a column; the length of this output list would correspond to the unique number of column entries:

```{r}
mtcars <- as_tibble(mtcars)
mtcars_lst <- mtcars %>% group_split(cyl)
```

Iteration will be covered in more detail in a future chapter (see Chapter 3), but using base R's `apply` family is straightforward. In this case since we're working with lists, we will use `lapply()` and pass a function (in this case, `rename_with()`):

```{r}
mtcars_lst <- lapply(mtcars_lst, function(x) 
  rename_with(x, function(y) paste0(y, "_", as.character(unique(x$cyl))), .cols = -1))
mtcars_lst[[1]]
```

A `tidyverse` alternative to `apply()` is `purrr::map()` and its variations:

```{r, warning=FALSE}
mtcars_lst <- mtcars %>% group_split(cyl)
mtcars_lst <- mtcars_lst %>% 
  map(~ rename_with(.x, function(y) paste0(y, "_", as.character(unique(.x$cyl))), 
                    .cols = -1))
mtcars_lst[[1]]
```

Fitting a linear model is easy using iterations; in this case we fit `lm()` on the variables `mpg` and `gear` to identify their relationship:

```{r}
mtcars_lst <- mtcars %>% group_split(cyl)
mtcars_lst %>% 
  map(~ lm(mpg ~ gear, data = .x)) %>%
  map(coef)
```

Using `broom::tidy()` to clean up modelling result and output the model estimates:

```{r, warning=FALSE, message=FALSE}
library(broom)
mtcars_lst %>%
  map(~ lm(mpg ~ gear, data = .x)) %>%
  map(tidy) %>%
  bind_rows()
```

Using base R and the `apply()` family instead:

```{r}
models <- lapply(mtcars_lst, function(x) lm(mpg ~ gear, data = x))
coefs <- lapply(models, coef)
coefs[[1]]
```


## Rowwise operations

```{r}
df <- tibble(name = c('Brian', 'Connor'),
             coffee = sample(1:10, 2),
             wine = sample(1:5, 2),
             juice = sample(1:5, 2))
df
```

Sometimes it's useful to run calculations in a row-wise fashion; for example using `mutate()` with the helper function `c_across()`:

```{r}
df %>% rowwise() %>% mutate(total = sum(c_across(coffee:juice)),
                            avg = mean(c_across(coffee:juice)))
```

Calculating the proportions using a combination of row-wise and column-wise operations with `c_across()` and `across()`:

```{r}
df %>% rowwise() %>% mutate(total = sum(c_across(coffee:juice))) %>%
  ungroup() %>%
  mutate(across(coffee:juice, function(x) x/total,
                .names = "{col}_prop."))
```


## Conditional transformation

Conditional transformations are useful when creating categories based on a series of logical statements; this is done using `case_when()` to define the conditions:

```{r}
mtcars %>% mutate(mileage_class = 
  case_when(mpg > 20 ~ 'High',
            mpg < 20 ~ 'Low')) %>%
  relocate(mileage_class, .after = mpg)
```

In cases where we're working with character columns, it's useful to use `grepl` to match specific strings. After all, formula within `case_when()` needs to evaluate to a boolean on the left-hand side (e.g., `mpg > 20`). Additionally, in cases where the condition is not specified or met, you can use `TRUE ~ ` within `case_when()` as such:

```{r}
mtcars %>% mutate(CAR = case_when(grepl('Merc', CAR) ~ toupper(CAR), TRUE ~ CAR))
```

Conditional mutate using base R involves using `ifelse()`:

```{r}
mtcars$mileage_class <- ifelse(
  mtcars$mpg > 20, 'High', 'Low'
)
subset(mtcars, select = c(CAR, mpg, mileage_class, cyl:carb))
```


## Missing values

Handling missing values is tedious but often required when working with dodgy data.

First, for the sake of our exercise we insert NAs randomly in the `mtcars` dataset:

```{r}
mtcars_NA <- map_df(mtcars, function(x) 
  {x[sample(c(TRUE, NA), prob = c(0.95, 0.01), size = length(x), replace = TRUE)]})
mtcars_NA
```

Remove rows where any NA occurs:

```{r}
mtcars_NA %>% na.omit()
```

Identify columns with NAs and the number of occurrences:

```{r}
vapply(mtcars_NA, function(x) sum(is.na(x)), double(1))
```

Remove columns with more than one missing value: 

```{r}
mtcars_NA %>% select(where(function(x) sum(is.na(x)) < 1))
```

Replace missing values with zero using `tidyr::replace_na()`: 

```{r, eval = FALSE}
mtcars_NA %>% map_dfc(~ replace_na(.x, 0))
```

Base R and using `is.na()` instead:

```{r, eval=FALSE}
mtcars_NA[is.na(mtcars_NA)] <- 0 # not run
```


## Joining dataframes

Mutating joins in `tidyverse` are analogous to the inner and outer joins in SQL syntax:

```{r}
df1 <- tibble(
  name = c('Brian', 'Connor', 'Jon'),
  city = c('Tokyo', 'London', 'Milan'),
  age = c(28, 25, 21)
)
df2 <- tibble(
  name = c('Brian', 'Connor'),
  hair = c('black', 'brown'),
  eyes = c('dark', 'hazel')
)
```

```{r}
df1 %>% inner_join(df2, by = 'name')
```

```{r}
df1 %>% left_join(df2, by = 'name')
```

For a great visualization of the different join functions in `tidyverse`, check out the chapter on relational data in Hadley's book *R for Data Science* [here](https://r4ds.had.co.nz/relational-data.html). 

Base R uses `merge()` with the argument `all.x = ` and `all_y = `:

```{r}
merge(df1, df2, by = 'name')
merge(df1, df2, by = 'name', all.x = TRUE)
```


