<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 6 Everyday ML: Classification | Everyday-R</title>
  <meta name="description" content="<p>This is a minimal example of using the bookdown package to write a book.
The HTML output format for this example is bookdown::gitbook,
set in the _output.yml file.</p>" />
  <meta name="generator" content="bookdown 0.32.1 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 6 Everyday ML: Classification | Everyday-R" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="<p>This is a minimal example of using the bookdown package to write a book.
The HTML output format for this example is bookdown::gitbook,
set in the _output.yml file.</p>" />
  <meta name="github-repo" content="rstudio/bookdown-demo" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 6 Everyday ML: Classification | Everyday-R" />
  
  <meta name="twitter:description" content="<p>This is a minimal example of using the bookdown package to write a book.
The HTML output format for this example is bookdown::gitbook,
set in the _output.yml file.</p>" />
  

<meta name="author" content="by Brian Jungmin Park" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="everyday-exploratory-data-analysis.html"/>

<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Everyday-R by snowoflondon</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Introduction</a>
<ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#r-syntax-in-this-book"><i class="fa fa-check"></i><b>1.1</b> R syntax in this book</a></li>
<li class="chapter" data-level="1.2" data-path="index.html"><a href="index.html#r-packages-commonly-used-in-this-book"><i class="fa fa-check"></i><b>1.2</b> R packages commonly used in this book</a></li>
<li class="chapter" data-level="1.3" data-path="index.html"><a href="index.html#installing-r-packages"><i class="fa fa-check"></i><b>1.3</b> Installing R packages</a></li>
<li class="chapter" data-level="1.4" data-path="index.html"><a href="index.html#code-availability"><i class="fa fa-check"></i><b>1.4</b> Code availability</a></li>
<li class="chapter" data-level="1.5" data-path="index.html"><a href="index.html#website-hosting"><i class="fa fa-check"></i><b>1.5</b> Website hosting</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="everyday-data-wrangling.html"><a href="everyday-data-wrangling.html"><i class="fa fa-check"></i><b>2</b> Everyday data wrangling</a>
<ul>
<li class="chapter" data-level="2.1" data-path="everyday-data-wrangling.html"><a href="everyday-data-wrangling.html#renaming-column-headers"><i class="fa fa-check"></i><b>2.1</b> Renaming column headers</a></li>
<li class="chapter" data-level="2.2" data-path="everyday-data-wrangling.html"><a href="everyday-data-wrangling.html#grouped-operations"><i class="fa fa-check"></i><b>2.2</b> Grouped operations</a></li>
<li class="chapter" data-level="2.3" data-path="everyday-data-wrangling.html"><a href="everyday-data-wrangling.html#data-transformation"><i class="fa fa-check"></i><b>2.3</b> Data transformation</a></li>
<li class="chapter" data-level="2.4" data-path="everyday-data-wrangling.html"><a href="everyday-data-wrangling.html#joining-and-separating-character-columns"><i class="fa fa-check"></i><b>2.4</b> Joining and separating character columns</a></li>
<li class="chapter" data-level="2.5" data-path="everyday-data-wrangling.html"><a href="everyday-data-wrangling.html#filtering-rows"><i class="fa fa-check"></i><b>2.5</b> Filtering rows</a></li>
<li class="chapter" data-level="2.6" data-path="everyday-data-wrangling.html"><a href="everyday-data-wrangling.html#subsetting-columns-based-on-strings"><i class="fa fa-check"></i><b>2.6</b> Subsetting columns based on strings</a></li>
<li class="chapter" data-level="2.7" data-path="everyday-data-wrangling.html"><a href="everyday-data-wrangling.html#long-and-wide-data-formats"><i class="fa fa-check"></i><b>2.7</b> Long and wide data formats</a></li>
<li class="chapter" data-level="2.8" data-path="everyday-data-wrangling.html"><a href="everyday-data-wrangling.html#trimming-strings"><i class="fa fa-check"></i><b>2.8</b> Trimming strings</a></li>
<li class="chapter" data-level="2.9" data-path="everyday-data-wrangling.html"><a href="everyday-data-wrangling.html#iterating-over-list-of-dataframes"><i class="fa fa-check"></i><b>2.9</b> Iterating over list of dataframes</a></li>
<li class="chapter" data-level="2.10" data-path="everyday-data-wrangling.html"><a href="everyday-data-wrangling.html#rowwise-operations"><i class="fa fa-check"></i><b>2.10</b> Rowwise operations</a></li>
<li class="chapter" data-level="2.11" data-path="everyday-data-wrangling.html"><a href="everyday-data-wrangling.html#conditional-transformation"><i class="fa fa-check"></i><b>2.11</b> Conditional transformation</a></li>
<li class="chapter" data-level="2.12" data-path="everyday-data-wrangling.html"><a href="everyday-data-wrangling.html#missing-values"><i class="fa fa-check"></i><b>2.12</b> Missing values</a></li>
<li class="chapter" data-level="2.13" data-path="everyday-data-wrangling.html"><a href="everyday-data-wrangling.html#joining-dataframes"><i class="fa fa-check"></i><b>2.13</b> Joining dataframes</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="everyday-iterations.html"><a href="everyday-iterations.html"><i class="fa fa-check"></i><b>3</b> Everyday iterations</a>
<ul>
<li class="chapter" data-level="3.1" data-path="everyday-iterations.html"><a href="everyday-iterations.html#iterating-over-multiple-columns-using-apply-dplyr-and-purrr"><i class="fa fa-check"></i><b>3.1</b> Iterating over multiple columns using <code>apply</code>, <code>dplyr</code>, and <code>purrr</code></a></li>
<li class="chapter" data-level="3.2" data-path="everyday-iterations.html"><a href="everyday-iterations.html#iterating-over-lists"><i class="fa fa-check"></i><b>3.2</b> Iterating over lists</a></li>
<li class="chapter" data-level="3.3" data-path="everyday-iterations.html"><a href="everyday-iterations.html#iterating-over-vectors"><i class="fa fa-check"></i><b>3.3</b> Iterating over vectors</a></li>
<li class="chapter" data-level="3.4" data-path="everyday-iterations.html"><a href="everyday-iterations.html#iterating-with-two-inputs"><i class="fa fa-check"></i><b>3.4</b> Iterating with two inputs</a></li>
<li class="chapter" data-level="3.5" data-path="everyday-iterations.html"><a href="everyday-iterations.html#iterating-over-indices-and-names"><i class="fa fa-check"></i><b>3.5</b> Iterating over indices and names</a></li>
<li class="chapter" data-level="3.6" data-path="everyday-iterations.html"><a href="everyday-iterations.html#handling-errors-within-purrr"><i class="fa fa-check"></i><b>3.6</b> Handling errors within <code>purrr</code></a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="interlude-i-a-brief-glimpse-into-data.html"><a href="interlude-i-a-brief-glimpse-into-data.html"><i class="fa fa-check"></i><b>4</b> Interlude I: A brief glimpse into <code>data.table</code></a>
<ul>
<li class="chapter" data-level="4.1" data-path="interlude-i-a-brief-glimpse-into-data.html"><a href="interlude-i-a-brief-glimpse-into-data.html#data-wrangling-operations"><i class="fa fa-check"></i><b>4.1</b> Data wrangling operations</a></li>
<li class="chapter" data-level="4.2" data-path="interlude-i-a-brief-glimpse-into-data.html"><a href="interlude-i-a-brief-glimpse-into-data.html#sd-.sdcols-and"><i class="fa fa-check"></i><b>4.2</b> <code>.SD</code>, <code>.SDcols</code>, and <code>:=</code></a></li>
<li class="chapter" data-level="4.3" data-path="interlude-i-a-brief-glimpse-into-data.html"><a href="interlude-i-a-brief-glimpse-into-data.html#reshaping-data-using-melt-and-dcast"><i class="fa fa-check"></i><b>4.3</b> Reshaping data using <code>melt</code> and <code>dcast</code></a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="everyday-exploratory-data-analysis.html"><a href="everyday-exploratory-data-analysis.html"><i class="fa fa-check"></i><b>5</b> Everyday exploratory data analysis</a>
<ul>
<li class="chapter" data-level="5.1" data-path="everyday-exploratory-data-analysis.html"><a href="everyday-exploratory-data-analysis.html#workflow-1-continuous-data"><i class="fa fa-check"></i><b>5.1</b> Workflow 1: continuous data</a></li>
<li class="chapter" data-level="5.2" data-path="everyday-exploratory-data-analysis.html"><a href="everyday-exploratory-data-analysis.html#workflow-2-dates-and-ordinal-data"><i class="fa fa-check"></i><b>5.2</b> Workflow 2: dates and ordinal data</a></li>
<li class="chapter" data-level="5.3" data-path="everyday-exploratory-data-analysis.html"><a href="everyday-exploratory-data-analysis.html#visualization-of-clusters"><i class="fa fa-check"></i><b>5.3</b> Visualization of clusters</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="everyday-ml-classification.html"><a href="everyday-ml-classification.html"><i class="fa fa-check"></i><b>6</b> Everyday ML: Classification</a>
<ul>
<li class="chapter" data-level="6.1" data-path="everyday-ml-classification.html"><a href="everyday-ml-classification.html#model-training-and-predictions"><i class="fa fa-check"></i><b>6.1</b> Model training and predictions</a></li>
<li class="chapter" data-level="6.2" data-path="everyday-ml-classification.html"><a href="everyday-ml-classification.html#feature-selection-using-univariate-filters"><i class="fa fa-check"></i><b>6.2</b> Feature selection using univariate filters</a></li>
<li class="chapter" data-level="6.3" data-path="everyday-ml-classification.html"><a href="everyday-ml-classification.html#feature-selection-using-recursive-feature-elimination"><i class="fa fa-check"></i><b>6.3</b> Feature selection using recursive feature elimination</a></li>
<li class="chapter" data-level="6.4" data-path="everyday-ml-classification.html"><a href="everyday-ml-classification.html#hyperparameter-tuning"><i class="fa fa-check"></i><b>6.4</b> Hyperparameter tuning</a></li>
<li class="chapter" data-level="6.5" data-path="everyday-ml-classification.html"><a href="everyday-ml-classification.html#roc-and-precision-recall-curves"><i class="fa fa-check"></i><b>6.5</b> ROC and precision-recall curves</a></li>
<li class="chapter" data-level="6.6" data-path="everyday-ml-classification.html"><a href="everyday-ml-classification.html#model-comparisons"><i class="fa fa-check"></i><b>6.6</b> Model comparisons</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Everyday-R</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="everyday-ml-classification" class="section level1 hasAnchor" number="6">
<h1><span class="header-section-number">Chapter 6</span> Everyday ML: Classification<a href="everyday-ml-classification.html#everyday-ml-classification" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>In the preceeding chapters, I reviewed the fundamentals of wrangling data as well as running some exploratory data analysis to get a feel for the data at hand. In data science projects, it is often typical to frame problems in context of a model - how does a variable <em>y</em> behave according to some other variable <em>x</em>? For example, how does the pricing of a residential property behave according to the square footage? Is the relationship linear? Are there confounding variables that affect this relationship we have not accounted for?</p>
<p>In the simplest sense, fitting a linear model using ordinary least squares using <code>lm()</code> in R provide us with two parameters: the coefficient and the intercept. We can use these parameters to predict the housing price of a property based on the input <em>feature</em> - or <em>features</em> most likely - of that particular instance. This is the fundamental concept at the core of supervised learning. This example is a type of a <em>regression</em> as the target variable (i.e., the housing price) is a continuous variable. However, if the variable we were trying to predict is categorical (e.g., bins based on the bracket of housing price) the task would be <em>classification</em>.</p>
<p>The digression into concepts in ML and the details into each algorithm is beyond the scope of this book, but more details around specific topics are available on my <a href="https://brianjmpark.github.io/">blog</a> as well as documentation for popular ML packages such as Python’s <a href="https://scikit-learn.org/stable/">Scikit-Learn</a>.</p>
<p>In R, the workhorse of supervised learning models, whether it’s classification or regression, is the <code>caret</code> package.</p>
<div class="sourceCode" id="cb363"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb363-1"><a href="everyday-ml-classification.html#cb363-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tidyverse)</span>
<span id="cb363-2"><a href="everyday-ml-classification.html#cb363-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(caret)</span></code></pre></div>
<pre><code>## Loading required package: lattice</code></pre>
<pre><code>## 
## Attaching package: &#39;caret&#39;</code></pre>
<pre><code>## The following object is masked from &#39;package:purrr&#39;:
## 
##     lift</code></pre>
<div id="model-training-and-predictions" class="section level2 hasAnchor" number="6.1">
<h2><span class="header-section-number">6.1</span> Model training and predictions<a href="everyday-ml-classification.html#model-training-and-predictions" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>For the first exercise I will use a dataset from Kaggle <a href="https://www.kaggle.com/datasets/kkhandekar/all-datasets-for-practicing-ml">here</a> which I also uploaded onto my GitHub for reference:</p>
<div class="sourceCode" id="cb367"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb367-1"><a href="everyday-ml-classification.html#cb367-1" aria-hidden="true" tabindex="-1"></a>url <span class="ot">&lt;-</span> <span class="st">&#39;https://raw.githubusercontent.com/snowoflondon/everyday-r/main/datasets/Class_Winequality.csv&#39;</span></span>
<span id="cb367-2"><a href="everyday-ml-classification.html#cb367-2" aria-hidden="true" tabindex="-1"></a>df <span class="ot">&lt;-</span> <span class="fu">read_delim</span>(url, <span class="at">delim =</span> <span class="st">&#39;;&#39;</span>)</span></code></pre></div>
<pre><code>## 
## ── Column specification ──────────────────────────────────────
## cols(
##   `fixed acidity` = col_double(),
##   `volatile acidity` = col_double(),
##   `citric acid` = col_double(),
##   `residual sugar` = col_double(),
##   chlorides = col_double(),
##   `free sulfur dioxide` = col_double(),
##   `total sulfur dioxide` = col_double(),
##   density = col_double(),
##   pH = col_double(),
##   sulphates = col_double(),
##   alcohol = col_double(),
##   quality = col_double()
## )</code></pre>
<div class="sourceCode" id="cb369"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb369-1"><a href="everyday-ml-classification.html#cb369-1" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(df)</span></code></pre></div>
<pre><code>## # A tibble: 6 × 12
##   fixed acid…¹ volat…² citri…³ resid…⁴ chlor…⁵ free …⁶ total…⁷
##          &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;
## 1          7      0.27    0.36    20.7   0.045      45     170
## 2          6.3    0.3     0.34     1.6   0.049      14     132
## 3          8.1    0.28    0.4      6.9   0.05       30      97
## 4          7.2    0.23    0.32     8.5   0.058      47     186
## 5          7.2    0.23    0.32     8.5   0.058      47     186
## 6          8.1    0.28    0.4      6.9   0.05       30      97
## # … with 5 more variables: density &lt;dbl&gt;, pH &lt;dbl&gt;,
## #   sulphates &lt;dbl&gt;, alcohol &lt;dbl&gt;, quality &lt;dbl&gt;, and
## #   abbreviated variable names ¹​`fixed acidity`,
## #   ²​`volatile acidity`, ³​`citric acid`, ⁴​`residual sugar`,
## #   ⁵​chlorides, ⁶​`free sulfur dioxide`,
## #   ⁷​`total sulfur dioxide`</code></pre>
<p>This dataset has 11 features and a target label column called <code>quality</code>. Firstly, I convert the <code>quality</code> column into factors to reiterate the fact that we are working with a categorical column with defined levels.</p>
<div class="sourceCode" id="cb371"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb371-1"><a href="everyday-ml-classification.html#cb371-1" aria-hidden="true" tabindex="-1"></a>df <span class="ot">&lt;-</span> df <span class="sc">%&gt;%</span> <span class="fu">mutate</span>(<span class="at">quality =</span> <span class="fu">factor</span>(quality)) <span class="sc">%&gt;%</span> <span class="fu">relocate</span>(quality)</span></code></pre></div>
<p>A glimpse into the 11 features shows us that the values are heterogenous in scale:</p>
<div class="sourceCode" id="cb372"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb372-1"><a href="everyday-ml-classification.html#cb372-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(df <span class="sc">%&gt;%</span> <span class="fu">select</span>(<span class="sc">-</span>quality))</span></code></pre></div>
<pre><code>##  fixed acidity    volatile acidity  citric acid    
##  Min.   : 3.800   Min.   :0.0800   Min.   :0.0000  
##  1st Qu.: 6.300   1st Qu.:0.2100   1st Qu.:0.2700  
##  Median : 6.800   Median :0.2600   Median :0.3200  
##  Mean   : 6.855   Mean   :0.2782   Mean   :0.3342  
##  3rd Qu.: 7.300   3rd Qu.:0.3200   3rd Qu.:0.3900  
##  Max.   :14.200   Max.   :1.1000   Max.   :1.6600  
##  residual sugar     chlorides       free sulfur dioxide
##  Min.   : 0.600   Min.   :0.00900   Min.   :  2.00     
##  1st Qu.: 1.700   1st Qu.:0.03600   1st Qu.: 23.00     
##  Median : 5.200   Median :0.04300   Median : 34.00     
##  Mean   : 6.391   Mean   :0.04577   Mean   : 35.31     
##  3rd Qu.: 9.900   3rd Qu.:0.05000   3rd Qu.: 46.00     
##  Max.   :65.800   Max.   :0.34600   Max.   :289.00     
##  total sulfur dioxide    density             pH       
##  Min.   :  9.0        Min.   :0.9871   Min.   :2.720  
##  1st Qu.:108.0        1st Qu.:0.9917   1st Qu.:3.090  
##  Median :134.0        Median :0.9937   Median :3.180  
##  Mean   :138.4        Mean   :0.9940   Mean   :3.188  
##  3rd Qu.:167.0        3rd Qu.:0.9961   3rd Qu.:3.280  
##  Max.   :440.0        Max.   :1.0390   Max.   :3.820  
##    sulphates         alcohol     
##  Min.   :0.2200   Min.   : 8.00  
##  1st Qu.:0.4100   1st Qu.: 9.50  
##  Median :0.4700   Median :10.40  
##  Mean   :0.4898   Mean   :10.51  
##  3rd Qu.:0.5500   3rd Qu.:11.40  
##  Max.   :1.0800   Max.   :14.20</code></pre>
<p>For a quick exploratory analysis, take a look at the distribution of the features and their scales (i.e., y-axis). Typically in ML tasks, the scales need to be preprocessed prior to model training. This isn’t necessarily the case in models like the random forest, for example, but it is good practice regardless. I will circle back to this in a bit.</p>
<div class="sourceCode" id="cb374"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb374-1"><a href="everyday-ml-classification.html#cb374-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(RColorBrewer)</span>
<span id="cb374-2"><a href="everyday-ml-classification.html#cb374-2" aria-hidden="true" tabindex="-1"></a>dfm <span class="ot">&lt;-</span> df <span class="sc">%&gt;%</span> <span class="fu">pivot_longer</span>(<span class="sc">-</span>quality, <span class="at">names_to =</span> <span class="st">&#39;feature&#39;</span>, </span>
<span id="cb374-3"><a href="everyday-ml-classification.html#cb374-3" aria-hidden="true" tabindex="-1"></a>               <span class="at">values_to =</span> <span class="st">&#39;values&#39;</span>)</span>
<span id="cb374-4"><a href="everyday-ml-classification.html#cb374-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb374-5"><a href="everyday-ml-classification.html#cb374-5" aria-hidden="true" tabindex="-1"></a>dfm <span class="sc">%&gt;%</span> <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> quality, <span class="at">y =</span> values)) <span class="sc">+</span> </span>
<span id="cb374-6"><a href="everyday-ml-classification.html#cb374-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_boxplot</span>(<span class="fu">aes</span>(<span class="at">fill =</span> quality), <span class="at">alpha =</span> .<span class="dv">6</span>) <span class="sc">+</span> </span>
<span id="cb374-7"><a href="everyday-ml-classification.html#cb374-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">facet_wrap</span>(<span class="sc">~</span> feature, <span class="at">scales =</span> <span class="st">&#39;free&#39;</span>) <span class="sc">+</span> <span class="fu">theme_bw</span>() <span class="sc">+</span> </span>
<span id="cb374-8"><a href="everyday-ml-classification.html#cb374-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">legend.position =</span> <span class="st">&#39;none&#39;</span>) <span class="sc">+</span> </span>
<span id="cb374-9"><a href="everyday-ml-classification.html#cb374-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_fill_brewer</span>(<span class="at">palette =</span> <span class="st">&#39;Set1&#39;</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-190-1.png" width="672" /></p>
<p>Before doing any kind of preprocessing or normalization, it is imperative to split the data into training and testing to prevent information leak. The <code>createDataPartition()</code> function accepts the <code>p =</code> argument which defines the split fraction. Here I use 80/20 split.</p>
<div class="sourceCode" id="cb375"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb375-1"><a href="everyday-ml-classification.html#cb375-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">42</span>)</span>
<span id="cb375-2"><a href="everyday-ml-classification.html#cb375-2" aria-hidden="true" tabindex="-1"></a>df <span class="ot">&lt;-</span> df <span class="sc">%&gt;%</span> <span class="fu">mutate</span>(<span class="at">quality =</span> <span class="fu">paste0</span>(<span class="st">&#39;Group_&#39;</span>, quality)) <span class="sc">%&gt;%</span></span>
<span id="cb375-3"><a href="everyday-ml-classification.html#cb375-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">quality =</span> <span class="fu">factor</span>(quality))</span>
<span id="cb375-4"><a href="everyday-ml-classification.html#cb375-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb375-5"><a href="everyday-ml-classification.html#cb375-5" aria-hidden="true" tabindex="-1"></a>idx <span class="ot">&lt;-</span> <span class="fu">createDataPartition</span>(<span class="at">y =</span> df<span class="sc">$</span>quality, <span class="at">p =</span> .<span class="dv">8</span>,</span>
<span id="cb375-6"><a href="everyday-ml-classification.html#cb375-6" aria-hidden="true" tabindex="-1"></a>                           <span class="at">list =</span> <span class="cn">FALSE</span>, <span class="at">times =</span> <span class="dv">1</span>)</span>
<span id="cb375-7"><a href="everyday-ml-classification.html#cb375-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb375-8"><a href="everyday-ml-classification.html#cb375-8" aria-hidden="true" tabindex="-1"></a>df_train <span class="ot">&lt;-</span> df[idx,]</span>
<span id="cb375-9"><a href="everyday-ml-classification.html#cb375-9" aria-hidden="true" tabindex="-1"></a>df_test <span class="ot">&lt;-</span> df[<span class="sc">-</span>idx,]</span></code></pre></div>
<p>The <code>createDataPartition()</code> outputs an array of indices which can be used to split the original data.</p>
<p>Going back to the talk of scaling and preprocessing the data: a common procedure is to <code>center</code> and <code>scale</code>, that is - subtract the mean and divide by the standard deviation. If you’re familiar with <code>scikit-learn</code> in Python, this is analogous to running <code>StandardScaler()</code>.</p>
<div class="sourceCode" id="cb376"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb376-1"><a href="everyday-ml-classification.html#cb376-1" aria-hidden="true" tabindex="-1"></a>preProcObj <span class="ot">&lt;-</span> <span class="fu">preProcess</span>(df_train, <span class="at">method =</span> <span class="fu">c</span>(<span class="st">&#39;center&#39;</span>, <span class="st">&#39;scale&#39;</span>))</span>
<span id="cb376-2"><a href="everyday-ml-classification.html#cb376-2" aria-hidden="true" tabindex="-1"></a>preProcObj</span></code></pre></div>
<pre><code>## Created from 3920 samples and 12 variables
## 
## Pre-processing:
##   - centered (11)
##   - ignored (1)
##   - scaled (11)</code></pre>
<p>Evidently, the <code>preProcess()</code> function recognized the column containing the target labels and ignored it for preprocessing.</p>
<p>Preprocessing is done on the training data and the learned object is applied to both the training and testing data:</p>
<div class="sourceCode" id="cb378"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb378-1"><a href="everyday-ml-classification.html#cb378-1" aria-hidden="true" tabindex="-1"></a>df_train <span class="ot">&lt;-</span> <span class="fu">predict</span>(preProcObj, df_train)</span>
<span id="cb378-2"><a href="everyday-ml-classification.html#cb378-2" aria-hidden="true" tabindex="-1"></a>df_test <span class="ot">&lt;-</span> <span class="fu">predict</span>(preProcObj, df_test)</span></code></pre></div>
<p>Revisiting the features now shows the effect of the preprocessing step:</p>
<div class="sourceCode" id="cb379"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb379-1"><a href="everyday-ml-classification.html#cb379-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(df_train <span class="sc">%&gt;%</span> <span class="fu">select</span>(<span class="sc">-</span>quality))</span></code></pre></div>
<pre><code>##  fixed acidity      volatile acidity   citric acid     
##  Min.   :-3.63462   Min.   :-1.9812   Min.   :-2.7409  
##  1st Qu.:-0.66724   1st Qu.:-0.6815   1st Qu.:-0.5247  
##  Median :-0.07376   Median :-0.1816   Median :-0.1143  
##  Mean   : 0.00000   Mean   : 0.0000   Mean   : 0.0000  
##  3rd Qu.: 0.63841   3rd Qu.: 0.4182   3rd Qu.: 0.4602  
##  Max.   : 8.70968   Max.   : 7.2663   Max.   :10.8844  
##  residual sugar      chlorides       free sulfur dioxide
##  Min.   :-1.1408   Min.   :-1.6600   Min.   :-1.89691   
##  1st Qu.:-0.9250   1st Qu.:-0.4457   1st Qu.:-0.72325   
##  Median :-0.2384   Median :-0.1309   Median :-0.07773   
##  Mean   : 0.0000   Mean   : 0.0000   Mean   : 0.00000   
##  3rd Qu.: 0.6835   3rd Qu.: 0.1840   3rd Qu.: 0.62647   
##  Max.   :11.6490   Max.   :13.4966   Max.   :14.88647   
##  total sulfur dioxide    density              pH          
##  Min.   :-3.0417      Min.   :-2.2943   Min.   :-2.99893  
##  1st Qu.:-0.6983      1st Qu.:-0.7665   1st Qu.:-0.65873  
##  Median :-0.1065      Median :-0.1122   Median :-0.05697  
##  Mean   : 0.0000      Mean   : 0.0000   Mean   : 0.00000  
##  3rd Qu.: 0.6746      3rd Qu.: 0.7082   3rd Qu.: 0.61166  
##  Max.   : 7.1367      Max.   :14.9270   Max.   : 4.22224  
##    sulphates          alcohol        
##  Min.   :-2.4096   Min.   :-2.05259  
##  1st Qu.:-0.7084   1st Qu.:-0.83024  
##  Median :-0.1711   Median :-0.09683  
##  Mean   : 0.0000   Mean   : 0.00000  
##  3rd Qu.: 0.5452   3rd Qu.: 0.71807  
##  Max.   : 4.4850   Max.   : 2.99978</code></pre>
<p>The scales have been normalized, as evident here:</p>
<div class="sourceCode" id="cb381"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb381-1"><a href="everyday-ml-classification.html#cb381-1" aria-hidden="true" tabindex="-1"></a>df_train <span class="sc">%&gt;%</span> <span class="fu">pivot_longer</span>(<span class="sc">-</span>quality, <span class="at">names_to =</span> <span class="st">&#39;feature&#39;</span>,</span>
<span id="cb381-2"><a href="everyday-ml-classification.html#cb381-2" aria-hidden="true" tabindex="-1"></a>                          <span class="at">values_to =</span> <span class="st">&#39;values&#39;</span>) <span class="sc">%&gt;%</span></span>
<span id="cb381-3"><a href="everyday-ml-classification.html#cb381-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> quality, <span class="at">y =</span> values)) <span class="sc">+</span></span>
<span id="cb381-4"><a href="everyday-ml-classification.html#cb381-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_boxplot</span>(<span class="fu">aes</span>(<span class="at">fill =</span> quality), <span class="at">alpha =</span> .<span class="dv">6</span>) <span class="sc">+</span></span>
<span id="cb381-5"><a href="everyday-ml-classification.html#cb381-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">facet_wrap</span>(<span class="sc">~</span> feature, <span class="at">scales =</span> <span class="st">&#39;free&#39;</span>) <span class="sc">+</span> <span class="fu">theme_bw</span>() <span class="sc">+</span></span>
<span id="cb381-6"><a href="everyday-ml-classification.html#cb381-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">legend.position =</span> <span class="st">&#39;none&#39;</span>) <span class="sc">+</span> </span>
<span id="cb381-7"><a href="everyday-ml-classification.html#cb381-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_fill_brewer</span>(<span class="at">palette =</span> <span class="st">&#39;Set1&#39;</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-195-1.png" width="672" /></p>
<p>Once we’re ready to train the model, an important function is <code>trainControl()</code>. Here, typically we define the sampling method for the model training. I am using <code>method = cv</code> with <code>number = 5</code> for k-fold cross-validation with 5 folds. Alternatively, I could use <code>method = repeatedcv</code> with <code>number = 5</code> and <code>repeats = 5</code> for repeated cross-validation with 5 iterations, but for this exercise I will settle with the simple 5-fold cross validation.</p>
<div class="sourceCode" id="cb382"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb382-1"><a href="everyday-ml-classification.html#cb382-1" aria-hidden="true" tabindex="-1"></a>tr <span class="ot">&lt;-</span> <span class="fu">trainControl</span>(<span class="at">method =</span> <span class="st">&#39;cv&#39;</span>,</span>
<span id="cb382-2"><a href="everyday-ml-classification.html#cb382-2" aria-hidden="true" tabindex="-1"></a>                   <span class="at">number =</span> <span class="dv">5</span>,</span>
<span id="cb382-3"><a href="everyday-ml-classification.html#cb382-3" aria-hidden="true" tabindex="-1"></a>                   <span class="at">classProbs =</span> <span class="cn">TRUE</span>)</span>
<span id="cb382-4"><a href="everyday-ml-classification.html#cb382-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb382-5"><a href="everyday-ml-classification.html#cb382-5" aria-hidden="true" tabindex="-1"></a>model <span class="ot">&lt;-</span> <span class="fu">train</span>(quality <span class="sc">~</span> ., <span class="at">data =</span> df_train,</span>
<span id="cb382-6"><a href="everyday-ml-classification.html#cb382-6" aria-hidden="true" tabindex="-1"></a>               <span class="at">method =</span> <span class="st">&#39;ranger&#39;</span>, <span class="at">importance =</span> <span class="st">&#39;impurity&#39;</span>,</span>
<span id="cb382-7"><a href="everyday-ml-classification.html#cb382-7" aria-hidden="true" tabindex="-1"></a>               <span class="at">trControl =</span> tr)</span></code></pre></div>
<p>Above, I defined <code>method = ranger</code> within <code>train()</code>, which is a wrapper for training a random forest model. For all available methods for <code>train()</code>, see <code>caret</code>’s documentation <a href="https://topepo.github.io/caret/train-models-by-tag.html">here</a>. The <code>importance = 'impurity'</code> asks the model to use the Gini impurity method to rank variable importance. This will be useful later.</p>
<p>Calling the model object summarizes the model’s performance on the validation set (i.e., hold-out sets during k-fold cross validation).</p>
<div class="sourceCode" id="cb383"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb383-1"><a href="everyday-ml-classification.html#cb383-1" aria-hidden="true" tabindex="-1"></a>model</span></code></pre></div>
<pre><code>## Random Forest 
## 
## 3920 samples
##   11 predictor
##    7 classes: &#39;Group_3&#39;, &#39;Group_4&#39;, &#39;Group_5&#39;, &#39;Group_6&#39;, &#39;Group_7&#39;, &#39;Group_8&#39;, &#39;Group_9&#39; 
## 
## No pre-processing
## Resampling: Cross-Validated (5 fold) 
## Summary of sample sizes: 3136, 3135, 3136, 3137, 3136 
## Resampling results across tuning parameters:
## 
##   mtry  splitrule   Accuracy   Kappa    
##    2    gini        0.6670799  0.4785271
##    2    extratrees  0.6691211  0.4770672
##    6    gini        0.6578979  0.4671397
##    6    extratrees  0.6668264  0.4778833
##   11    gini        0.6571260  0.4668996
##   11    extratrees  0.6647859  0.4756944
## 
## Tuning parameter &#39;min.node.size&#39; was held constant at a
##  value of 1
## Accuracy was used to select the optimal model using
##  the largest value.
## The final values used for the model were mtry = 2,
##  splitrule = extratrees and min.node.size = 1.</code></pre>
<p>Various hyperparametes were tested and the combination with the highest validation accuracy was chosen:</p>
<div class="sourceCode" id="cb385"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb385-1"><a href="everyday-ml-classification.html#cb385-1" aria-hidden="true" tabindex="-1"></a>model<span class="sc">$</span>bestTune</span></code></pre></div>
<pre><code>##   mtry  splitrule min.node.size
## 2    2 extratrees             1</code></pre>
<p>The performance on the resamples during the cross validation process can be found here:</p>
<div class="sourceCode" id="cb387"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb387-1"><a href="everyday-ml-classification.html#cb387-1" aria-hidden="true" tabindex="-1"></a>model<span class="sc">$</span>resample</span></code></pre></div>
<pre><code>##    Accuracy     Kappa Resample
## 1 0.6530612 0.4487323    Fold1
## 2 0.6811224 0.4989916    Fold3
## 3 0.6942675 0.5131632    Fold2
## 4 0.6683673 0.4796363    Fold5
## 5 0.6487867 0.4448126    Fold4</code></pre>
<p>The testing dataset has not been touched at all during model training. For model evaluation, above model is tested on this hold-out set using <code>predict()</code>:</p>
<div class="sourceCode" id="cb389"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb389-1"><a href="everyday-ml-classification.html#cb389-1" aria-hidden="true" tabindex="-1"></a>pred <span class="ot">&lt;-</span> <span class="fu">predict</span>(model, df_test)</span></code></pre></div>
<p>For a clean summary of model evaluation, use <code>confusionMatrix()</code>:</p>
<div class="sourceCode" id="cb390"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb390-1"><a href="everyday-ml-classification.html#cb390-1" aria-hidden="true" tabindex="-1"></a><span class="fu">confusionMatrix</span>(<span class="at">data =</span> pred, <span class="at">reference =</span> df_test<span class="sc">$</span>quality, </span>
<span id="cb390-2"><a href="everyday-ml-classification.html#cb390-2" aria-hidden="true" tabindex="-1"></a>                <span class="at">mode =</span> <span class="st">&#39;prec_recall&#39;</span>)</span></code></pre></div>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction Group_3 Group_4 Group_5 Group_6 Group_7 Group_8
##    Group_3       0       0       0       0       0       0
##    Group_4       0       7       1       0       0       0
##    Group_5       3      14     217      55       3       1
##    Group_6       1      11      71     360      82      15
##    Group_7       0       0       2      23      90       7
##    Group_8       0       0       0       1       1      12
##    Group_9       0       0       0       0       0       0
##           Reference
## Prediction Group_9
##    Group_3       0
##    Group_4       0
##    Group_5       0
##    Group_6       1
##    Group_7       0
##    Group_8       0
##    Group_9       0
## 
## Overall Statistics
##                                         
##                Accuracy : 0.7014        
##                  95% CI : (0.6717, 0.73)
##     No Information Rate : 0.4489        
##     P-Value [Acc &gt; NIR] : &lt; 2.2e-16     
##                                         
##                   Kappa : 0.533         
##                                         
##  Mcnemar&#39;s Test P-Value : NA            
## 
## Statistics by Class:
## 
##                      Class: Group_3 Class: Group_4
## Precision                        NA       0.875000
## Recall                      0.00000       0.218750
## F1                               NA       0.350000
## Prevalence                  0.00409       0.032720
## Detection Rate              0.00000       0.007157
## Detection Prevalence        0.00000       0.008180
## Balanced Accuracy           0.50000       0.608846
##                      Class: Group_5 Class: Group_6
## Precision                    0.7406         0.6654
## Recall                       0.7457         0.8200
## F1                           0.7432         0.7347
## Prevalence                   0.2975         0.4489
## Detection Rate               0.2219         0.3681
## Detection Prevalence         0.2996         0.5532
## Balanced Accuracy            0.8175         0.7421
##                      Class: Group_7 Class: Group_8
## Precision                   0.73770        0.85714
## Recall                      0.51136        0.34286
## F1                          0.60403        0.48980
## Prevalence                  0.17996        0.03579
## Detection Rate              0.09202        0.01227
## Detection Prevalence        0.12474        0.01431
## Balanced Accuracy           0.73573        0.67037
##                      Class: Group_9
## Precision                        NA
## Recall                     0.000000
## F1                               NA
## Prevalence                 0.001022
## Detection Rate             0.000000
## Detection Prevalence       0.000000
## Balanced Accuracy          0.500000</code></pre>
<p>Certain models such as the random forest have built-in feature importance. During model training, I defined <code>importance = 'impurity'</code>, which means that the feature importance is calculated using the mean decrease in impurity after permutation of a given feature. Accessing this information is useful when we want to know which variables have the greatest influence on model performance and conversely, which ones have the least.</p>
<div class="sourceCode" id="cb392"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb392-1"><a href="everyday-ml-classification.html#cb392-1" aria-hidden="true" tabindex="-1"></a><span class="fu">varImp</span>(model)<span class="sc">$</span>importance</span></code></pre></div>
<pre><code>##                           Overall
## `fixed acidity`          0.000000
## `volatile acidity`      35.165026
## `citric acid`            8.761852
## `residual sugar`        11.579565
## chlorides                4.090038
## `free sulfur dioxide`   14.832990
## `total sulfur dioxide`  12.411664
## density                 38.947736
## pH                       7.484357
## sulphates                2.801255
## alcohol                100.000000</code></pre>
<p>The variable importance score is automatically scaled so that the highest score is set to 100. This can be turned off using <code>scale = FALSE</code> within <code>varImp()</code>.</p>
<p>A quick visualization of variable importance is useful:</p>
<div class="sourceCode" id="cb394"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb394-1"><a href="everyday-ml-classification.html#cb394-1" aria-hidden="true" tabindex="-1"></a>df_imp <span class="ot">&lt;-</span> <span class="fu">varImp</span>(model)<span class="sc">$</span>importance <span class="sc">%&gt;%</span> </span>
<span id="cb394-2"><a href="everyday-ml-classification.html#cb394-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">rownames_to_column</span>(<span class="at">var =</span> <span class="st">&#39;Var&#39;</span>) <span class="sc">%&gt;%</span></span>
<span id="cb394-3"><a href="everyday-ml-classification.html#cb394-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">as_tibble</span>() <span class="sc">%&gt;%</span> <span class="fu">arrange</span>(<span class="fu">desc</span>(Overall))</span>
<span id="cb394-4"><a href="everyday-ml-classification.html#cb394-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb394-5"><a href="everyday-ml-classification.html#cb394-5" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(df_imp, <span class="fu">aes</span>(<span class="at">x =</span> <span class="fu">reorder</span>(Var, Overall), <span class="at">y =</span> Overall)) <span class="sc">+</span> </span>
<span id="cb394-6"><a href="everyday-ml-classification.html#cb394-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">stat =</span> <span class="st">&#39;identity&#39;</span>, <span class="at">color =</span> <span class="st">&#39;red&#39;</span>) <span class="sc">+</span> </span>
<span id="cb394-7"><a href="everyday-ml-classification.html#cb394-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_segment</span>(<span class="fu">aes</span>(<span class="at">x =</span> <span class="fu">reorder</span>(Var, Overall), </span>
<span id="cb394-8"><a href="everyday-ml-classification.html#cb394-8" aria-hidden="true" tabindex="-1"></a>                   <span class="at">xend =</span> <span class="fu">reorder</span>(Var, Overall),</span>
<span id="cb394-9"><a href="everyday-ml-classification.html#cb394-9" aria-hidden="true" tabindex="-1"></a>                   <span class="at">y =</span> <span class="dv">0</span>,</span>
<span id="cb394-10"><a href="everyday-ml-classification.html#cb394-10" aria-hidden="true" tabindex="-1"></a>                   <span class="at">yend =</span> Overall)) <span class="sc">+</span> </span>
<span id="cb394-11"><a href="everyday-ml-classification.html#cb394-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_classic</span>() <span class="sc">+</span> <span class="fu">coord_flip</span>() <span class="sc">+</span> <span class="fu">xlab</span>(<span class="st">&#39;&#39;</span>) <span class="sc">+</span> <span class="fu">ylab</span>(<span class="st">&#39;Var. Imp.&#39;</span>) <span class="sc">+</span></span>
<span id="cb394-12"><a href="everyday-ml-classification.html#cb394-12" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">text =</span> <span class="fu">element_text</span>(<span class="at">size =</span> <span class="dv">14</span>))</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-203-1.png" width="672" /></p>
</div>
<div id="feature-selection-using-univariate-filters" class="section level2 hasAnchor" number="6.2">
<h2><span class="header-section-number">6.2</span> Feature selection using univariate filters<a href="everyday-ml-classification.html#feature-selection-using-univariate-filters" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>When the dataset is considerably larger, the number of <em>n</em> features may grow extremely large. In these scenarios, it may be advisable to reduce the number of features to save computation time but also to reduce model complexity.</p>
<p>Of course, dimensionality reduction is possible, though this transforms the data and the original meaning of the features is lost. An alternative method is <em>feature selection</em> - selecting important features and discarding unimportant ones. This relates specifically to the concept of feature importance in the previous section.</p>
<p><code>caret</code> offers a simple way to rank features when built-in feature importance measures are not available. This is by using univariate filters, which are essentially fitting <em>n</em> individual models (where <em>n</em> is the number of features) against the target label and ranking them based on their statistical significance.</p>
<p><code>anoveScores()</code> is used for classification models and fits an ANOVA for each feature against the label. The null hypothesis here assumes the mean values for each feature is equal for all labels. <code>gamScores()</code> is used for regression models and uses a generalized additive model to look for functional relationships between the features and the label. In both cases, each feature in the predictor set is passed individually.</p>
<p>For this part I will use the <code>Sonar</code> dataset from <code>mlbench</code>.</p>
<div class="sourceCode" id="cb395"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb395-1"><a href="everyday-ml-classification.html#cb395-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(mlbench)</span>
<span id="cb395-2"><a href="everyday-ml-classification.html#cb395-2" aria-hidden="true" tabindex="-1"></a><span class="fu">data</span>(Sonar)</span>
<span id="cb395-3"><a href="everyday-ml-classification.html#cb395-3" aria-hidden="true" tabindex="-1"></a>Sonar <span class="ot">&lt;-</span> <span class="fu">as_tibble</span>(Sonar)</span>
<span id="cb395-4"><a href="everyday-ml-classification.html#cb395-4" aria-hidden="true" tabindex="-1"></a>Sonar</span></code></pre></div>
<pre><code>## # A tibble: 208 × 61
##        V1     V2     V3     V4     V5     V6     V7     V8
##     &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;
##  1 0.02   0.0371 0.0428 0.0207 0.0954 0.0986 0.154  0.160 
##  2 0.0453 0.0523 0.0843 0.0689 0.118  0.258  0.216  0.348 
##  3 0.0262 0.0582 0.110  0.108  0.0974 0.228  0.243  0.377 
##  4 0.01   0.0171 0.0623 0.0205 0.0205 0.0368 0.110  0.128 
##  5 0.0762 0.0666 0.0481 0.0394 0.059  0.0649 0.121  0.247 
##  6 0.0286 0.0453 0.0277 0.0174 0.0384 0.099  0.120  0.183 
##  7 0.0317 0.0956 0.132  0.141  0.167  0.171  0.0731 0.140 
##  8 0.0519 0.0548 0.0842 0.0319 0.116  0.0922 0.103  0.0613
##  9 0.0223 0.0375 0.0484 0.0475 0.0647 0.0591 0.0753 0.0098
## 10 0.0164 0.0173 0.0347 0.007  0.0187 0.0671 0.106  0.0697
## # … with 198 more rows, and 53 more variables: V9 &lt;dbl&gt;,
## #   V10 &lt;dbl&gt;, V11 &lt;dbl&gt;, V12 &lt;dbl&gt;, V13 &lt;dbl&gt;, V14 &lt;dbl&gt;,
## #   V15 &lt;dbl&gt;, V16 &lt;dbl&gt;, V17 &lt;dbl&gt;, V18 &lt;dbl&gt;, V19 &lt;dbl&gt;,
## #   V20 &lt;dbl&gt;, V21 &lt;dbl&gt;, V22 &lt;dbl&gt;, V23 &lt;dbl&gt;, V24 &lt;dbl&gt;,
## #   V25 &lt;dbl&gt;, V26 &lt;dbl&gt;, V27 &lt;dbl&gt;, V28 &lt;dbl&gt;, V29 &lt;dbl&gt;,
## #   V30 &lt;dbl&gt;, V31 &lt;dbl&gt;, V32 &lt;dbl&gt;, V33 &lt;dbl&gt;, V34 &lt;dbl&gt;,
## #   V35 &lt;dbl&gt;, V36 &lt;dbl&gt;, V37 &lt;dbl&gt;, V38 &lt;dbl&gt;, V39 &lt;dbl&gt;, …</code></pre>
<p>The target labels in <code>Sonar</code> has two classes:</p>
<div class="sourceCode" id="cb397"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb397-1"><a href="everyday-ml-classification.html#cb397-1" aria-hidden="true" tabindex="-1"></a>Sonar<span class="sc">$</span>Class <span class="sc">%&gt;%</span> <span class="fu">str</span>()</span></code></pre></div>
<pre><code>##  Factor w/ 2 levels &quot;M&quot;,&quot;R&quot;: 2 2 2 2 2 2 2 2 2 2 ...</code></pre>
<p>Since this is a classification task, I will use <code>anovaScores()</code> to output a score for each feature.</p>
<div class="sourceCode" id="cb399"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb399-1"><a href="everyday-ml-classification.html#cb399-1" aria-hidden="true" tabindex="-1"></a>fit_anova <span class="ot">&lt;-</span> <span class="cf">function</span>(x, y) {</span>
<span id="cb399-2"><a href="everyday-ml-classification.html#cb399-2" aria-hidden="true" tabindex="-1"></a>    anova_res <span class="ot">&lt;-</span> <span class="fu">apply</span>(x, <span class="dv">2</span>, <span class="cf">function</span>(f) {<span class="fu">anovaScores</span>(f, y)})</span>
<span id="cb399-3"><a href="everyday-ml-classification.html#cb399-3" aria-hidden="true" tabindex="-1"></a>    <span class="fu">return</span>(anova_res)</span>
<span id="cb399-4"><a href="everyday-ml-classification.html#cb399-4" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb399-5"><a href="everyday-ml-classification.html#cb399-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb399-6"><a href="everyday-ml-classification.html#cb399-6" aria-hidden="true" tabindex="-1"></a>aov_res <span class="ot">&lt;-</span> <span class="fu">fit_anova</span>(<span class="at">x =</span> <span class="fu">select</span>(Sonar, <span class="sc">-</span>Class), </span>
<span id="cb399-7"><a href="everyday-ml-classification.html#cb399-7" aria-hidden="true" tabindex="-1"></a>                     <span class="at">y =</span> Sonar<span class="sc">$</span>Class)</span>
<span id="cb399-8"><a href="everyday-ml-classification.html#cb399-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb399-9"><a href="everyday-ml-classification.html#cb399-9" aria-hidden="true" tabindex="-1"></a>aov_res <span class="ot">&lt;-</span> <span class="fu">as.data.frame</span>(aov_res)</span>
<span id="cb399-10"><a href="everyday-ml-classification.html#cb399-10" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(aov_res)</span></code></pre></div>
<pre><code>##         aov_res
## V1 0.0000719490
## V2 0.0007779402
## V3 0.0054167746
## V4 0.0002607819
## V5 0.0012544689
## V6 0.0567376774</code></pre>
<p>The output for each feature is the p-value for the whole model F-test. These can be ranked to find the features with the greatest degree of relationship with the target labels:</p>
<div class="sourceCode" id="cb401"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb401-1"><a href="everyday-ml-classification.html#cb401-1" aria-hidden="true" tabindex="-1"></a>aov_res <span class="ot">&lt;-</span> aov_res <span class="sc">%&gt;%</span> <span class="fu">rownames_to_column</span>(<span class="at">var =</span> <span class="st">&#39;Var&#39;</span>) <span class="sc">%&gt;%</span></span>
<span id="cb401-2"><a href="everyday-ml-classification.html#cb401-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">as_tibble</span>() <span class="sc">%&gt;%</span> <span class="fu">rename</span>(<span class="at">pVal =</span> aov_res) <span class="sc">%&gt;%</span> <span class="fu">arrange</span>(aov_res)</span>
<span id="cb401-3"><a href="everyday-ml-classification.html#cb401-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb401-4"><a href="everyday-ml-classification.html#cb401-4" aria-hidden="true" tabindex="-1"></a>aov_res</span></code></pre></div>
<pre><code>## # A tibble: 60 × 2
##    Var       pVal
##    &lt;chr&gt;    &lt;dbl&gt;
##  1 V11   6.59e-11
##  2 V12   4.64e- 9
##  3 V49   1.96e- 7
##  4 V10   4.60e- 7
##  5 V45   5.30e- 7
##  6 V48   1.19e- 6
##  7 V9    2.20e- 6
##  8 V13   4.22e- 6
##  9 V46   7.16e- 6
## 10 V47   9.49e- 6
## # … with 50 more rows</code></pre>
</div>
<div id="feature-selection-using-recursive-feature-elimination" class="section level2 hasAnchor" number="6.3">
<h2><span class="header-section-number">6.3</span> Feature selection using recursive feature elimination<a href="everyday-ml-classification.html#feature-selection-using-recursive-feature-elimination" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>An alternative method for feature selection is recursive feature elimination (RFE). RFE is a wrapper method that uses another model to rank features based on variable importance. This model does not have to be the same model used in the downstream model prediction task.</p>
<p>The feature importance ranking method depends which model the RFE wrapper uses. Tree models such as the random forest, as previous mentioned, can use impurity scores or mean accuracy decrease to calculate this.</p>
<p>The <code>rfeControl()</code> function specifies the RFE model as well as the resampling method. Then <code>rfe()</code> runs the algorithm to identify important features as well as the model accuracy as the RFE recursively removes the less important features and trains the model.</p>
<div class="sourceCode" id="cb403"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb403-1"><a href="everyday-ml-classification.html#cb403-1" aria-hidden="true" tabindex="-1"></a>rfec <span class="ot">&lt;-</span> <span class="fu">rfeControl</span>(<span class="at">functions =</span> rfFuncs, <span class="at">method =</span> <span class="st">&#39;cv&#39;</span>,</span>
<span id="cb403-2"><a href="everyday-ml-classification.html#cb403-2" aria-hidden="true" tabindex="-1"></a>                  <span class="at">number =</span> <span class="dv">5</span>)</span>
<span id="cb403-3"><a href="everyday-ml-classification.html#cb403-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb403-4"><a href="everyday-ml-classification.html#cb403-4" aria-hidden="true" tabindex="-1"></a>rfeObj <span class="ot">&lt;-</span> <span class="fu">rfe</span>(<span class="at">x =</span> <span class="fu">select</span>(Sonar, <span class="sc">-</span>Class), <span class="at">y =</span> Sonar<span class="sc">$</span>Class,</span>
<span id="cb403-5"><a href="everyday-ml-classification.html#cb403-5" aria-hidden="true" tabindex="-1"></a>              <span class="at">rfeControl =</span> rfec)</span></code></pre></div>
<p>Calling the output shows that the top 5 most important features show overlap with the result from <code>anovaScores()</code> from the previous section, which is good. It also shows that keeping the original 60 features here shows the best model accuracy, which is fine. This won’t always be the case with increasing number of dimensions in the data.</p>
<div class="sourceCode" id="cb404"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb404-1"><a href="everyday-ml-classification.html#cb404-1" aria-hidden="true" tabindex="-1"></a>rfeObj</span></code></pre></div>
<pre><code>## 
## Recursive feature selection
## 
## Outer resampling method: Cross-Validated (5 fold) 
## 
## Resampling performance over subset size:
## 
##  Variables Accuracy  Kappa AccuracySD KappaSD Selected
##          4   0.7212 0.4398    0.05514 0.11064         
##          8   0.7308 0.4557    0.04581 0.09426         
##         16   0.7790 0.5539    0.05389 0.10782         
##         60   0.8176 0.6290    0.06359 0.12886        *
## 
## The top 5 variables (out of 60):
##    V11, V12, V9, V10, V48</code></pre>
<p>The fitted model and its performance can be retrieved as such:</p>
<div class="sourceCode" id="cb406"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb406-1"><a href="everyday-ml-classification.html#cb406-1" aria-hidden="true" tabindex="-1"></a>rfeObj<span class="sc">$</span>fit</span></code></pre></div>
<pre><code>## 
## Call:
##  randomForest(x = x, y = y, importance = TRUE) 
##                Type of random forest: classification
##                      Number of trees: 500
## No. of variables tried at each split: 7
## 
##         OOB estimate of  error rate: 15.87%
## Confusion matrix:
##     M  R class.error
## M 101 10  0.09009009
## R  23 74  0.23711340</code></pre>
<p>The ranking of the features can be retreived here, which is useful if we were to select the first few and subset our original dataset:</p>
<div class="sourceCode" id="cb408"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb408-1"><a href="everyday-ml-classification.html#cb408-1" aria-hidden="true" tabindex="-1"></a>rfeObj<span class="sc">$</span>optVariables</span></code></pre></div>
<pre><code>##  [1] &quot;V11&quot; &quot;V12&quot; &quot;V9&quot;  &quot;V10&quot; &quot;V48&quot; &quot;V47&quot; &quot;V36&quot; &quot;V49&quot; &quot;V28&quot;
## [10] &quot;V45&quot; &quot;V37&quot; &quot;V21&quot; &quot;V46&quot; &quot;V16&quot; &quot;V13&quot; &quot;V17&quot; &quot;V51&quot; &quot;V27&quot;
## [19] &quot;V4&quot;  &quot;V15&quot; &quot;V20&quot; &quot;V18&quot; &quot;V52&quot; &quot;V5&quot;  &quot;V44&quot; &quot;V23&quot; &quot;V1&quot; 
## [28] &quot;V31&quot; &quot;V43&quot; &quot;V26&quot; &quot;V22&quot; &quot;V35&quot; &quot;V19&quot; &quot;V30&quot; &quot;V14&quot; &quot;V8&quot; 
## [37] &quot;V39&quot; &quot;V34&quot; &quot;V32&quot; &quot;V24&quot; &quot;V54&quot; &quot;V42&quot; &quot;V29&quot; &quot;V59&quot; &quot;V55&quot;
## [46] &quot;V25&quot; &quot;V2&quot;  &quot;V6&quot;  &quot;V41&quot; &quot;V58&quot; &quot;V38&quot; &quot;V3&quot;  &quot;V33&quot; &quot;V40&quot;
## [55] &quot;V50&quot; &quot;V7&quot;  &quot;V53&quot; &quot;V57&quot; &quot;V60&quot; &quot;V56&quot;</code></pre>
<p>Calling <code>ggplot()</code> on the RFE result provides a visual look:</p>
<div class="sourceCode" id="cb410"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb410-1"><a href="everyday-ml-classification.html#cb410-1" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(rfeObj) <span class="sc">+</span> <span class="fu">theme_bw</span>()</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-212-1.png" width="672" /></p>
</div>
<div id="hyperparameter-tuning" class="section level2 hasAnchor" number="6.4">
<h2><span class="header-section-number">6.4</span> Hyperparameter tuning<a href="everyday-ml-classification.html#hyperparameter-tuning" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Previously when we trained the random forest model using <code>train()</code>, it automatically deduced the optimal values for the model hyperparameters. Under the hood, <code>train()</code> ran a grid search to find these values, but we can define our own grid as well.</p>
<p>Hyperparameter tuning should of course be done on the training set, so I will use the <code>Sonar</code> dataset to arrive at the training and testing sets:</p>
<div class="sourceCode" id="cb411"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb411-1"><a href="everyday-ml-classification.html#cb411-1" aria-hidden="true" tabindex="-1"></a>idx <span class="ot">&lt;-</span> <span class="fu">createDataPartition</span>(<span class="at">y =</span> Sonar<span class="sc">$</span>Class, <span class="at">p =</span> .<span class="dv">8</span>,</span>
<span id="cb411-2"><a href="everyday-ml-classification.html#cb411-2" aria-hidden="true" tabindex="-1"></a>                           <span class="at">list =</span> <span class="cn">FALSE</span>, <span class="at">times =</span> <span class="dv">1</span>)</span>
<span id="cb411-3"><a href="everyday-ml-classification.html#cb411-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb411-4"><a href="everyday-ml-classification.html#cb411-4" aria-hidden="true" tabindex="-1"></a>df_train <span class="ot">&lt;-</span> Sonar[idx,]</span>
<span id="cb411-5"><a href="everyday-ml-classification.html#cb411-5" aria-hidden="true" tabindex="-1"></a>df_test <span class="ot">&lt;-</span> Sonar[<span class="sc">-</span>idx,]</span>
<span id="cb411-6"><a href="everyday-ml-classification.html#cb411-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb411-7"><a href="everyday-ml-classification.html#cb411-7" aria-hidden="true" tabindex="-1"></a>preProcObj <span class="ot">&lt;-</span> <span class="fu">preProcess</span>(df_train, <span class="at">method =</span> <span class="fu">c</span>(<span class="st">&#39;center&#39;</span>, <span class="st">&#39;scale&#39;</span>))</span>
<span id="cb411-8"><a href="everyday-ml-classification.html#cb411-8" aria-hidden="true" tabindex="-1"></a>df_train <span class="ot">&lt;-</span> <span class="fu">predict</span>(preProcObj, df_train)</span>
<span id="cb411-9"><a href="everyday-ml-classification.html#cb411-9" aria-hidden="true" tabindex="-1"></a>df_test <span class="ot">&lt;-</span> <span class="fu">predict</span>(preProcObj, df_test)</span></code></pre></div>
<p>For the random forest model, I define the possible values for the three hyperparameters as such, and then train by providing an input for <code>tuneGrid =</code>.</p>
<div class="sourceCode" id="cb412"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb412-1"><a href="everyday-ml-classification.html#cb412-1" aria-hidden="true" tabindex="-1"></a>rf_grid <span class="ot">&lt;-</span> <span class="fu">expand.grid</span>(<span class="at">mtry =</span> <span class="fu">c</span>(<span class="dv">2</span>, <span class="dv">4</span>, <span class="dv">8</span>, <span class="dv">10</span>), </span>
<span id="cb412-2"><a href="everyday-ml-classification.html#cb412-2" aria-hidden="true" tabindex="-1"></a>                       <span class="at">splitrule =</span> <span class="fu">c</span>(<span class="st">&quot;gini&quot;</span>, <span class="st">&quot;extratrees&quot;</span>), </span>
<span id="cb412-3"><a href="everyday-ml-classification.html#cb412-3" aria-hidden="true" tabindex="-1"></a>                       <span class="at">min.node.size =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">3</span>, <span class="dv">5</span>))</span>
<span id="cb412-4"><a href="everyday-ml-classification.html#cb412-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb412-5"><a href="everyday-ml-classification.html#cb412-5" aria-hidden="true" tabindex="-1"></a>model_rf <span class="ot">&lt;-</span> <span class="fu">train</span>(Class <span class="sc">~</span> ., <span class="at">data =</span> df_train, <span class="at">method =</span> <span class="st">&#39;ranger&#39;</span>,</span>
<span id="cb412-6"><a href="everyday-ml-classification.html#cb412-6" aria-hidden="true" tabindex="-1"></a>                    <span class="at">importance =</span> <span class="st">&#39;impurity&#39;</span>, <span class="at">trControl =</span> tr,</span>
<span id="cb412-7"><a href="everyday-ml-classification.html#cb412-7" aria-hidden="true" tabindex="-1"></a>                    <span class="at">tuneGrid =</span> rf_grid)</span></code></pre></div>
<p>Calling the model then shows the model performance (with the specified resampling method) for each combination of the grid search:</p>
<div class="sourceCode" id="cb413"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb413-1"><a href="everyday-ml-classification.html#cb413-1" aria-hidden="true" tabindex="-1"></a>model_rf</span></code></pre></div>
<pre><code>## Random Forest 
## 
## 167 samples
##  60 predictor
##   2 classes: &#39;M&#39;, &#39;R&#39; 
## 
## No pre-processing
## Resampling: Cross-Validated (5 fold) 
## Summary of sample sizes: 133, 133, 134, 133, 135 
## Resampling results across tuning parameters:
## 
##   mtry  splitrule   min.node.size  Accuracy   Kappa    
##    2    gini        1              0.8151181  0.6228532
##    2    gini        3              0.8327540  0.6590288
##    2    gini        5              0.8090463  0.6096000
##    2    extratrees  1              0.8207999  0.6350424
##    2    extratrees  3              0.8211676  0.6358619
##    2    extratrees  5              0.8022504  0.5957834
##    4    gini        1              0.8329434  0.6587746
##    4    gini        3              0.8210004  0.6353605
##    4    gini        5              0.8268828  0.6467128
##    4    extratrees  1              0.8206217  0.6332840
##    4    extratrees  3              0.8325758  0.6576618
##    4    extratrees  5              0.8450646  0.6844312
##    8    gini        1              0.8208222  0.6363709
##    8    gini        3              0.8268828  0.6495138
##    8    gini        5              0.8208222  0.6357195
##    8    extratrees  1              0.8445187  0.6823220
##    8    extratrees  3              0.8388146  0.6720493
##    8    extratrees  5              0.8272282  0.6488559
##   10    gini        1              0.8329434  0.6599363
##   10    gini        3              0.8388369  0.6717421
##   10    gini        5              0.8267045  0.6480589
##   10    extratrees  1              0.8206217  0.6339633
##   10    extratrees  3              0.8441511  0.6816926
##   10    extratrees  5              0.8265040  0.6457295
## 
## Accuracy was used to select the optimal model using
##  the largest value.
## The final values used for the model were mtry = 4,
##  splitrule = extratrees and min.node.size = 5.</code></pre>
<p>As before, the best set of hyperparameters can be retrieved:</p>
<div class="sourceCode" id="cb415"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb415-1"><a href="everyday-ml-classification.html#cb415-1" aria-hidden="true" tabindex="-1"></a>model_rf<span class="sc">$</span>bestTune</span></code></pre></div>
<pre><code>##    mtry  splitrule min.node.size
## 12    4 extratrees             5</code></pre>
<p>Instead of a predefined grid search, we can do a randomized search instead. This can be done by setting <code>search = 'random'</code> within <code>trainControl()</code> first and then specifying <code>tuneLength =</code> in <code>train()</code>.</p>
<p>Since we’ve only used random forest models so far, here I will do a similar grid search but using a support vector machine (SVM) with the radial basis function kernel instead:</p>
<div class="sourceCode" id="cb417"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb417-1"><a href="everyday-ml-classification.html#cb417-1" aria-hidden="true" tabindex="-1"></a>tr_svm <span class="ot">&lt;-</span> <span class="fu">trainControl</span>(<span class="at">method =</span> <span class="st">&#39;cv&#39;</span>,</span>
<span id="cb417-2"><a href="everyday-ml-classification.html#cb417-2" aria-hidden="true" tabindex="-1"></a>                   <span class="at">number =</span> <span class="dv">5</span>,</span>
<span id="cb417-3"><a href="everyday-ml-classification.html#cb417-3" aria-hidden="true" tabindex="-1"></a>                   <span class="at">classProbs =</span> <span class="cn">TRUE</span>,</span>
<span id="cb417-4"><a href="everyday-ml-classification.html#cb417-4" aria-hidden="true" tabindex="-1"></a>                   <span class="at">search =</span> <span class="st">&#39;random&#39;</span>)</span>
<span id="cb417-5"><a href="everyday-ml-classification.html#cb417-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb417-6"><a href="everyday-ml-classification.html#cb417-6" aria-hidden="true" tabindex="-1"></a>model_svm <span class="ot">&lt;-</span> <span class="fu">train</span>(Class <span class="sc">~</span> ., <span class="at">data =</span> df_train, </span>
<span id="cb417-7"><a href="everyday-ml-classification.html#cb417-7" aria-hidden="true" tabindex="-1"></a>                   <span class="at">method =</span> <span class="st">&#39;svmRadial&#39;</span>,</span>
<span id="cb417-8"><a href="everyday-ml-classification.html#cb417-8" aria-hidden="true" tabindex="-1"></a>                   <span class="at">trControl =</span> tr_svm, </span>
<span id="cb417-9"><a href="everyday-ml-classification.html#cb417-9" aria-hidden="true" tabindex="-1"></a>                   <span class="at">tunelength =</span> <span class="dv">8</span>)</span></code></pre></div>
<p>Calling the model shows the best combination for <code>C</code> (cost) and <code>sigma</code> based on the model accuracy:</p>
<div class="sourceCode" id="cb418"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb418-1"><a href="everyday-ml-classification.html#cb418-1" aria-hidden="true" tabindex="-1"></a>model_svm</span></code></pre></div>
<pre><code>## Support Vector Machines with Radial Basis Function Kernel 
## 
## 167 samples
##  60 predictor
##   2 classes: &#39;M&#39;, &#39;R&#39; 
## 
## No pre-processing
## Resampling: Cross-Validated (5 fold) 
## Summary of sample sizes: 133, 133, 133, 134, 135 
## Resampling results across tuning parameters:
## 
##   sigma        C           Accuracy   Kappa    
##   0.002678978   58.142065  0.7708445  0.5357187
##   0.024781262  563.129570  0.8680481  0.7316302
##   0.033519260    2.093291  0.8564617  0.7081256
## 
## Accuracy was used to select the optimal model using
##  the largest value.
## The final values used for the model were sigma =
##  0.02478126 and C = 563.1296.</code></pre>
</div>
<div id="roc-and-precision-recall-curves" class="section level2 hasAnchor" number="6.5">
<h2><span class="header-section-number">6.5</span> ROC and precision-recall curves<a href="everyday-ml-classification.html#roc-and-precision-recall-curves" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>A nice way to visualize model evaluation is by using ROC curves, which uses metrics that were already calculated previously - precision and recall.</p>
<p>For this I will generate predictions for the random forest model using <code>Sonar</code>. Setting <code>type = 'prob'</code> yields probabilities for each label classification instead of the label itself:</p>
<div class="sourceCode" id="cb420"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb420-1"><a href="everyday-ml-classification.html#cb420-1" aria-hidden="true" tabindex="-1"></a>pred <span class="ot">&lt;-</span> <span class="fu">predict</span>(model_rf, df_test, <span class="at">type =</span> <span class="st">&#39;prob&#39;</span>)</span>
<span id="cb420-2"><a href="everyday-ml-classification.html#cb420-2" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(pred)</span></code></pre></div>
<pre><code>##           M         R
## 1 0.5870667 0.4129333
## 2 0.6210333 0.3789667
## 3 0.4531667 0.5468333
## 4 0.2409667 0.7590333
## 5 0.6803667 0.3196333
## 6 0.1867333 0.8132667</code></pre>
<p>The package <code>MLeval</code> can be used to generate ROC curves as such; here we achieve the ROC area under the curve of 0.96.</p>
<div class="sourceCode" id="cb422"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb422-1"><a href="everyday-ml-classification.html#cb422-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(MLeval)</span></code></pre></div>
<pre><code>## 
## Attaching package: &#39;MLeval&#39;</code></pre>
<pre><code>## The following object is masked _by_ &#39;.GlobalEnv&#39;:
## 
##     Sonar</code></pre>
<div class="sourceCode" id="cb425"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb425-1"><a href="everyday-ml-classification.html#cb425-1" aria-hidden="true" tabindex="-1"></a>roc_rf <span class="ot">&lt;-</span> <span class="fu">evalm</span>(<span class="fu">data.frame</span>(pred, df_test<span class="sc">$</span>Class, <span class="at">Group =</span> <span class="st">&#39;RF&#39;</span>), </span>
<span id="cb425-2"><a href="everyday-ml-classification.html#cb425-2" aria-hidden="true" tabindex="-1"></a>                <span class="at">showplots =</span> <span class="cn">FALSE</span>, <span class="at">silent =</span> <span class="cn">TRUE</span>)</span>
<span id="cb425-3"><a href="everyday-ml-classification.html#cb425-3" aria-hidden="true" tabindex="-1"></a>roc_rf<span class="sc">$</span>roc</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-220-1.png" width="672" /></p>
<p>Similarly, a precision-recall curve can also be visualized. This curve shows the tradeoff between the two metrics for each threshold.</p>
<div class="sourceCode" id="cb426"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb426-1"><a href="everyday-ml-classification.html#cb426-1" aria-hidden="true" tabindex="-1"></a>roc_rf<span class="sc">$</span>proc</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-221-1.png" width="672" /></p>
<p>The values can be retrieved here:</p>
<div class="sourceCode" id="cb427"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb427-1"><a href="everyday-ml-classification.html#cb427-1" aria-hidden="true" tabindex="-1"></a>roc_rf<span class="sc">$</span>optres</span></code></pre></div>
<pre><code>## $RF
##               Score        CI
## SENS          0.842 0.62-0.94
## SPEC          0.909 0.72-0.97
## MCC           0.755      &lt;NA&gt;
## Informedness  0.751      &lt;NA&gt;
## PREC          0.889 0.67-0.97
## NPV           0.870 0.68-0.95
## FPR           0.091      &lt;NA&gt;
## F1            0.865      &lt;NA&gt;
## TP           16.000      &lt;NA&gt;
## FP            2.000      &lt;NA&gt;
## TN           20.000      &lt;NA&gt;
## FN            3.000      &lt;NA&gt;
## AUC-ROC       0.930 0.84-1.02
## AUC-PR        0.880      &lt;NA&gt;
## AUC-PRG       0.610      &lt;NA&gt;</code></pre>
<p>For completeness I will make these figures for the SVM model as well:</p>
<div class="sourceCode" id="cb429"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb429-1"><a href="everyday-ml-classification.html#cb429-1" aria-hidden="true" tabindex="-1"></a>pred2 <span class="ot">&lt;-</span> <span class="fu">predict</span>(model_svm, df_test, <span class="at">type =</span> <span class="st">&#39;prob&#39;</span>)</span></code></pre></div>
<div class="sourceCode" id="cb430"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb430-1"><a href="everyday-ml-classification.html#cb430-1" aria-hidden="true" tabindex="-1"></a>roc_svm <span class="ot">&lt;-</span> <span class="fu">evalm</span>(<span class="fu">data.frame</span>(pred2, df_test<span class="sc">$</span>Class, <span class="at">Group =</span> <span class="st">&#39;SVM&#39;</span>), </span>
<span id="cb430-2"><a href="everyday-ml-classification.html#cb430-2" aria-hidden="true" tabindex="-1"></a>                <span class="at">showplots =</span> <span class="cn">FALSE</span>, <span class="at">silent =</span> <span class="cn">TRUE</span>)</span>
<span id="cb430-3"><a href="everyday-ml-classification.html#cb430-3" aria-hidden="true" tabindex="-1"></a>roc_svm<span class="sc">$</span>roc</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-224-1.png" width="672" /></p>
<div class="sourceCode" id="cb431"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb431-1"><a href="everyday-ml-classification.html#cb431-1" aria-hidden="true" tabindex="-1"></a>roc_svm<span class="sc">$</span>proc</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-225-1.png" width="672" /></p>
</div>
<div id="model-comparisons" class="section level2 hasAnchor" number="6.6">
<h2><span class="header-section-number">6.6</span> Model comparisons<a href="everyday-ml-classification.html#model-comparisons" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><code>caret</code> provides an elegant way to compare the performance of multiple models for model selection. We have two models trained on <code>Sonar</code> dataset already, so I will train two more.</p>
<p>Here I am using a gradient boosted machine (<code>gbm</code>) and a k-nearest neighbors (<code>knn</code>).</p>
<div class="sourceCode" id="cb432"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb432-1"><a href="everyday-ml-classification.html#cb432-1" aria-hidden="true" tabindex="-1"></a><span class="co"># model_rf</span></span>
<span id="cb432-2"><a href="everyday-ml-classification.html#cb432-2" aria-hidden="true" tabindex="-1"></a><span class="co"># model_svm</span></span>
<span id="cb432-3"><a href="everyday-ml-classification.html#cb432-3" aria-hidden="true" tabindex="-1"></a>model_gbm <span class="ot">&lt;-</span> <span class="fu">train</span>(Class <span class="sc">~</span>., <span class="at">data =</span> df_train, </span>
<span id="cb432-4"><a href="everyday-ml-classification.html#cb432-4" aria-hidden="true" tabindex="-1"></a>                   <span class="at">method =</span> <span class="st">&#39;gbm&#39;</span>, <span class="at">trControl =</span> tr, </span>
<span id="cb432-5"><a href="everyday-ml-classification.html#cb432-5" aria-hidden="true" tabindex="-1"></a>                   <span class="at">verbose =</span> <span class="cn">FALSE</span>)</span>
<span id="cb432-6"><a href="everyday-ml-classification.html#cb432-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb432-7"><a href="everyday-ml-classification.html#cb432-7" aria-hidden="true" tabindex="-1"></a>model_knn <span class="ot">&lt;-</span> <span class="fu">train</span>(Class <span class="sc">~</span> ., <span class="at">data =</span> df_train,</span>
<span id="cb432-8"><a href="everyday-ml-classification.html#cb432-8" aria-hidden="true" tabindex="-1"></a>                   <span class="at">method =</span> <span class="st">&#39;knn&#39;</span>, <span class="at">trControl =</span> tr)</span></code></pre></div>
<div class="sourceCode" id="cb433"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb433-1"><a href="everyday-ml-classification.html#cb433-1" aria-hidden="true" tabindex="-1"></a>model_gbm</span></code></pre></div>
<pre><code>## Stochastic Gradient Boosting 
## 
## 167 samples
##  60 predictor
##   2 classes: &#39;M&#39;, &#39;R&#39; 
## 
## No pre-processing
## Resampling: Cross-Validated (5 fold) 
## Summary of sample sizes: 134, 133, 134, 133, 134 
## Resampling results across tuning parameters:
## 
##   interaction.depth  n.trees  Accuracy   Kappa    
##   1                   50      0.8263815  0.6476087
##   1                  100      0.8500891  0.6962862
##   1                  150      0.8438503  0.6846292
##   2                   50      0.8074866  0.6102189
##   2                  100      0.8376114  0.6720095
##   2                  150      0.8556150  0.7082185
##   3                   50      0.8379679  0.6717409
##   3                  100      0.8436720  0.6837614
##   3                  150      0.8557932  0.7087382
## 
## Tuning parameter &#39;shrinkage&#39; was held constant at a value
##  of 0.1
## Tuning parameter &#39;n.minobsinnode&#39; was held
##  constant at a value of 10
## Accuracy was used to select the optimal model using
##  the largest value.
## The final values used for the model were n.trees =
##  150, interaction.depth = 3, shrinkage = 0.1
##  and n.minobsinnode = 10.</code></pre>
<div class="sourceCode" id="cb435"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb435-1"><a href="everyday-ml-classification.html#cb435-1" aria-hidden="true" tabindex="-1"></a>model_knn</span></code></pre></div>
<pre><code>## k-Nearest Neighbors 
## 
## 167 samples
##  60 predictor
##   2 classes: &#39;M&#39;, &#39;R&#39; 
## 
## No pre-processing
## Resampling: Cross-Validated (5 fold) 
## Summary of sample sizes: 133, 133, 134, 134, 134 
## Resampling results across tuning parameters:
## 
##   k  Accuracy   Kappa    
##   5  0.7852050  0.5633587
##   7  0.7672014  0.5278251
##   9  0.7433155  0.4775343
## 
## Accuracy was used to select the optimal model using
##  the largest value.
## The final value used for the model was k = 5.</code></pre>
<p>Both accuracy and kappa are then used to compare the model performance across the four models:</p>
<div class="sourceCode" id="cb437"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb437-1"><a href="everyday-ml-classification.html#cb437-1" aria-hidden="true" tabindex="-1"></a>comps <span class="ot">&lt;-</span> <span class="fu">resamples</span>(</span>
<span id="cb437-2"><a href="everyday-ml-classification.html#cb437-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">list</span>(<span class="at">RF =</span> model_rf, <span class="at">SVM =</span> model_svm, <span class="at">GBM =</span> model_gbm, </span>
<span id="cb437-3"><a href="everyday-ml-classification.html#cb437-3" aria-hidden="true" tabindex="-1"></a>       <span class="at">KNN =</span> model_knn)</span>
<span id="cb437-4"><a href="everyday-ml-classification.html#cb437-4" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb437-5"><a href="everyday-ml-classification.html#cb437-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb437-6"><a href="everyday-ml-classification.html#cb437-6" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(comps)</span></code></pre></div>
<pre><code>## 
## Call:
## summary.resamples(object = comps)
## 
## Models: RF, SVM, GBM, KNN 
## Number of resamples: 5 
## 
## Accuracy 
##          Min.   1st Qu.    Median      Mean   3rd Qu.
## RF  0.7941176 0.8235294 0.8484848 0.8450646 0.8529412
## SVM 0.8181818 0.8235294 0.8750000 0.8680481 0.8823529
## GBM 0.7575758 0.8484848 0.8529412 0.8557932 0.8787879
## KNN 0.7058824 0.7272727 0.7352941 0.7852050 0.8484848
##          Max. NA&#39;s
## RF  0.9062500    0
## SVM 0.9411765    0
## GBM 0.9411765    0
## KNN 0.9090909    0
## 
## Kappa 
##          Min.   1st Qu.    Median      Mean   3rd Qu.
## RF  0.5853659 0.6382979 0.6857143 0.6844312 0.7017544
## SVM 0.6250000 0.6382979 0.7490196 0.7316302 0.7638889
## GBM 0.5056180 0.6961326 0.7017544 0.7087382 0.7582418
## KNN 0.4055944 0.4469274 0.4593640 0.5633587 0.6892655
##          Max. NA&#39;s
## RF  0.8110236    0
## SVM 0.8819444    0
## GBM 0.8819444    0
## KNN 0.8156425    0</code></pre>
<p>And finally, a quick visualization at the model performance comparisons:</p>
<div class="sourceCode" id="cb439"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb439-1"><a href="everyday-ml-classification.html#cb439-1" aria-hidden="true" tabindex="-1"></a><span class="fu">dotplot</span>(comps)</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-230-1.png" width="672" /></p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="everyday-exploratory-data-analysis.html" class="navigation navigation-prev navigation-unique" aria-label="Previous page"><i class="fa fa-angle-left"></i></a>

    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/USERNAME/REPO/edit/BRANCH/05-Classification.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["_main.pdf", "_main.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

</body>

</html>
