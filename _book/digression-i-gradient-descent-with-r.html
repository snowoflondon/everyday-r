<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 8 Digression I: Gradient descent with R | Everyday-R: Practical R for Data Science</title>
  <meta name="description" content="<p>This is a minimal example of using the bookdown package to write a book.
The HTML output format for this example is bookdown::gitbook,
set in the _output.yml file.</p>" />
  <meta name="generator" content="bookdown 0.34 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 8 Digression I: Gradient descent with R | Everyday-R: Practical R for Data Science" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="<p>This is a minimal example of using the bookdown package to write a book.
The HTML output format for this example is bookdown::gitbook,
set in the _output.yml file.</p>" />
  <meta name="github-repo" content="rstudio/bookdown-demo" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 8 Digression I: Gradient descent with R | Everyday-R: Practical R for Data Science" />
  
  <meta name="twitter:description" content="<p>This is a minimal example of using the bookdown package to write a book.
The HTML output format for this example is bookdown::gitbook,
set in the _output.yml file.</p>" />
  

<meta name="author" content="by Brian Jungmin Park" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="everyday-ml-regression.html"/>
<link rel="next" href="digression-ii-survival-analysis-with-r.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Everyday-R by snowoflondon</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Introduction</a>
<ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#preface"><i class="fa fa-check"></i><b>1.1</b> Preface</a></li>
<li class="chapter" data-level="1.2" data-path="index.html"><a href="index.html#r-syntax-in-this-book"><i class="fa fa-check"></i><b>1.2</b> R syntax in this book</a></li>
<li class="chapter" data-level="1.3" data-path="index.html"><a href="index.html#r-packages-commonly-used-in-this-book"><i class="fa fa-check"></i><b>1.3</b> R packages commonly used in this book</a></li>
<li class="chapter" data-level="1.4" data-path="index.html"><a href="index.html#installing-r-packages"><i class="fa fa-check"></i><b>1.4</b> Installing R packages</a></li>
<li class="chapter" data-level="1.5" data-path="index.html"><a href="index.html#code-availability"><i class="fa fa-check"></i><b>1.5</b> Code availability</a></li>
<li class="chapter" data-level="1.6" data-path="index.html"><a href="index.html#website-hosting"><i class="fa fa-check"></i><b>1.6</b> Website hosting</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="everyday-data-wrangling.html"><a href="everyday-data-wrangling.html"><i class="fa fa-check"></i><b>2</b> Everyday data wrangling</a>
<ul>
<li class="chapter" data-level="2.1" data-path="everyday-data-wrangling.html"><a href="everyday-data-wrangling.html#renaming-column-headers"><i class="fa fa-check"></i><b>2.1</b> Renaming column headers</a></li>
<li class="chapter" data-level="2.2" data-path="everyday-data-wrangling.html"><a href="everyday-data-wrangling.html#grouped-operations"><i class="fa fa-check"></i><b>2.2</b> Grouped operations</a></li>
<li class="chapter" data-level="2.3" data-path="everyday-data-wrangling.html"><a href="everyday-data-wrangling.html#data-transformation"><i class="fa fa-check"></i><b>2.3</b> Data transformation</a></li>
<li class="chapter" data-level="2.4" data-path="everyday-data-wrangling.html"><a href="everyday-data-wrangling.html#joining-and-separating-character-columns"><i class="fa fa-check"></i><b>2.4</b> Joining and separating character columns</a></li>
<li class="chapter" data-level="2.5" data-path="everyday-data-wrangling.html"><a href="everyday-data-wrangling.html#filtering-rows"><i class="fa fa-check"></i><b>2.5</b> Filtering rows</a></li>
<li class="chapter" data-level="2.6" data-path="everyday-data-wrangling.html"><a href="everyday-data-wrangling.html#subsetting-columns-based-on-strings"><i class="fa fa-check"></i><b>2.6</b> Subsetting columns based on strings</a></li>
<li class="chapter" data-level="2.7" data-path="everyday-data-wrangling.html"><a href="everyday-data-wrangling.html#long-and-wide-data-formats"><i class="fa fa-check"></i><b>2.7</b> Long and wide data formats</a></li>
<li class="chapter" data-level="2.8" data-path="everyday-data-wrangling.html"><a href="everyday-data-wrangling.html#trimming-strings"><i class="fa fa-check"></i><b>2.8</b> Trimming strings</a></li>
<li class="chapter" data-level="2.9" data-path="everyday-data-wrangling.html"><a href="everyday-data-wrangling.html#iterating-over-list-of-dataframes"><i class="fa fa-check"></i><b>2.9</b> Iterating over list of dataframes</a></li>
<li class="chapter" data-level="2.10" data-path="everyday-data-wrangling.html"><a href="everyday-data-wrangling.html#rowwise-operations"><i class="fa fa-check"></i><b>2.10</b> Rowwise operations</a></li>
<li class="chapter" data-level="2.11" data-path="everyday-data-wrangling.html"><a href="everyday-data-wrangling.html#conditional-transformation"><i class="fa fa-check"></i><b>2.11</b> Conditional transformation</a></li>
<li class="chapter" data-level="2.12" data-path="everyday-data-wrangling.html"><a href="everyday-data-wrangling.html#missing-values"><i class="fa fa-check"></i><b>2.12</b> Missing values</a></li>
<li class="chapter" data-level="2.13" data-path="everyday-data-wrangling.html"><a href="everyday-data-wrangling.html#joining-dataframes"><i class="fa fa-check"></i><b>2.13</b> Joining dataframes</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="everyday-iterations.html"><a href="everyday-iterations.html"><i class="fa fa-check"></i><b>3</b> Everyday iterations</a>
<ul>
<li class="chapter" data-level="3.1" data-path="everyday-iterations.html"><a href="everyday-iterations.html#iterating-over-multiple-columns-using-apply-dplyr-and-purrr"><i class="fa fa-check"></i><b>3.1</b> Iterating over multiple columns using <code>apply()</code>, <code>dplyr</code>, and <code>purrr</code></a></li>
<li class="chapter" data-level="3.2" data-path="everyday-iterations.html"><a href="everyday-iterations.html#iterating-over-lists"><i class="fa fa-check"></i><b>3.2</b> Iterating over lists</a></li>
<li class="chapter" data-level="3.3" data-path="everyday-iterations.html"><a href="everyday-iterations.html#iterating-over-vectors"><i class="fa fa-check"></i><b>3.3</b> Iterating over vectors</a></li>
<li class="chapter" data-level="3.4" data-path="everyday-iterations.html"><a href="everyday-iterations.html#iterating-with-two-inputs"><i class="fa fa-check"></i><b>3.4</b> Iterating with two inputs</a></li>
<li class="chapter" data-level="3.5" data-path="everyday-iterations.html"><a href="everyday-iterations.html#iterating-over-indices-and-names"><i class="fa fa-check"></i><b>3.5</b> Iterating over indices and names</a></li>
<li class="chapter" data-level="3.6" data-path="everyday-iterations.html"><a href="everyday-iterations.html#handling-errors-within-purrr"><i class="fa fa-check"></i><b>3.6</b> Handling errors within <code>purrr</code></a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="interlude-i-a-brief-glimpse-into-data.html"><a href="interlude-i-a-brief-glimpse-into-data.html"><i class="fa fa-check"></i><b>4</b> Interlude I: A brief glimpse into <code>data.table</code></a>
<ul>
<li class="chapter" data-level="4.1" data-path="interlude-i-a-brief-glimpse-into-data.html"><a href="interlude-i-a-brief-glimpse-into-data.html#data-wrangling-operations"><i class="fa fa-check"></i><b>4.1</b> Data wrangling operations</a></li>
<li class="chapter" data-level="4.2" data-path="interlude-i-a-brief-glimpse-into-data.html"><a href="interlude-i-a-brief-glimpse-into-data.html#sd-.sdcols-and"><i class="fa fa-check"></i><b>4.2</b> <code>.SD</code>, <code>.SDcols</code>, and <code>:=</code></a></li>
<li class="chapter" data-level="4.3" data-path="interlude-i-a-brief-glimpse-into-data.html"><a href="interlude-i-a-brief-glimpse-into-data.html#reshaping-data-using-melt-and-dcast"><i class="fa fa-check"></i><b>4.3</b> Reshaping data using <code>melt</code> and <code>dcast</code></a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="everyday-exploratory-data-analysis.html"><a href="everyday-exploratory-data-analysis.html"><i class="fa fa-check"></i><b>5</b> Everyday exploratory data analysis</a>
<ul>
<li class="chapter" data-level="5.1" data-path="everyday-exploratory-data-analysis.html"><a href="everyday-exploratory-data-analysis.html#workflow-1-continuous-data"><i class="fa fa-check"></i><b>5.1</b> Workflow 1: continuous data</a></li>
<li class="chapter" data-level="5.2" data-path="everyday-exploratory-data-analysis.html"><a href="everyday-exploratory-data-analysis.html#workflow-2-dates-and-ordinal-data"><i class="fa fa-check"></i><b>5.2</b> Workflow 2: dates and ordinal data</a></li>
<li class="chapter" data-level="5.3" data-path="everyday-exploratory-data-analysis.html"><a href="everyday-exploratory-data-analysis.html#visualization-of-clusters"><i class="fa fa-check"></i><b>5.3</b> Visualization of clusters</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="everyday-ml-classification.html"><a href="everyday-ml-classification.html"><i class="fa fa-check"></i><b>6</b> Everyday ML: Classification</a>
<ul>
<li class="chapter" data-level="6.1" data-path="everyday-ml-classification.html"><a href="everyday-ml-classification.html#model-training-and-predictions"><i class="fa fa-check"></i><b>6.1</b> Model training and predictions</a></li>
<li class="chapter" data-level="6.2" data-path="everyday-ml-classification.html"><a href="everyday-ml-classification.html#feature-selection-using-univariate-filters"><i class="fa fa-check"></i><b>6.2</b> Feature selection using univariate filters</a></li>
<li class="chapter" data-level="6.3" data-path="everyday-ml-classification.html"><a href="everyday-ml-classification.html#feature-selection-using-recursive-feature-elimination"><i class="fa fa-check"></i><b>6.3</b> Feature selection using recursive feature elimination</a></li>
<li class="chapter" data-level="6.4" data-path="everyday-ml-classification.html"><a href="everyday-ml-classification.html#feature-selection-for-correlated-and-low-variance-predictors"><i class="fa fa-check"></i><b>6.4</b> Feature selection for correlated and low-variance predictors</a></li>
<li class="chapter" data-level="6.5" data-path="everyday-ml-classification.html"><a href="everyday-ml-classification.html#hyperparameter-tuning"><i class="fa fa-check"></i><b>6.5</b> Hyperparameter tuning</a></li>
<li class="chapter" data-level="6.6" data-path="everyday-ml-classification.html"><a href="everyday-ml-classification.html#roc-and-precision-recall-curves"><i class="fa fa-check"></i><b>6.6</b> ROC and precision-recall curves</a></li>
<li class="chapter" data-level="6.7" data-path="everyday-ml-classification.html"><a href="everyday-ml-classification.html#model-comparisons"><i class="fa fa-check"></i><b>6.7</b> Model comparisons</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="everyday-ml-regression.html"><a href="everyday-ml-regression.html"><i class="fa fa-check"></i><b>7</b> Everyday ML: Regression</a>
<ul>
<li class="chapter" data-level="7.1" data-path="everyday-ml-regression.html"><a href="everyday-ml-regression.html#simple-linear-regression"><i class="fa fa-check"></i><b>7.1</b> Simple linear regression</a></li>
<li class="chapter" data-level="7.2" data-path="everyday-ml-regression.html"><a href="everyday-ml-regression.html#using-regression-for-prediction"><i class="fa fa-check"></i><b>7.2</b> Using regression for prediction</a></li>
<li class="chapter" data-level="7.3" data-path="everyday-ml-regression.html"><a href="everyday-ml-regression.html#categorical-predictors-and-factor-encoding"><i class="fa fa-check"></i><b>7.3</b> Categorical predictors and factor encoding</a></li>
<li class="chapter" data-level="7.4" data-path="everyday-ml-regression.html"><a href="everyday-ml-regression.html#elastic-net-regression"><i class="fa fa-check"></i><b>7.4</b> Elastic net regression</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="digression-i-gradient-descent-with-r.html"><a href="digression-i-gradient-descent-with-r.html"><i class="fa fa-check"></i><b>8</b> Digression I: Gradient descent with R</a></li>
<li class="chapter" data-level="9" data-path="digression-ii-survival-analysis-with-r.html"><a href="digression-ii-survival-analysis-with-r.html"><i class="fa fa-check"></i><b>9</b> Digression II: Survival analysis with R</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Everyday-R: Practical R for Data Science</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="digression-i-gradient-descent-with-r" class="section level1 hasAnchor" number="8">
<h1><span class="header-section-number">Chapter 8</span> Digression I: Gradient descent with R<a href="digression-i-gradient-descent-with-r.html#digression-i-gradient-descent-with-r" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>Previous two chapters covered two main facets of supervised models: classification and regression. Though it may seem trivial, one of the most important aspects of machine learning is the model training process. When training a model, one must consider both domain knowledge (which may aid in feature engineering) as well as model complexity. Related to these ideas is the concept of parameter space - therein which we can formally define the process of model training as <em>searching for a combination of model parameters that minimizes a cost function</em>. Now, this isn’t a ML book so there is no need to do a deep-dive here at all; however, the preceding definition can boil down to a simple task of minimizing error - a concept that extends to the simplest of models such as a simple linear model.</p>
<p>In brief, model training is an iterative process where we evaluate a prediction <span class="math inline">\(y(x_{i})\)</span> over a training set, evaluate a loss (i.e., deviation between <span class="math inline">\(y(x_{i})\)</span> and <span class="math inline">\(y\)</span> - the ground truth), then tweak the model (i.e., model parameters) to minimize the loss.</p>
<p>In simple linear regression, this loss term is actually squared to account for deviations in either direction. This is called the square loss. Summing the square loss over the number of samples then, we get the <em>mean squared error</em> or <em>MSE</em>.</p>
<p><span class="math inline">\(MSE=J(\theta_{1},\theta_{0})=\frac{1}{N}\sum_{i=1}^{N}(y(x_{i})-y_{i})^{2}\)</span></p>
<p>Here, <span class="math inline">\(\theta\)</span> is the model’s parameter; since this is a simple linear regression (i.e., the familiar <span class="math inline">\(y=mx+b\)</span>), <span class="math inline">\(\theta_{1}\)</span> describes the slope and <span class="math inline">\(\theta_{0}\)</span> the intercept (i.e., the bias term). Note that the MSE is a continuous function; it is differentiable everywhere and has just one global minimum. This makes gradient descent trivial, as we will see below.</p>
<p>In gradient descent, we will find a value of <span class="math inline">\(\theta\)</span> that minimizes the MSE. This means that we will look for the derivative (in this case, it’s actually partial deriviatves since we have two terms - the slope and the intercept) of the cost function and tweak the model parameters in the opposite direction (so that we are going <em>down</em> the slope). This will ensure that we will (eventually) converge to a minimum value.</p>
<p>Keep in mind - here, we are looking for the local gradient descent with respect to <span class="math inline">\(\theta\)</span>. This means that the problem gets more challenging with complex cost functions with multiple local minima. This sounds annoying, but will not affect us in our example with the MSE.</p>
<p>Annoyingly, we have to do some math before we get implement gradient descent for our example. However, finding the partial derivatives for the MSE function is not too difficult:</p>
<p>For <span class="math inline">\(j = 0,1\)</span>:</p>
<p><span class="math inline">\(\frac{\partial}{\partial \theta_{j}}J(\theta_{1},\theta_{0})=\frac{\partial}{\partial \theta_{j}}[\frac{1}{N}\sum_{i=1}^{N}(y(x_{i})-y_{i})^{2}]\)</span></p>
<p><span class="math inline">\(\frac{\partial}{\partial \theta_{j}}J(\theta_{1},\theta_{0})=\frac{1}{N}\frac{\partial}{\partial \theta_{j}}\sum_{i=1}^{N}(y(x_{i})-y_{i})^{2}\)</span></p>
<p>After taking the constant <span class="math inline">\(\frac{1}{N}\)</span> outside of the partial derivative, the RHS can be solved using the chain rule:</p>
<p><span class="math inline">\(\frac{\partial}{\partial \theta_{j}}J(\theta_{1},\theta_{0})=\frac{1}{N}\sum_{i=1}^{N}2(y(x_{i})-y_{i})*\frac{\partial}{\partial \theta_{j}}(y(x_{i})-y_{i})\)</span></p>
<p><span class="math inline">\(\frac{\partial}{\partial \theta_{j}}J(\theta_{1},\theta_{0})=\frac{1}{N}\sum_{i=1}^{N}2(y(x_{i})-y_{i})*(\frac{\partial}{\partial \theta_{j}}y(x_{i})\frac{\partial}{\partial \theta_{j}}y_{i})\)</span></p>
<p>The <span class="math inline">\(\frac{\partial}{\partial \theta_{j}}y_{i}\)</span> term evaluates to zero since <span class="math inline">\(y_{i}\)</span> is just a constant. Then:</p>
<p><span class="math inline">\(\frac{\partial}{\partial \theta_{j}}J(\theta_{1},\theta_{0})=\frac{2}{N}\sum_{i=1}^{N}(y(x_{i})-y_{i})*\frac{\partial}{\partial \theta_{j}}y(x_{i})\)</span></p>
<p>The above equation describes a sum of residuals and a partial derivative term. The partial derivative term must be solved with respect to both the slope and the intercept. Firstly, for the intercept:</p>
<p><span class="math inline">\(\frac{\partial}{\partial \theta_{0}}y(x_{i})=\frac{\partial}{\partial \theta_{0}}\theta_{i}x_{i}+\theta_{0}\)</span></p>
<p><span class="math inline">\(\frac{\partial}{\partial \theta_{0}}y(x_{i})=\frac{\partial}{\partial \theta_{0}}\theta_{i}x_{i}+\frac{\partial}{\partial \theta_{0}}\theta_{0}\)</span></p>
<p>On the RHS, first term is a constant and second term evaluates to 1.</p>
<p><span class="math inline">\(\frac{\partial}{\partial \theta_{0}}y(x_{i})=1\)</span></p>
<p>Solving the partial derivative for the slope then:</p>
<p><span class="math inline">\(\frac{\partial}{\partial \theta_{1}}y(x_{i})=\frac{\partial}{\partial \theta_{0}}\theta_{i}x_{i}+\frac{\partial}{\partial \theta_{0}}\theta_{0}\)</span></p>
<p>On the RHS, first term evaluates to <span class="math inline">\(x_{i}\)</span> and second term is a constant. So:</p>
<p><span class="math inline">\(\frac{\partial}{\partial \theta_{1}}y(x_{i})=x_{i}\)</span></p>
<p>Finally, we can substitute the respective partial derivatives in the equation from before:</p>
<p><span class="math inline">\(\frac{\partial}{\partial \theta_{0}}J(\theta_{1},\theta_{0})=\frac{2}{N}\sum_{i=1}^{N}y(x_{i})-y_{i}\)</span></p>
<p><span class="math inline">\(\frac{\partial}{\partial \theta_{1}}J(\theta_{1},\theta_{0})=\frac{2}{N}\sum_{i=1}^{N}y(x_{i})-y_{i}*x_{i}\)</span></p>
<p>In words, the derivative with respect to the intercept term <span class="math inline">\(\theta_{0}\)</span> is just the sum of residuals multipled by a constant. The deriative with respect to the slope <span class="math inline">\(\theta{1}\)</span> is the same, but with an extra <span class="math inline">\(x_{i}\)</span> term. For gradient descent, now that we’ve found the partial derivatives, we need to multiply these partial derivatives by a small factor (this factor is also called the <em>learning rate</em>) to find new values for the slope and the intercept.</p>
<p>Finally, let’s visualize this in R using the equation we’ve derived:</p>
<div class="sourceCode" id="cb482"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb482-1"><a href="digression-i-gradient-descent-with-r.html#cb482-1" tabindex="-1"></a><span class="fu">library</span>(tidyverse)</span>
<span id="cb482-2"><a href="digression-i-gradient-descent-with-r.html#cb482-2" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb482-3"><a href="digression-i-gradient-descent-with-r.html#cb482-3" tabindex="-1"></a>theta_0 <span class="ot">&lt;-</span> <span class="dv">5</span></span>
<span id="cb482-4"><a href="digression-i-gradient-descent-with-r.html#cb482-4" tabindex="-1"></a>theta_1 <span class="ot">&lt;-</span> <span class="dv">2</span></span>
<span id="cb482-5"><a href="digression-i-gradient-descent-with-r.html#cb482-5" tabindex="-1"></a>n_obs <span class="ot">&lt;-</span> <span class="dv">500</span></span>
<span id="cb482-6"><a href="digression-i-gradient-descent-with-r.html#cb482-6" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(n_obs)</span></code></pre></div>
<p>Here we’ve initialized the parameter values using random values (i.e., random initialization). We’ve also generated simulated data which we will fit a linear model on.</p>
<div class="sourceCode" id="cb483"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb483-1"><a href="digression-i-gradient-descent-with-r.html#cb483-1" tabindex="-1"></a>y <span class="ot">&lt;-</span> theta_1<span class="sc">*</span>x <span class="sc">+</span> theta_0 <span class="sc">+</span> <span class="fu">rnorm</span>(n_obs, <span class="dv">0</span>, <span class="dv">3</span>)</span>
<span id="cb483-2"><a href="digression-i-gradient-descent-with-r.html#cb483-2" tabindex="-1"></a>data <span class="ot">&lt;-</span> <span class="fu">tibble</span>(<span class="at">x =</span> x, <span class="at">y =</span> y)</span>
<span id="cb483-3"><a href="digression-i-gradient-descent-with-r.html#cb483-3" tabindex="-1"></a><span class="fu">ggplot</span>(data, <span class="fu">aes</span>(<span class="at">x =</span> x, <span class="at">y =</span> y)) <span class="sc">+</span> </span>
<span id="cb483-4"><a href="digression-i-gradient-descent-with-r.html#cb483-4" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">size =</span> <span class="dv">2</span>) <span class="sc">+</span> <span class="fu">theme_bw</span>() <span class="sc">+</span> </span>
<span id="cb483-5"><a href="digression-i-gradient-descent-with-r.html#cb483-5" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">&#39;Simulated Data&#39;</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-271-1.png" width="672" /></p>
<div class="sourceCode" id="cb484"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb484-1"><a href="digression-i-gradient-descent-with-r.html#cb484-1" tabindex="-1"></a><span class="fu">rm</span>(theta_0, theta_1)</span></code></pre></div>
<p>Now we fit a linear model using <code>lm()</code>, as seen previously:</p>
<div class="sourceCode" id="cb485"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb485-1"><a href="digression-i-gradient-descent-with-r.html#cb485-1" tabindex="-1"></a>ols <span class="ot">&lt;-</span> <span class="fu">lm</span>(y <span class="sc">~</span> x, <span class="at">data =</span> data)</span>
<span id="cb485-2"><a href="digression-i-gradient-descent-with-r.html#cb485-2" tabindex="-1"></a><span class="fu">summary</span>(ols)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = y ~ x, data = data)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -8.2671 -2.0105  0.0313  1.8922  8.2113 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)   4.9986     0.1357   36.84   &lt;2e-16 ***
## x             1.8381     0.1395   13.18   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 3.032 on 498 degrees of freedom
## Multiple R-squared:  0.2585, Adjusted R-squared:  0.257 
## F-statistic: 173.6 on 1 and 498 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>Let’s print out the model parameters:</p>
<div class="sourceCode" id="cb487"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb487-1"><a href="digression-i-gradient-descent-with-r.html#cb487-1" tabindex="-1"></a>ols<span class="sc">$</span>coefficients</span></code></pre></div>
<pre><code>## (Intercept)           x 
##    4.998597    1.838081</code></pre>
<p>Now we define a function to calculate the MSE:</p>
<div class="sourceCode" id="cb489"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb489-1"><a href="digression-i-gradient-descent-with-r.html#cb489-1" tabindex="-1"></a>cost_function <span class="ot">&lt;-</span> <span class="cf">function</span>(theta_0, theta_1, x, y){</span>
<span id="cb489-2"><a href="digression-i-gradient-descent-with-r.html#cb489-2" tabindex="-1"></a>  pred <span class="ot">&lt;-</span> theta_1<span class="sc">*</span>x <span class="sc">+</span> theta_0</span>
<span id="cb489-3"><a href="digression-i-gradient-descent-with-r.html#cb489-3" tabindex="-1"></a>  res_sq <span class="ot">&lt;-</span> (y <span class="sc">-</span> pred)<span class="sc">^</span><span class="dv">2</span></span>
<span id="cb489-4"><a href="digression-i-gradient-descent-with-r.html#cb489-4" tabindex="-1"></a>  res_ss <span class="ot">&lt;-</span> <span class="fu">sum</span>(res_sq)</span>
<span id="cb489-5"><a href="digression-i-gradient-descent-with-r.html#cb489-5" tabindex="-1"></a>  <span class="fu">return</span>(<span class="fu">mean</span>(res_ss))</span>
<span id="cb489-6"><a href="digression-i-gradient-descent-with-r.html#cb489-6" tabindex="-1"></a>}</span>
<span id="cb489-7"><a href="digression-i-gradient-descent-with-r.html#cb489-7" tabindex="-1"></a></span>
<span id="cb489-8"><a href="digression-i-gradient-descent-with-r.html#cb489-8" tabindex="-1"></a><span class="fu">cost_function</span>(<span class="at">theta_0 =</span> ols<span class="sc">$</span>coefficients[<span class="dv">1</span>][[<span class="dv">1</span>]], </span>
<span id="cb489-9"><a href="digression-i-gradient-descent-with-r.html#cb489-9" tabindex="-1"></a>         <span class="at">theta_1 =</span> ols<span class="sc">$</span>coefficients[<span class="dv">2</span>][[<span class="dv">1</span>]],</span>
<span id="cb489-10"><a href="digression-i-gradient-descent-with-r.html#cb489-10" tabindex="-1"></a>         <span class="at">x =</span> data<span class="sc">$</span>x, <span class="at">y =</span> data<span class="sc">$</span>y)</span></code></pre></div>
<pre><code>## [1] 4577.087</code></pre>
<div class="sourceCode" id="cb491"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb491-1"><a href="digression-i-gradient-descent-with-r.html#cb491-1" tabindex="-1"></a><span class="fu">sum</span>(<span class="fu">resid</span>(ols)<span class="sc">^</span><span class="dv">2</span>)</span></code></pre></div>
<pre><code>## [1] 4577.087</code></pre>
<p>Now, for the gradient descent: in the function below, <code>delta_theta_0</code> and <code>delta_theta_1</code> corresponds to the two derived equations earlier from the partial derivatives. We also define the learning rate <code>alpha</code> and the number of iterations <code>iter</code>.</p>
<div class="sourceCode" id="cb493"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb493-1"><a href="digression-i-gradient-descent-with-r.html#cb493-1" tabindex="-1"></a>gradient_desc <span class="ot">&lt;-</span> <span class="cf">function</span>(theta_0, theta_1, x, y){</span>
<span id="cb493-2"><a href="digression-i-gradient-descent-with-r.html#cb493-2" tabindex="-1"></a>  N <span class="ot">=</span> <span class="fu">length</span>(x)</span>
<span id="cb493-3"><a href="digression-i-gradient-descent-with-r.html#cb493-3" tabindex="-1"></a>  pred <span class="ot">&lt;-</span> theta_1<span class="sc">*</span>x <span class="sc">+</span> theta_0</span>
<span id="cb493-4"><a href="digression-i-gradient-descent-with-r.html#cb493-4" tabindex="-1"></a>  res <span class="ot">&lt;-</span> y <span class="sc">-</span> pred</span>
<span id="cb493-5"><a href="digression-i-gradient-descent-with-r.html#cb493-5" tabindex="-1"></a>  delta_theta_0 <span class="ot">&lt;-</span> (<span class="dv">2</span><span class="sc">/</span>N)<span class="sc">*</span><span class="fu">sum</span>(res)</span>
<span id="cb493-6"><a href="digression-i-gradient-descent-with-r.html#cb493-6" tabindex="-1"></a>  delta_theta_1 <span class="ot">&lt;-</span> (<span class="dv">2</span><span class="sc">/</span>N)<span class="sc">*</span><span class="fu">sum</span>(res<span class="sc">*</span>x)</span>
<span id="cb493-7"><a href="digression-i-gradient-descent-with-r.html#cb493-7" tabindex="-1"></a>  <span class="fu">return</span>(<span class="fu">c</span>(delta_theta_0, delta_theta_1))</span>
<span id="cb493-8"><a href="digression-i-gradient-descent-with-r.html#cb493-8" tabindex="-1"></a>}</span>
<span id="cb493-9"><a href="digression-i-gradient-descent-with-r.html#cb493-9" tabindex="-1"></a></span>
<span id="cb493-10"><a href="digression-i-gradient-descent-with-r.html#cb493-10" tabindex="-1"></a>alpha <span class="ot">&lt;-</span> <span class="fl">0.1</span></span>
<span id="cb493-11"><a href="digression-i-gradient-descent-with-r.html#cb493-11" tabindex="-1"></a>iter <span class="ot">&lt;-</span> <span class="dv">100</span></span></code></pre></div>
<p>Using the function <code>gradient_desc()</code>, we define a function to tweak the model parameters by the obtained partial derivatives scaled by <code>alpha</code>:</p>
<div class="sourceCode" id="cb494"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb494-1"><a href="digression-i-gradient-descent-with-r.html#cb494-1" tabindex="-1"></a>minimize_function <span class="ot">&lt;-</span> <span class="cf">function</span>(theta_0, theta_1, x, y, alpha){</span>
<span id="cb494-2"><a href="digression-i-gradient-descent-with-r.html#cb494-2" tabindex="-1"></a>  gd <span class="ot">&lt;-</span> <span class="fu">gradient_desc</span>(theta_0, theta_1, x, y)</span>
<span id="cb494-3"><a href="digression-i-gradient-descent-with-r.html#cb494-3" tabindex="-1"></a>  d_theta_0 <span class="ot">&lt;-</span> gd[<span class="dv">1</span>] <span class="sc">*</span> alpha</span>
<span id="cb494-4"><a href="digression-i-gradient-descent-with-r.html#cb494-4" tabindex="-1"></a>  d_theta_1 <span class="ot">&lt;-</span> gd[<span class="dv">2</span>] <span class="sc">*</span> alpha</span>
<span id="cb494-5"><a href="digression-i-gradient-descent-with-r.html#cb494-5" tabindex="-1"></a>  new_theta_0 <span class="ot">&lt;-</span> theta_0 <span class="sc">+</span> d_theta_0</span>
<span id="cb494-6"><a href="digression-i-gradient-descent-with-r.html#cb494-6" tabindex="-1"></a>  new_theta_1 <span class="ot">&lt;-</span> theta_1 <span class="sc">+</span> d_theta_1</span>
<span id="cb494-7"><a href="digression-i-gradient-descent-with-r.html#cb494-7" tabindex="-1"></a>  <span class="fu">return</span>(<span class="fu">c</span>(new_theta_0, new_theta_1))</span>
<span id="cb494-8"><a href="digression-i-gradient-descent-with-r.html#cb494-8" tabindex="-1"></a>}</span></code></pre></div>
<p>For the random initialization, we will use 0 and 0 and then progressively iterate through the gradient descent algorithm (<code>iter</code> number of times) and examine the parameter values at the end.</p>
<div class="sourceCode" id="cb495"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb495-1"><a href="digression-i-gradient-descent-with-r.html#cb495-1" tabindex="-1"></a>res <span class="ot">&lt;-</span> <span class="fu">list</span>()</span>
<span id="cb495-2"><a href="digression-i-gradient-descent-with-r.html#cb495-2" tabindex="-1"></a>res[[<span class="dv">1</span>]] <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">0</span>)</span>
<span id="cb495-3"><a href="digression-i-gradient-descent-with-r.html#cb495-3" tabindex="-1"></a></span>
<span id="cb495-4"><a href="digression-i-gradient-descent-with-r.html#cb495-4" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">2</span><span class="sc">:</span>iter){</span>
<span id="cb495-5"><a href="digression-i-gradient-descent-with-r.html#cb495-5" tabindex="-1"></a>  res[[i]] <span class="ot">&lt;-</span> <span class="fu">minimize_function</span>(</span>
<span id="cb495-6"><a href="digression-i-gradient-descent-with-r.html#cb495-6" tabindex="-1"></a>    res[[i<span class="dv">-1</span>]][<span class="dv">1</span>], res[[i<span class="dv">-1</span>]][<span class="dv">2</span>], data<span class="sc">$</span>x, data<span class="sc">$</span>y, alpha</span>
<span id="cb495-7"><a href="digression-i-gradient-descent-with-r.html#cb495-7" tabindex="-1"></a>  )</span>
<span id="cb495-8"><a href="digression-i-gradient-descent-with-r.html#cb495-8" tabindex="-1"></a>}</span>
<span id="cb495-9"><a href="digression-i-gradient-descent-with-r.html#cb495-9" tabindex="-1"></a></span>
<span id="cb495-10"><a href="digression-i-gradient-descent-with-r.html#cb495-10" tabindex="-1"></a>res <span class="ot">&lt;-</span> <span class="fu">lapply</span>(res, <span class="cf">function</span>(x) <span class="fu">as.data.frame</span>(<span class="fu">t</span>(x))) <span class="sc">%&gt;%</span> <span class="fu">bind_rows</span>()</span>
<span id="cb495-11"><a href="digression-i-gradient-descent-with-r.html#cb495-11" tabindex="-1"></a><span class="fu">colnames</span>(res) <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&#39;theta0&#39;</span>, <span class="st">&#39;theta1&#39;</span>)</span>
<span id="cb495-12"><a href="digression-i-gradient-descent-with-r.html#cb495-12" tabindex="-1"></a></span>
<span id="cb495-13"><a href="digression-i-gradient-descent-with-r.html#cb495-13" tabindex="-1"></a>loss <span class="ot">&lt;-</span> res <span class="sc">%&gt;%</span> <span class="fu">as_tibble</span>() <span class="sc">%&gt;%</span> <span class="fu">rowwise</span>() <span class="sc">%&gt;%</span></span>
<span id="cb495-14"><a href="digression-i-gradient-descent-with-r.html#cb495-14" tabindex="-1"></a>  <span class="fu">summarise</span>(<span class="at">mse =</span> <span class="fu">cost_function</span>(theta0, theta1, data<span class="sc">$</span>x, data<span class="sc">$</span>y))</span>
<span id="cb495-15"><a href="digression-i-gradient-descent-with-r.html#cb495-15" tabindex="-1"></a></span>
<span id="cb495-16"><a href="digression-i-gradient-descent-with-r.html#cb495-16" tabindex="-1"></a>res <span class="ot">&lt;-</span> res <span class="sc">%&gt;%</span> <span class="fu">bind_cols</span>(loss) <span class="sc">%&gt;%</span></span>
<span id="cb495-17"><a href="digression-i-gradient-descent-with-r.html#cb495-17" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">iteration =</span> <span class="fu">seq</span>(<span class="dv">1</span>, <span class="dv">100</span>)) <span class="sc">%&gt;%</span> <span class="fu">as_tibble</span>()</span>
<span id="cb495-18"><a href="digression-i-gradient-descent-with-r.html#cb495-18" tabindex="-1"></a></span>
<span id="cb495-19"><a href="digression-i-gradient-descent-with-r.html#cb495-19" tabindex="-1"></a>res</span></code></pre></div>
<pre><code>## # A tibble: 100 × 4
##    theta0 theta1    mse iteration
##     &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;     &lt;int&gt;
##  1   0     0     18985.         1
##  2   1.01  0.382 13725.         2
##  3   1.82  0.685 10385.         3
##  4   2.46  0.925  8265.         4
##  5   2.98  1.12   6918.         5
##  6   3.39  1.27   6064.         6
##  7   3.71  1.39   5521.         7
##  8   3.97  1.48   5176.         8
##  9   4.18  1.55   4958.         9
## 10   4.35  1.61   4819.        10
## # ℹ 90 more rows</code></pre>
<p>From the tibble above, we can see that the parameter values (slope and intercept) approach what we’ve gotten earlier from the linear model (1.83 and 4.99, respectively). The MSE also decreases over iterations, which we can visualize:</p>
<div class="sourceCode" id="cb497"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb497-1"><a href="digression-i-gradient-descent-with-r.html#cb497-1" tabindex="-1"></a><span class="fu">ggplot</span>(res, <span class="fu">aes</span>(<span class="at">x =</span> iteration, <span class="at">y =</span> mse)) <span class="sc">+</span> </span>
<span id="cb497-2"><a href="digression-i-gradient-descent-with-r.html#cb497-2" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">size =</span> <span class="dv">2</span>) <span class="sc">+</span> </span>
<span id="cb497-3"><a href="digression-i-gradient-descent-with-r.html#cb497-3" tabindex="-1"></a>  <span class="fu">theme_classic</span>() <span class="sc">+</span> <span class="fu">geom_line</span>(<span class="fu">aes</span>(<span class="at">group =</span> <span class="dv">1</span>)) <span class="sc">+</span></span>
<span id="cb497-4"><a href="digression-i-gradient-descent-with-r.html#cb497-4" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">&#39;Gradient descent over 100 iterations&#39;</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-278-1.png" width="672" /></p>
<p>As a visual aid to see how gradient descent is working, we can visualize the regression line at each iteration over our synthetic data. Here, the blue line indicates the regression line at the first iteration, which has a slope and intercept of 0 (since we initialized them at 0). Over the 100 iterations, the regression line approaches the line of best fit we obtained earlier using <code>lm()</code>. The green line indicates the line at the 100th iteration.</p>
<div class="sourceCode" id="cb498"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb498-1"><a href="digression-i-gradient-descent-with-r.html#cb498-1" tabindex="-1"></a><span class="fu">ggplot</span>(data, <span class="fu">aes</span>(<span class="at">x =</span> x, <span class="at">y =</span> y)) <span class="sc">+</span> </span>
<span id="cb498-2"><a href="digression-i-gradient-descent-with-r.html#cb498-2" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">size =</span> <span class="dv">2</span>) <span class="sc">+</span> </span>
<span id="cb498-3"><a href="digression-i-gradient-descent-with-r.html#cb498-3" tabindex="-1"></a>  <span class="fu">geom_abline</span>(<span class="fu">aes</span>(<span class="at">intercept =</span> theta0, <span class="at">slope =</span> theta1),</span>
<span id="cb498-4"><a href="digression-i-gradient-descent-with-r.html#cb498-4" tabindex="-1"></a>              <span class="at">data =</span> res, <span class="at">linewidth =</span> <span class="fl">0.5</span>, <span class="at">color =</span> <span class="st">&#39;red&#39;</span>) <span class="sc">+</span> </span>
<span id="cb498-5"><a href="digression-i-gradient-descent-with-r.html#cb498-5" tabindex="-1"></a>  <span class="fu">theme_classic</span>() <span class="sc">+</span> </span>
<span id="cb498-6"><a href="digression-i-gradient-descent-with-r.html#cb498-6" tabindex="-1"></a>  <span class="fu">geom_abline</span>(<span class="fu">aes</span>(<span class="at">intercept =</span> theta0, <span class="at">slope =</span> theta1), </span>
<span id="cb498-7"><a href="digression-i-gradient-descent-with-r.html#cb498-7" tabindex="-1"></a>              <span class="at">data =</span> res <span class="sc">%&gt;%</span> <span class="fu">slice_head</span>(), </span>
<span id="cb498-8"><a href="digression-i-gradient-descent-with-r.html#cb498-8" tabindex="-1"></a>              <span class="at">linewidth =</span> <span class="fl">0.5</span>, <span class="at">color =</span> <span class="st">&#39;blue&#39;</span>) <span class="sc">+</span> </span>
<span id="cb498-9"><a href="digression-i-gradient-descent-with-r.html#cb498-9" tabindex="-1"></a>  <span class="fu">geom_abline</span>(<span class="fu">aes</span>(<span class="at">intercept =</span> theta0, <span class="at">slope =</span> theta1), </span>
<span id="cb498-10"><a href="digression-i-gradient-descent-with-r.html#cb498-10" tabindex="-1"></a>              <span class="at">data =</span> res <span class="sc">%&gt;%</span> <span class="fu">slice_tail</span>(), </span>
<span id="cb498-11"><a href="digression-i-gradient-descent-with-r.html#cb498-11" tabindex="-1"></a>              <span class="at">linewidth =</span> <span class="fl">0.5</span>, <span class="at">color =</span> <span class="st">&#39;green&#39;</span>) <span class="sc">+</span></span>
<span id="cb498-12"><a href="digression-i-gradient-descent-with-r.html#cb498-12" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">&#39;Gradient descent over 100 iterations&#39;</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-279-1.png" width="672" /></p>

</div>
            </section>

          </div>
        </div>
      </div>
<a href="everyday-ml-regression.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="digression-ii-survival-analysis-with-r.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/USERNAME/REPO/edit/BRANCH/07-DigressionOne.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["_main.pdf", "_main.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
