[["index.html", "Everyday-R: Practical R for Data Science Chapter 1 Introduction 1.1 Preface 1.2 R syntax in this book 1.3 R packages commonly used in this book 1.4 Installing R packages 1.5 Code availability 1.6 Website hosting", " Everyday-R: Practical R for Data Science by Brian Jungmin Park Chapter 1 Introduction Note: this book is a work in progress. All source code for this project are available on my GitHub, which is linked in 1.4. This book serves as a collection of R Markdown files helps users in learning the practical syntax and usage of R for data science. Mainly, code snippets and workflow aimed at tackling everyday tasks in data science will be covered, which includes data cleaning, data wrangling, iterations, machine learning with caret, data visualization, and web app design using Shiny. Each broad topic will be split into chapters, though there will be some overlap. 1.1 Preface This book assumes readers have a basic grasp of R programming language. There are tons of useful resources that provides the first few steps into programming with R and I hope this book can serve as a useful complement to using R - a pocket reference of sorts! It depends on who you ask, but many users emphasize the importance of learning the syntax of base R before diving into commonly used packages like dplyr and data.table. It is definitely a good idea to get the hang of base R if you’re developing an app or a package for example - this would lessen the number of dependencies your program has! However, similarly to pandas in Python, the popularity of dplyr and associated packages within the tidyverse suite has soared and you wouldn’t be surprised to see tidyverse solutions as the top answers in forums like Stack Overflow (this can be frustrating if you’re a base R purist). Using tidyverse for data science can definitely make your life easy - I find their syntax more pretty intuitive - but I’d like to sit on the fence on the base R vs. tidyverse debate; you should know both! For that reason, in this book I will try to use both interchangeably. 1.2 R syntax in this book Code chunks will be presented in a typical Markdown format as such, with the code output below: runif(n = 20, min = 0, max = 100) ## [1] 0.6287769 41.9417952 93.2573413 85.4930268 29.6407180 ## [6] 48.9640196 76.2243000 19.8012127 14.1718541 30.4860297 ## [11] 57.6782228 65.5361902 34.0591668 24.3310946 28.7245092 ## [16] 39.7921209 32.2284175 61.7982466 34.1228876 94.9914141 When using commands outside of base R, the loading of the parent package will be explicitly shown to avoid confusion: library(microbenchmark) microbenchmark::microbenchmark(runif(n = 20, min = 0, max = 100)) ## Unit: nanoseconds ## expr min lq mean median uq ## runif(n = 20, min = 0, max = 100) 902 1025 1249.68 1107 1189 ## max neval ## 10004 100 Typically in longer chains of code, I will use %&gt;% from magrittr as a pipe. This is usually standard practice in code using packages from the tidyverse so it’s a good habit to start using it. However, keep in mind - as of the recent R version (shown below), there is a native R pipe |&gt; which behave almost - but not always - in a similar fashion. Finally, here is the R version I am currently using: version ## _ ## platform aarch64-apple-darwin20 ## arch aarch64 ## os darwin20 ## system aarch64, darwin20 ## status ## major 4 ## minor 3.1 ## year 2023 ## month 06 ## day 16 ## svn rev 84548 ## language R ## version.string R version 4.3.1 (2023-06-16) ## nickname Beagle Scouts 1.3 R packages commonly used in this book tidyverse: a collection of packages for data science, including dplyr, purrr, stringr, forcats, readr, and ggplot. caret: package for implementation of machine learning models, with support for algorithms such as ranger, rpart, xgbTree, and svmLinear. mlbench: package for benchmarks and datasets in machine learning. broom: package for summarizing of model estimates. ggpubr: package for publication-ready data visualizations. Shiny: package for implementation and designing of interactive web apps. 1.4 Installing R packages R packages found in this book are available on CRAN and thus can be installed simply by running install.packages(). For packages not on CRAN (or if you want to download developmental versions of a package), you can install packages straight from a GitHub repository by running devtools::install_github(). 1.5 Code availability All code used to compile this book as well as the individual markdown files are available on my repository here 1.6 Website hosting This book is hosted on the shinyapps server, deployed with the R package rsconnect. The markdown files are compiled in this book format using the R package bookdown. "],["everyday-data-wrangling.html", "Chapter 2 Everyday data wrangling 2.1 Renaming column headers 2.2 Grouped operations 2.3 Data transformation 2.4 Joining and separating character columns 2.5 Filtering rows 2.6 Subsetting columns based on strings 2.7 Long and wide data formats 2.8 Trimming strings 2.9 Iterating over list of dataframes 2.10 Rowwise operations 2.11 Conditional transformation 2.12 Missing values 2.13 Joining dataframes", " Chapter 2 Everyday data wrangling Suppose there exists a test dataset and the user is tasked with proving their proficiency in exploratory data analysis (EDA). Alternatively, the user may be handed a piece of data (e.g., maybe an Excel sheet) and is asked to run some preliminary analysis. Prior to jumping straight into EDA and statistical modeling, it’s a good idea to transform the original data into a desirable format. This includes, but is not limited to, missing data imputation, scalar transformation, feature selection, group aggregation, and data filtering. The following notebook outlines the fundamentals of data wrangling, using practical base R and tidyverse solutions interchangeably. library(tidyverse) 2.1 Renaming column headers The first few steps in data analysis is to gain intuition around the data at hand - typically, a m x n dataset is structured in a way such that there are m records (also referred to as instances or samples) and m features (also referred to as predictors). Getting a good sense and understanding of the features we’re working with often requires cleaning the data before jumping into things like feature selection and feature engineering. Suppose we have a 32 x 11 dataset mtcars where rownames correspond to the car model: data(mtcars) mtcars &lt;- mtcars %&gt;% as_tibble(rownames = &#39;CAR&#39;) There are 11 columns here and since this is a toy dataset, it is already neat and tidy. However, sometimes we may need to rename the columns to trim specific strings or to make downstream analysis easier. Renaming column headers with dplyr::rename() is as simple as rename(new_name = old_name), but with rename_with() there’s more flexibility: mtcars %&gt;% rename_with(toupper) ## # A tibble: 32 × 12 ## CAR MPG CYL DISP HP DRAT WT QSEC VS AM ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Mazda RX4 21 6 160 110 3.9 2.62 16.5 0 1 ## 2 Mazda RX… 21 6 160 110 3.9 2.88 17.0 0 1 ## 3 Datsun 7… 22.8 4 108 93 3.85 2.32 18.6 1 1 ## 4 Hornet 4… 21.4 6 258 110 3.08 3.22 19.4 1 0 ## 5 Hornet S… 18.7 8 360 175 3.15 3.44 17.0 0 0 ## 6 Valiant 18.1 6 225 105 2.76 3.46 20.2 1 0 ## 7 Duster 3… 14.3 8 360 245 3.21 3.57 15.8 0 0 ## 8 Merc 240D 24.4 4 147. 62 3.69 3.19 20 1 0 ## 9 Merc 230 22.8 4 141. 95 3.92 3.15 22.9 1 0 ## 10 Merc 280 19.2 6 168. 123 3.92 3.44 18.3 1 0 ## # ℹ 22 more rows ## # ℹ 2 more variables: GEAR &lt;dbl&gt;, CARB &lt;dbl&gt; You can pass any function within rename_with() to alter the column headers and use the .cols = argument to define which columns you’d apply the function to: mtcars %&gt;% rename_with(function(x) paste0(x, &quot;_1&quot;), .cols = -1) ## # A tibble: 32 × 12 ## CAR mpg_1 cyl_1 disp_1 hp_1 drat_1 wt_1 qsec_1 vs_1 am_1 ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Mazda… 21 6 160 110 3.9 2.62 16.5 0 1 ## 2 Mazda… 21 6 160 110 3.9 2.88 17.0 0 1 ## 3 Datsu… 22.8 4 108 93 3.85 2.32 18.6 1 1 ## 4 Horne… 21.4 6 258 110 3.08 3.22 19.4 1 0 ## 5 Horne… 18.7 8 360 175 3.15 3.44 17.0 0 0 ## 6 Valia… 18.1 6 225 105 2.76 3.46 20.2 1 0 ## 7 Duste… 14.3 8 360 245 3.21 3.57 15.8 0 0 ## 8 Merc … 24.4 4 147. 62 3.69 3.19 20 1 0 ## 9 Merc … 22.8 4 141. 95 3.92 3.15 22.9 1 0 ## 10 Merc … 19.2 6 168. 123 3.92 3.44 18.3 1 0 ## # ℹ 22 more rows ## # ℹ 2 more variables: gear_1 &lt;dbl&gt;, carb_1 &lt;dbl&gt; Alternatively with base R, you can call the names attribute and alter that directly: names(mtcars)[-1] &lt;- paste0(names(mtcars[-1]), &#39;_1&#39;) # not run 2.2 Grouped operations Grouped operations are most useful when you’re working with data where there is a categorical variable. Grouped summaries are powerful tools easily done with dplyr::summarise(): mtcars %&gt;% group_by(cyl) %&gt;% summarise(avg_mpg = mean(mpg)) ## # A tibble: 3 × 2 ## cyl avg_mpg ## &lt;dbl&gt; &lt;dbl&gt; ## 1 4 26.7 ## 2 6 19.7 ## 3 8 15.1 Using multiple columns at once and renaming the output columns: mtcars %&gt;% group_by(cyl) %&gt;% summarise(across(c(mpg, disp, hp), mean, .names = &quot;mean_{col}&quot;)) ## # A tibble: 3 × 4 ## cyl mean_mpg mean_disp mean_hp ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 4 26.7 105. 82.6 ## 2 6 19.7 183. 122. ## 3 8 15.1 353. 209. mtcars %&gt;% group_by(cyl) %&gt;% summarise(across(where(is.numeric), mean, .names = &quot;mean_{col}&quot;)) ## # A tibble: 3 × 11 ## cyl mean_mpg mean_disp mean_hp mean_drat mean_wt mean_qsec ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 4 26.7 105. 82.6 4.07 2.29 19.1 ## 2 6 19.7 183. 122. 3.59 3.12 18.0 ## 3 8 15.1 353. 209. 3.23 4.00 16.8 ## # ℹ 4 more variables: mean_vs &lt;dbl&gt;, mean_am &lt;dbl&gt;, ## # mean_gear &lt;dbl&gt;, mean_carb &lt;dbl&gt; Alternatively, in base R, you can use aggregate() with the ~; you can read this as grouping mpg by cyl where by corresponds to the ~: aggregate(mpg ~ cyl, data = mtcars, FUN = mean) ## cyl mpg ## 1 4 26.66364 ## 2 6 19.74286 ## 3 8 15.10000 Since aggregate() uses the formula syntax (~), using the . placeholder on the LHS calls all variables not called in the RHS: aggregate(. ~ cyl, data = mtcars, FUN = mean) When calculating the size of the groups, you can use n(): mtcars %&gt;% group_by(cyl) %&gt;% summarise(count = n()) ## # A tibble: 3 × 2 ## cyl count ## &lt;dbl&gt; &lt;int&gt; ## 1 4 11 ## 2 6 7 ## 3 8 14 The n() function is useful when filtering by groups of size greater than a certain value: mtcars %&gt;% group_by(cyl) %&gt;% filter(n() &gt; 7) %&gt;% ungroup() ## # A tibble: 25 × 12 ## CAR mpg cyl disp hp drat wt qsec vs am ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Datsun 7… 22.8 4 108 93 3.85 2.32 18.6 1 1 ## 2 Hornet S… 18.7 8 360 175 3.15 3.44 17.0 0 0 ## 3 Duster 3… 14.3 8 360 245 3.21 3.57 15.8 0 0 ## 4 Merc 240D 24.4 4 147. 62 3.69 3.19 20 1 0 ## 5 Merc 230 22.8 4 141. 95 3.92 3.15 22.9 1 0 ## 6 Merc 450… 16.4 8 276. 180 3.07 4.07 17.4 0 0 ## 7 Merc 450… 17.3 8 276. 180 3.07 3.73 17.6 0 0 ## 8 Merc 450… 15.2 8 276. 180 3.07 3.78 18 0 0 ## 9 Cadillac… 10.4 8 472 205 2.93 5.25 18.0 0 0 ## 10 Lincoln … 10.4 8 460 215 3 5.42 17.8 0 0 ## # ℹ 15 more rows ## # ℹ 2 more variables: gear &lt;dbl&gt;, carb &lt;dbl&gt; Using n() gives us access to finding the group sizes using tally() or count(): mtcars %&gt;% group_by(cyl) %&gt;% tally() ## # A tibble: 3 × 2 ## cyl n ## &lt;dbl&gt; &lt;int&gt; ## 1 4 11 ## 2 6 7 ## 3 8 14 We can label encode (i.e., give a unique integer identifier for the current groups) using cur_group_id(): mtcars %&gt;% group_by(cyl) %&gt;% mutate(ID = cur_group_id()) %&gt;% select(ID, cyl) ## # A tibble: 32 × 2 ## # Groups: cyl [3] ## ID cyl ## &lt;int&gt; &lt;dbl&gt; ## 1 2 6 ## 2 2 6 ## 3 1 4 ## 4 2 6 ## 5 3 8 ## 6 2 6 ## 7 3 8 ## 8 1 4 ## 9 1 4 ## 10 2 6 ## # ℹ 22 more rows 2.3 Data transformation Sometimes it’s useful to transform numeric columns; this is particularly helpful when performing feature transformations (e.g., logarithmic, absolute value) or creating new columns entirely from preexisting columns. mtcars %&gt;% mutate(mpg = mpg*2) ## # A tibble: 32 × 12 ## CAR mpg cyl disp hp drat wt qsec vs am ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Mazda RX4 42 6 160 110 3.9 2.62 16.5 0 1 ## 2 Mazda RX… 42 6 160 110 3.9 2.88 17.0 0 1 ## 3 Datsun 7… 45.6 4 108 93 3.85 2.32 18.6 1 1 ## 4 Hornet 4… 42.8 6 258 110 3.08 3.22 19.4 1 0 ## 5 Hornet S… 37.4 8 360 175 3.15 3.44 17.0 0 0 ## 6 Valiant 36.2 6 225 105 2.76 3.46 20.2 1 0 ## 7 Duster 3… 28.6 8 360 245 3.21 3.57 15.8 0 0 ## 8 Merc 240D 48.8 4 147. 62 3.69 3.19 20 1 0 ## 9 Merc 230 45.6 4 141. 95 3.92 3.15 22.9 1 0 ## 10 Merc 280 38.4 6 168. 123 3.92 3.44 18.3 1 0 ## # ℹ 22 more rows ## # ℹ 2 more variables: gear &lt;dbl&gt;, carb &lt;dbl&gt; Using across() allows you to specifically select which columns you’d like to apply a given function on. The first part of the across() argument should therefore specify the columns of interest. Here, where() is particularly useful as it returns indices (kind of like grep()) after evaluating a logical expression, such as is.numeric(). The second part of across() takes the function you’d like to apply, while the optional .names argument allows you to create new columns rather than overwrite the old ones. mtcars %&gt;% mutate(across(where(is.numeric), function(x) x*2, .names = &quot;double_{col}&quot;)) ## # A tibble: 32 × 23 ## CAR mpg cyl disp hp drat wt qsec vs am ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Mazda RX4 21 6 160 110 3.9 2.62 16.5 0 1 ## 2 Mazda RX… 21 6 160 110 3.9 2.88 17.0 0 1 ## 3 Datsun 7… 22.8 4 108 93 3.85 2.32 18.6 1 1 ## 4 Hornet 4… 21.4 6 258 110 3.08 3.22 19.4 1 0 ## 5 Hornet S… 18.7 8 360 175 3.15 3.44 17.0 0 0 ## 6 Valiant 18.1 6 225 105 2.76 3.46 20.2 1 0 ## 7 Duster 3… 14.3 8 360 245 3.21 3.57 15.8 0 0 ## 8 Merc 240D 24.4 4 147. 62 3.69 3.19 20 1 0 ## 9 Merc 230 22.8 4 141. 95 3.92 3.15 22.9 1 0 ## 10 Merc 280 19.2 6 168. 123 3.92 3.44 18.3 1 0 ## # ℹ 22 more rows ## # ℹ 13 more variables: gear &lt;dbl&gt;, carb &lt;dbl&gt;, double_mpg &lt;dbl&gt;, ## # double_cyl &lt;dbl&gt;, double_disp &lt;dbl&gt;, double_hp &lt;dbl&gt;, ## # double_drat &lt;dbl&gt;, double_wt &lt;dbl&gt;, double_qsec &lt;dbl&gt;, ## # double_vs &lt;dbl&gt;, double_am &lt;dbl&gt;, double_gear &lt;dbl&gt;, ## # double_carb &lt;dbl&gt; mtcars %&gt;% mutate(across(where(is.character), function(x) gsub(&#39;[a-z]$&#39;, &#39;&#39;, x))) %&gt;% select(CAR) ## # A tibble: 32 × 1 ## CAR ## &lt;chr&gt; ## 1 Mazda RX4 ## 2 Mazda RX4 Wa ## 3 Datsun 710 ## 4 Hornet 4 Driv ## 5 Hornet Sportabou ## 6 Valian ## 7 Duster 360 ## 8 Merc 240D ## 9 Merc 230 ## 10 Merc 280 ## # ℹ 22 more rows Conditional mutate() can also be done using mutate_if(), though the _if() functions, along with variants _at() and _all() have been effectively replaced by across(), as seen above. You can overwrite the existing columns completely; also keep in mind that you can pass any function within across() as such: mtcars %&gt;% mutate(across(c(mpg, disp), function(x) x*3)) ## # A tibble: 32 × 12 ## CAR mpg cyl disp hp drat wt qsec vs am ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Mazda RX4 63 6 480 110 3.9 2.62 16.5 0 1 ## 2 Mazda RX… 63 6 480 110 3.9 2.88 17.0 0 1 ## 3 Datsun 7… 68.4 4 324 93 3.85 2.32 18.6 1 1 ## 4 Hornet 4… 64.2 6 774 110 3.08 3.22 19.4 1 0 ## 5 Hornet S… 56.1 8 1080 175 3.15 3.44 17.0 0 0 ## 6 Valiant 54.3 6 675 105 2.76 3.46 20.2 1 0 ## 7 Duster 3… 42.9 8 1080 245 3.21 3.57 15.8 0 0 ## 8 Merc 240D 73.2 4 440. 62 3.69 3.19 20 1 0 ## 9 Merc 230 68.4 4 422. 95 3.92 3.15 22.9 1 0 ## 10 Merc 280 57.6 6 503. 123 3.92 3.44 18.3 1 0 ## # ℹ 22 more rows ## # ℹ 2 more variables: gear &lt;dbl&gt;, carb &lt;dbl&gt; Iterating across the columns using purrr::modify() instead: mtcars %&gt;% modify_if(is.numeric, ~ .x*3) ## # A tibble: 32 × 12 ## CAR mpg cyl disp hp drat wt qsec vs am ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Mazda RX4 63 18 480 330 11.7 7.86 49.4 0 3 ## 2 Mazda RX… 63 18 480 330 11.7 8.62 51.1 0 3 ## 3 Datsun 7… 68.4 12 324 279 11.6 6.96 55.8 3 3 ## 4 Hornet 4… 64.2 18 774 330 9.24 9.64 58.3 3 0 ## 5 Hornet S… 56.1 24 1080 525 9.45 10.3 51.1 0 0 ## 6 Valiant 54.3 18 675 315 8.28 10.4 60.7 3 0 ## 7 Duster 3… 42.9 24 1080 735 9.63 10.7 47.5 0 0 ## 8 Merc 240D 73.2 12 440. 186 11.1 9.57 60 3 0 ## 9 Merc 230 68.4 12 422. 285 11.8 9.45 68.7 3 0 ## 10 Merc 280 57.6 18 503. 369 11.8 10.3 54.9 3 0 ## # ℹ 22 more rows ## # ℹ 2 more variables: gear &lt;dbl&gt;, carb &lt;dbl&gt; Mutating columns of characters is also straightforward; this is helpful when working with messy columns that require cleaning. mtcars %&gt;% mutate(CAR = gsub(&#39;Merc&#39;, &#39;Mercedes&#39;, CAR)) ## # A tibble: 32 × 12 ## CAR mpg cyl disp hp drat wt qsec vs am ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Mazda RX4 21 6 160 110 3.9 2.62 16.5 0 1 ## 2 Mazda RX… 21 6 160 110 3.9 2.88 17.0 0 1 ## 3 Datsun 7… 22.8 4 108 93 3.85 2.32 18.6 1 1 ## 4 Hornet 4… 21.4 6 258 110 3.08 3.22 19.4 1 0 ## 5 Hornet S… 18.7 8 360 175 3.15 3.44 17.0 0 0 ## 6 Valiant 18.1 6 225 105 2.76 3.46 20.2 1 0 ## 7 Duster 3… 14.3 8 360 245 3.21 3.57 15.8 0 0 ## 8 Mercedes… 24.4 4 147. 62 3.69 3.19 20 1 0 ## 9 Mercedes… 22.8 4 141. 95 3.92 3.15 22.9 1 0 ## 10 Mercedes… 19.2 6 168. 123 3.92 3.44 18.3 1 0 ## # ℹ 22 more rows ## # ℹ 2 more variables: gear &lt;dbl&gt;, carb &lt;dbl&gt; mtcars %&gt;% mutate(CAR = gsub(&#39; &#39;, &#39;_&#39;, CAR)) %&gt;% mutate(across(CAR, tolower)) ## # A tibble: 32 × 12 ## CAR mpg cyl disp hp drat wt qsec vs am ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 mazda_rx4 21 6 160 110 3.9 2.62 16.5 0 1 ## 2 mazda_rx… 21 6 160 110 3.9 2.88 17.0 0 1 ## 3 datsun_7… 22.8 4 108 93 3.85 2.32 18.6 1 1 ## 4 hornet_4… 21.4 6 258 110 3.08 3.22 19.4 1 0 ## 5 hornet_s… 18.7 8 360 175 3.15 3.44 17.0 0 0 ## 6 valiant 18.1 6 225 105 2.76 3.46 20.2 1 0 ## 7 duster_3… 14.3 8 360 245 3.21 3.57 15.8 0 0 ## 8 merc_240d 24.4 4 147. 62 3.69 3.19 20 1 0 ## 9 merc_230 22.8 4 141. 95 3.92 3.15 22.9 1 0 ## 10 merc_280 19.2 6 168. 123 3.92 3.44 18.3 1 0 ## # ℹ 22 more rows ## # ℹ 2 more variables: gear &lt;dbl&gt;, carb &lt;dbl&gt; 2.4 Joining and separating character columns For the CAR column comprised of strings, you can separate the individual strings into multiple columns; the extra = 'merge' argument tells the function to separate the string based on the first instance of ” ” and merge the rest: mtcars %&gt;% separate(CAR, c(&#39;Brand&#39;, &#39;Model&#39;), sep = &quot; &quot;, extra = &#39;merge&#39;) ## # A tibble: 32 × 13 ## Brand Model mpg cyl disp hp drat wt qsec vs ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Mazda RX4 21 6 160 110 3.9 2.62 16.5 0 ## 2 Mazda RX4 Wag 21 6 160 110 3.9 2.88 17.0 0 ## 3 Datsun 710 22.8 4 108 93 3.85 2.32 18.6 1 ## 4 Hornet 4 Drive 21.4 6 258 110 3.08 3.22 19.4 1 ## 5 Hornet Sporta… 18.7 8 360 175 3.15 3.44 17.0 0 ## 6 Valiant &lt;NA&gt; 18.1 6 225 105 2.76 3.46 20.2 1 ## 7 Duster 360 14.3 8 360 245 3.21 3.57 15.8 0 ## 8 Merc 240D 24.4 4 147. 62 3.69 3.19 20 1 ## 9 Merc 230 22.8 4 141. 95 3.92 3.15 22.9 1 ## 10 Merc 280 19.2 6 168. 123 3.92 3.44 18.3 1 ## # ℹ 22 more rows ## # ℹ 3 more variables: am &lt;dbl&gt;, gear &lt;dbl&gt;, carb &lt;dbl&gt; Chaining multiple tidyverse verbs can be done using the pipe %&gt;%; this %&gt;% symbol is part of the magrittr package, which is loaded with tidyverse. Recently as of R version 4.1.0, the native pipe |&gt; was introduced, which behaves in mostly a similar manner as the magrittr pipe (though there are a few important differences). Here, I will stick to the %&gt;% notation. mtcars %&gt;% separate(CAR, c(&#39;Brand&#39;, &#39;Model&#39;), sep = &quot; &quot;, extra = &#39;merge&#39;) %&gt;% group_by(Brand) %&gt;% summarise(count = n(), mean_mpg = mean(mpg)) %&gt;% arrange(desc(count)) ## # A tibble: 22 × 3 ## Brand count mean_mpg ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Merc 7 19.0 ## 2 Fiat 2 29.8 ## 3 Hornet 2 20.0 ## 4 Mazda 2 21 ## 5 Toyota 2 27.7 ## 6 AMC 1 15.2 ## 7 Cadillac 1 10.4 ## 8 Camaro 1 13.3 ## 9 Chrysler 1 14.7 ## 10 Datsun 1 22.8 ## # ℹ 12 more rows The pipe notation is frequently used between tidyverse verbs but you can also use it in some base R operations as well. Separation of columns by strings, using base R instead involves using strsplit() which outputs a list and therefore we need to unpack it afterwards: mtcars$Brand &lt;- unlist( lapply(strsplit(mtcars$CAR, split = &#39; &#39;), function(x) x[1]) ) # not run mtcars$Model &lt;- unlist( lapply(strsplit(mtcars$CAR, split = &#39; &#39;), function(x) x[2]) ) # not run The opposite of separate() is unite() which combines columns into a single column: data(iris) iris &lt;- as_tibble(iris) iris %&gt;% unite(&#39;Petal.Dimensions&#39;, c(`Petal.Length`, `Petal.Width`), sep = &quot; x &quot;, remove = FALSE) ## # A tibble: 150 × 6 ## Sepal.Length Sepal.Width Petal.Dimensions Petal.Length ## &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 5.1 3.5 1.4 x 0.2 1.4 ## 2 4.9 3 1.4 x 0.2 1.4 ## 3 4.7 3.2 1.3 x 0.2 1.3 ## 4 4.6 3.1 1.5 x 0.2 1.5 ## 5 5 3.6 1.4 x 0.2 1.4 ## 6 5.4 3.9 1.7 x 0.4 1.7 ## 7 4.6 3.4 1.4 x 0.3 1.4 ## 8 5 3.4 1.5 x 0.2 1.5 ## 9 4.4 2.9 1.4 x 0.2 1.4 ## 10 4.9 3.1 1.5 x 0.1 1.5 ## # ℹ 140 more rows ## # ℹ 2 more variables: Petal.Width &lt;dbl&gt;, Species &lt;fct&gt; In base R, we create a new column using the $ notation: iris$Petal.Dimensions &lt;- paste(iris$Petal.Length, iris$Petal.Width, sep = &quot; x &quot;) # not run 2.5 Filtering rows Typically, filtering involves a condition based on a column to select the corresponding rows. Here I am combining stringr::str_detect() with dplyr::filter() since the column I am filtering on is a character vector. mtcars %&gt;% filter(str_detect(CAR, &#39;Mazda&#39;)) ## # A tibble: 2 × 12 ## CAR mpg cyl disp hp drat wt qsec vs am ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Mazda RX4 21 6 160 110 3.9 2.62 16.5 0 1 ## 2 Mazda RX4… 21 6 160 110 3.9 2.88 17.0 0 1 ## # ℹ 2 more variables: gear &lt;dbl&gt;, carb &lt;dbl&gt; This function accepts logical operators and regex as well: mtcars %&gt;% filter(str_detect(CAR, &#39;Mazda | Merc&#39;)) ## # A tibble: 2 × 12 ## CAR mpg cyl disp hp drat wt qsec vs am ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Mazda RX4 21 6 160 110 3.9 2.62 16.5 0 1 ## 2 Mazda RX4… 21 6 160 110 3.9 2.88 17.0 0 1 ## # ℹ 2 more variables: gear &lt;dbl&gt;, carb &lt;dbl&gt; Ignoring upper/lower case distinction using regex(..., ignore_case = TRUE): mtcars %&gt;% filter(str_detect(CAR, regex(&#39;mazda&#39;, ignore_case = TRUE))) ## # A tibble: 2 × 12 ## CAR mpg cyl disp hp drat wt qsec vs am ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Mazda RX4 21 6 160 110 3.9 2.62 16.5 0 1 ## 2 Mazda RX4… 21 6 160 110 3.9 2.88 17.0 0 1 ## # ℹ 2 more variables: gear &lt;dbl&gt;, carb &lt;dbl&gt; Alternatively, using base R using grepl(): mtcars[grepl(&#39;Mazda&#39;, mtcars$CAR),] # not run mtcars[grepl(&#39;Mazda|Merc&#39;, mtcars$CAR),] # not run Using tolower() to make sure we’re on the same page in regards to case: mtcars[grepl(tolower(&#39;Mazda&#39;), tolower(mtcars$CAR)),] ## # A tibble: 2 × 12 ## CAR mpg cyl disp hp drat wt qsec vs am ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Mazda RX4 21 6 160 110 3.9 2.62 16.5 0 1 ## 2 Mazda RX4… 21 6 160 110 3.9 2.88 17.0 0 1 ## # ℹ 2 more variables: gear &lt;dbl&gt;, carb &lt;dbl&gt; Filtering rows based on a numeric column is also trivial: mtcars %&gt;% filter(between(mpg, 18, 20)) ## # A tibble: 5 × 12 ## CAR mpg cyl disp hp drat wt qsec vs am ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Hornet Sp… 18.7 8 360 175 3.15 3.44 17.0 0 0 ## 2 Valiant 18.1 6 225 105 2.76 3.46 20.2 1 0 ## 3 Merc 280 19.2 6 168. 123 3.92 3.44 18.3 1 0 ## 4 Pontiac F… 19.2 8 400 175 3.08 3.84 17.0 0 0 ## 5 Ferrari D… 19.7 6 145 175 3.62 2.77 15.5 0 1 ## # ℹ 2 more variables: gear &lt;dbl&gt;, carb &lt;dbl&gt; mtcars %&gt;% filter(cyl %in% c(6, 8)) ## # A tibble: 21 × 12 ## CAR mpg cyl disp hp drat wt qsec vs am ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Mazda RX4 21 6 160 110 3.9 2.62 16.5 0 1 ## 2 Mazda RX… 21 6 160 110 3.9 2.88 17.0 0 1 ## 3 Hornet 4… 21.4 6 258 110 3.08 3.22 19.4 1 0 ## 4 Hornet S… 18.7 8 360 175 3.15 3.44 17.0 0 0 ## 5 Valiant 18.1 6 225 105 2.76 3.46 20.2 1 0 ## 6 Duster 3… 14.3 8 360 245 3.21 3.57 15.8 0 0 ## 7 Merc 280 19.2 6 168. 123 3.92 3.44 18.3 1 0 ## 8 Merc 280C 17.8 6 168. 123 3.92 3.44 18.9 1 0 ## 9 Merc 450… 16.4 8 276. 180 3.07 4.07 17.4 0 0 ## 10 Merc 450… 17.3 8 276. 180 3.07 3.73 17.6 0 0 ## # ℹ 11 more rows ## # ℹ 2 more variables: gear &lt;dbl&gt;, carb &lt;dbl&gt; A useful function is the negation of %in%, which we can define manually: `%nin%` &lt;- Negate(`%in%`) mtcars %&gt;% filter(cyl %nin% c(6, 8)) ## # A tibble: 11 × 12 ## CAR mpg cyl disp hp drat wt qsec vs am ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Datsun 7… 22.8 4 108 93 3.85 2.32 18.6 1 1 ## 2 Merc 240D 24.4 4 147. 62 3.69 3.19 20 1 0 ## 3 Merc 230 22.8 4 141. 95 3.92 3.15 22.9 1 0 ## 4 Fiat 128 32.4 4 78.7 66 4.08 2.2 19.5 1 1 ## 5 Honda Ci… 30.4 4 75.7 52 4.93 1.62 18.5 1 1 ## 6 Toyota C… 33.9 4 71.1 65 4.22 1.84 19.9 1 1 ## 7 Toyota C… 21.5 4 120. 97 3.7 2.46 20.0 1 0 ## 8 Fiat X1-9 27.3 4 79 66 4.08 1.94 18.9 1 1 ## 9 Porsche … 26 4 120. 91 4.43 2.14 16.7 0 1 ## 10 Lotus Eu… 30.4 4 95.1 113 3.77 1.51 16.9 1 1 ## 11 Volvo 14… 21.4 4 121 109 4.11 2.78 18.6 1 1 ## # ℹ 2 more variables: gear &lt;dbl&gt;, carb &lt;dbl&gt; In base R we can use which() instead to subset: mtcars[which(mtcars$mpg &gt; 18 &amp; mtcars$mpg &lt; 20),] # not run mtcars[which(mtcars$cyl %in% c(6,8)),] # not run 2.6 Subsetting columns based on strings Subsetting columns typically involves using square brackets [ ] in base R, but can be done easily with dplyr::select(); select() supports multiple selector functions such as starts_with(), ends_with(), contains(), and match(). The selector everything() also exists, which is used to - you guessed it - select every column. As a word of caution: sometimes it’s good practice to explicitly state select() we’re calling is from dplyr (i.e., dplyr::select()) to avoid confusion when working with many different packages at once that could potentially introduce conflicts. mtcars %&gt;% select(contains(&#39;m&#39;)) ## # A tibble: 32 × 2 ## mpg am ## &lt;dbl&gt; &lt;dbl&gt; ## 1 21 1 ## 2 21 1 ## 3 22.8 1 ## 4 21.4 0 ## 5 18.7 0 ## 6 18.1 0 ## 7 14.3 0 ## 8 24.4 0 ## 9 22.8 0 ## 10 19.2 0 ## # ℹ 22 more rows You can use multiple helper functions at once: mtcars %&gt;% select(starts_with(&#39;m&#39;), ends_with(&#39;c&#39;)) ## # A tibble: 32 × 2 ## mpg qsec ## &lt;dbl&gt; &lt;dbl&gt; ## 1 21 16.5 ## 2 21 17.0 ## 3 22.8 18.6 ## 4 21.4 19.4 ## 5 18.7 17.0 ## 6 18.1 20.2 ## 7 14.3 15.8 ## 8 24.4 20 ## 9 22.8 22.9 ## 10 19.2 18.3 ## # ℹ 22 more rows Using regex allows more flexibility in terms of pattern matching, as well as with anchors ^ and $: mtcars %&gt;% select(matches(&#39;^m&#39;), matches(&#39;c$&#39;)) ## # A tibble: 32 × 2 ## mpg qsec ## &lt;dbl&gt; &lt;dbl&gt; ## 1 21 16.5 ## 2 21 17.0 ## 3 22.8 18.6 ## 4 21.4 19.4 ## 5 18.7 17.0 ## 6 18.1 20.2 ## 7 14.3 15.8 ## 8 24.4 20 ## 9 22.8 22.9 ## 10 19.2 18.3 ## # ℹ 22 more rows Alternatively, in base R and regex: mtcars[,grepl(&#39;^m|c$&#39;, names(mtcars))] ## # A tibble: 32 × 2 ## mpg qsec ## &lt;dbl&gt; &lt;dbl&gt; ## 1 21 16.5 ## 2 21 17.0 ## 3 22.8 18.6 ## 4 21.4 19.4 ## 5 18.7 17.0 ## 6 18.1 20.2 ## 7 14.3 15.8 ## 8 24.4 20 ## 9 22.8 22.9 ## 10 19.2 18.3 ## # ℹ 22 more rows 2.7 Long and wide data formats Transforming datasets into long/wide formats is a typical task in exploratory data analysis and this can be done using tidyr: mtcars_long &lt;- mtcars %&gt;% pivot_longer(-1, names_to = &#39;Metric&#39;, values_to = &#39;Values&#39;) mtcars_long ## # A tibble: 352 × 3 ## CAR Metric Values ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 Mazda RX4 mpg 21 ## 2 Mazda RX4 cyl 6 ## 3 Mazda RX4 disp 160 ## 4 Mazda RX4 hp 110 ## 5 Mazda RX4 drat 3.9 ## 6 Mazda RX4 wt 2.62 ## 7 Mazda RX4 qsec 16.5 ## 8 Mazda RX4 vs 0 ## 9 Mazda RX4 am 1 ## 10 Mazda RX4 gear 4 ## # ℹ 342 more rows Having the dataset in this long format allows us to create visualizations such as the boxplot much easier. Pivoting the long format back to a wide format as also straightforward with tidyr: mtcars_wide &lt;- mtcars_long %&gt;% pivot_wider(names_from = &#39;Metric&#39;, values_from = &#39;Values&#39;) mtcars_wide ## # A tibble: 32 × 12 ## CAR mpg cyl disp hp drat wt qsec vs am ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Mazda RX4 21 6 160 110 3.9 2.62 16.5 0 1 ## 2 Mazda RX… 21 6 160 110 3.9 2.88 17.0 0 1 ## 3 Datsun 7… 22.8 4 108 93 3.85 2.32 18.6 1 1 ## 4 Hornet 4… 21.4 6 258 110 3.08 3.22 19.4 1 0 ## 5 Hornet S… 18.7 8 360 175 3.15 3.44 17.0 0 0 ## 6 Valiant 18.1 6 225 105 2.76 3.46 20.2 1 0 ## 7 Duster 3… 14.3 8 360 245 3.21 3.57 15.8 0 0 ## 8 Merc 240D 24.4 4 147. 62 3.69 3.19 20 1 0 ## 9 Merc 230 22.8 4 141. 95 3.92 3.15 22.9 1 0 ## 10 Merc 280 19.2 6 168. 123 3.92 3.44 18.3 1 0 ## # ℹ 22 more rows ## # ℹ 2 more variables: gear &lt;dbl&gt;, carb &lt;dbl&gt; Having the data in a wide format may be useful when plotting visualizations such as a heatmap, where row and column annotations correspond to the heatmap perimeter. In base R, you can use reshape() with the argument direction = but I don’t find its syntax very intuitive. Alternatively, using data.table gets us the solution much easier: library(data.table) melt(setDT(mtcars), id.vars = c(&#39;CAR&#39;), variable.name = &#39;Metric&#39;) ## CAR Metric value ## 1: Mazda RX4 mpg 21.0 ## 2: Mazda RX4 Wag mpg 21.0 ## 3: Datsun 710 mpg 22.8 ## 4: Hornet 4 Drive mpg 21.4 ## 5: Hornet Sportabout mpg 18.7 ## --- ## 348: Lotus Europa carb 2.0 ## 349: Ford Pantera L carb 4.0 ## 350: Ferrari Dino carb 6.0 ## 351: Maserati Bora carb 8.0 ## 352: Volvo 142E carb 2.0 Note that using data.table returns a data.table object (due to the setDT() function), which differs from the tibble we’ve been using for packages within tidyverse. There are some advantages to using data.table instead, especially when working with very large datasets. For the sake of this book I will mostly use base R and tidyverse, other than exceptional cases. However, for a brief discourse on data.table, refer to Chapter 4. 2.8 Trimming strings Trimming character columns, then re-encoding them as factors: iris %&gt;% mutate(Species = strtrim(Species, 3)) %&gt;% mutate(Species = factor(Species, levels = c(&#39;set&#39;, &#39;ver&#39;, &#39;vir&#39;))) ## # A tibble: 150 × 5 ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; ## 1 5.1 3.5 1.4 0.2 set ## 2 4.9 3 1.4 0.2 set ## 3 4.7 3.2 1.3 0.2 set ## 4 4.6 3.1 1.5 0.2 set ## 5 5 3.6 1.4 0.2 set ## 6 5.4 3.9 1.7 0.4 set ## 7 4.6 3.4 1.4 0.3 set ## 8 5 3.4 1.5 0.2 set ## 9 4.4 2.9 1.4 0.2 set ## 10 4.9 3.1 1.5 0.1 set ## # ℹ 140 more rows Sometimes when you’re importing datasets from external and/or untrustworthy sources (e.g., an Excel sheet) it’s worth checking for any whitespaces. In that case you can use stringr::str_trim() to remove all whitespaces prior to data analysis. iris %&gt;% mutate(Species = str_trim(Species)) ## # A tibble: 150 × 5 ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa ## 4 4.6 3.1 1.5 0.2 setosa ## 5 5 3.6 1.4 0.2 setosa ## 6 5.4 3.9 1.7 0.4 setosa ## 7 4.6 3.4 1.4 0.3 setosa ## 8 5 3.4 1.5 0.2 setosa ## 9 4.4 2.9 1.4 0.2 setosa ## 10 4.9 3.1 1.5 0.1 setosa ## # ℹ 140 more rows 2.9 Iterating over list of dataframes Analogously to base R’s split(), using group_split() allows us to convert the dataset to a list based on a column; the length of this output list would correspond to the unique number of column entries: mtcars &lt;- as_tibble(mtcars) mtcars_lst &lt;- mtcars %&gt;% group_split(cyl) Iteration will be covered in more detail in a future chapter (see Chapter 3), but using base R’s apply family is straightforward. In this case since we’re working with lists, we will use lapply() and pass a function (in this case, rename_with()): mtcars_lst &lt;- lapply(mtcars_lst, function(x) rename_with(x, function(y) paste0(y, &quot;_&quot;, as.character(unique(x$cyl))), .cols = -1)) mtcars_lst[[1]] ## # A tibble: 11 × 12 ## CAR mpg_4 cyl_4 disp_4 hp_4 drat_4 wt_4 qsec_4 vs_4 am_4 ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Datsu… 22.8 4 108 93 3.85 2.32 18.6 1 1 ## 2 Merc … 24.4 4 147. 62 3.69 3.19 20 1 0 ## 3 Merc … 22.8 4 141. 95 3.92 3.15 22.9 1 0 ## 4 Fiat … 32.4 4 78.7 66 4.08 2.2 19.5 1 1 ## 5 Honda… 30.4 4 75.7 52 4.93 1.62 18.5 1 1 ## 6 Toyot… 33.9 4 71.1 65 4.22 1.84 19.9 1 1 ## 7 Toyot… 21.5 4 120. 97 3.7 2.46 20.0 1 0 ## 8 Fiat … 27.3 4 79 66 4.08 1.94 18.9 1 1 ## 9 Porsc… 26 4 120. 91 4.43 2.14 16.7 0 1 ## 10 Lotus… 30.4 4 95.1 113 3.77 1.51 16.9 1 1 ## 11 Volvo… 21.4 4 121 109 4.11 2.78 18.6 1 1 ## # ℹ 2 more variables: gear_4 &lt;dbl&gt;, carb_4 &lt;dbl&gt; A tidyverse alternative to apply() is purrr::map() and its variations: mtcars_lst &lt;- mtcars %&gt;% group_split(cyl) mtcars_lst &lt;- mtcars_lst %&gt;% map(~ rename_with(.x, function(y) paste0(y, &quot;_&quot;, as.character(unique(.x$cyl))), .cols = -1)) mtcars_lst[[1]] ## # A tibble: 11 × 12 ## CAR mpg_4 cyl_4 disp_4 hp_4 drat_4 wt_4 qsec_4 vs_4 am_4 ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Datsu… 22.8 4 108 93 3.85 2.32 18.6 1 1 ## 2 Merc … 24.4 4 147. 62 3.69 3.19 20 1 0 ## 3 Merc … 22.8 4 141. 95 3.92 3.15 22.9 1 0 ## 4 Fiat … 32.4 4 78.7 66 4.08 2.2 19.5 1 1 ## 5 Honda… 30.4 4 75.7 52 4.93 1.62 18.5 1 1 ## 6 Toyot… 33.9 4 71.1 65 4.22 1.84 19.9 1 1 ## 7 Toyot… 21.5 4 120. 97 3.7 2.46 20.0 1 0 ## 8 Fiat … 27.3 4 79 66 4.08 1.94 18.9 1 1 ## 9 Porsc… 26 4 120. 91 4.43 2.14 16.7 0 1 ## 10 Lotus… 30.4 4 95.1 113 3.77 1.51 16.9 1 1 ## 11 Volvo… 21.4 4 121 109 4.11 2.78 18.6 1 1 ## # ℹ 2 more variables: gear_4 &lt;dbl&gt;, carb_4 &lt;dbl&gt; Fitting a linear model is easy using iterations; in this case we fit lm() on the variables mpg and gear to identify their relationship: mtcars_lst &lt;- mtcars %&gt;% group_split(cyl) mtcars_lst %&gt;% map(~ lm(mpg ~ gear, data = .x)) %&gt;% map(coef) ## [[1]] ## (Intercept) gear ## 15.08125 2.83125 ## ## [[2]] ## (Intercept) gear ## 19.82 -0.02 ## ## [[3]] ## (Intercept) gear ## 14.525 0.175 Using broom::tidy() to clean up modelling result and output the model estimates: library(broom) mtcars_lst %&gt;% map(~ lm(mpg ~ gear, data = .x)) %&gt;% map(tidy) %&gt;% bind_rows() ## # A tibble: 6 × 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 15.1 10.8 1.39 0.197 ## 2 gear 2.83 2.62 1.08 0.308 ## 3 (Intercept) 19.8 3.68 5.38 0.00299 ## 4 gear -0.0200 0.942 -0.0212 0.984 ## 5 (Intercept) 14.5 3.41 4.25 0.00112 ## 6 gear 0.175 1.02 0.172 0.866 Using base R and the apply() family instead: models &lt;- lapply(mtcars_lst, function(x) lm(mpg ~ gear, data = x)) coefs &lt;- lapply(models, coef) coefs[[1]] ## (Intercept) gear ## 15.08125 2.83125 2.10 Rowwise operations df &lt;- tibble(name = c(&#39;Brian&#39;, &#39;Connor&#39;), coffee = sample(1:10, 2), wine = sample(1:5, 2), juice = sample(1:5, 2)) df ## # A tibble: 2 × 4 ## name coffee wine juice ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 Brian 6 1 2 ## 2 Connor 2 3 4 Sometimes it’s useful to run calculations in a row-wise fashion; for example using mutate() with the helper function c_across(): df %&gt;% rowwise() %&gt;% mutate(total = sum(c_across(coffee:juice)), avg = mean(c_across(coffee:juice))) ## # A tibble: 2 × 6 ## # Rowwise: ## name coffee wine juice total avg ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Brian 6 1 2 9 3 ## 2 Connor 2 3 4 9 3 Calculating the proportions using a combination of row-wise and column-wise operations with c_across() and across(): df %&gt;% rowwise() %&gt;% mutate(total = sum(c_across(coffee:juice))) %&gt;% ungroup() %&gt;% mutate(across(coffee:juice, function(x) x/total, .names = &quot;{col}_prop.&quot;)) ## # A tibble: 2 × 8 ## name coffee wine juice total coffee_prop. wine_prop. ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Brian 6 1 2 9 0.667 0.111 ## 2 Connor 2 3 4 9 0.222 0.333 ## # ℹ 1 more variable: juice_prop. &lt;dbl&gt; 2.11 Conditional transformation Conditional transformations are useful when creating categories based on a series of logical statements; this is done using case_when() to define the conditions: mtcars %&gt;% mutate(mileage_class = case_when(mpg &gt; 20 ~ &#39;High&#39;, mpg &lt; 20 ~ &#39;Low&#39;)) %&gt;% relocate(mileage_class, .after = mpg) ## # A tibble: 32 × 13 ## CAR mpg mileage_class cyl disp hp drat wt qsec ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Mazda … 21 High 6 160 110 3.9 2.62 16.5 ## 2 Mazda … 21 High 6 160 110 3.9 2.88 17.0 ## 3 Datsun… 22.8 High 4 108 93 3.85 2.32 18.6 ## 4 Hornet… 21.4 High 6 258 110 3.08 3.22 19.4 ## 5 Hornet… 18.7 Low 8 360 175 3.15 3.44 17.0 ## 6 Valiant 18.1 Low 6 225 105 2.76 3.46 20.2 ## 7 Duster… 14.3 Low 8 360 245 3.21 3.57 15.8 ## 8 Merc 2… 24.4 High 4 147. 62 3.69 3.19 20 ## 9 Merc 2… 22.8 High 4 141. 95 3.92 3.15 22.9 ## 10 Merc 2… 19.2 Low 6 168. 123 3.92 3.44 18.3 ## # ℹ 22 more rows ## # ℹ 4 more variables: vs &lt;dbl&gt;, am &lt;dbl&gt;, gear &lt;dbl&gt;, carb &lt;dbl&gt; In cases where we’re working with character columns, it’s useful to use grepl to match specific strings. After all, formula within case_when() needs to evaluate to a boolean on the left-hand side (e.g., mpg &gt; 20). Additionally, in cases where the condition is not specified or met, you can use TRUE ~ within case_when() as such: mtcars %&gt;% mutate(CAR = case_when(grepl(&#39;Merc&#39;, CAR) ~ toupper(CAR), TRUE ~ CAR)) ## # A tibble: 32 × 12 ## CAR mpg cyl disp hp drat wt qsec vs am ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Mazda RX4 21 6 160 110 3.9 2.62 16.5 0 1 ## 2 Mazda RX… 21 6 160 110 3.9 2.88 17.0 0 1 ## 3 Datsun 7… 22.8 4 108 93 3.85 2.32 18.6 1 1 ## 4 Hornet 4… 21.4 6 258 110 3.08 3.22 19.4 1 0 ## 5 Hornet S… 18.7 8 360 175 3.15 3.44 17.0 0 0 ## 6 Valiant 18.1 6 225 105 2.76 3.46 20.2 1 0 ## 7 Duster 3… 14.3 8 360 245 3.21 3.57 15.8 0 0 ## 8 MERC 240D 24.4 4 147. 62 3.69 3.19 20 1 0 ## 9 MERC 230 22.8 4 141. 95 3.92 3.15 22.9 1 0 ## 10 MERC 280 19.2 6 168. 123 3.92 3.44 18.3 1 0 ## # ℹ 22 more rows ## # ℹ 2 more variables: gear &lt;dbl&gt;, carb &lt;dbl&gt; Conditional mutate using base R involves using ifelse(): mtcars$mileage_class &lt;- ifelse( mtcars$mpg &gt; 20, &#39;High&#39;, &#39;Low&#39; ) subset(mtcars, select = c(CAR, mpg, mileage_class, cyl:carb)) ## # A tibble: 32 × 13 ## CAR mpg mileage_class cyl disp hp drat wt qsec ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Mazda … 21 High 6 160 110 3.9 2.62 16.5 ## 2 Mazda … 21 High 6 160 110 3.9 2.88 17.0 ## 3 Datsun… 22.8 High 4 108 93 3.85 2.32 18.6 ## 4 Hornet… 21.4 High 6 258 110 3.08 3.22 19.4 ## 5 Hornet… 18.7 Low 8 360 175 3.15 3.44 17.0 ## 6 Valiant 18.1 Low 6 225 105 2.76 3.46 20.2 ## 7 Duster… 14.3 Low 8 360 245 3.21 3.57 15.8 ## 8 Merc 2… 24.4 High 4 147. 62 3.69 3.19 20 ## 9 Merc 2… 22.8 High 4 141. 95 3.92 3.15 22.9 ## 10 Merc 2… 19.2 Low 6 168. 123 3.92 3.44 18.3 ## # ℹ 22 more rows ## # ℹ 4 more variables: vs &lt;dbl&gt;, am &lt;dbl&gt;, gear &lt;dbl&gt;, carb &lt;dbl&gt; 2.12 Missing values Handling missing values is tedious but often required when working with dodgy data. First, for the sake of our exercise we insert NAs randomly in the mtcars dataset: mtcars_NA &lt;- map_df(mtcars, function(x) {x[sample(c(TRUE, NA), prob = c(0.95, 0.01), size = length(x), replace = TRUE)]}) mtcars_NA ## # A tibble: 32 × 13 ## CAR mpg cyl disp hp drat wt qsec vs am ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Mazda RX4 21 6 160 110 3.9 2.62 16.5 0 1 ## 2 Mazda RX… 21 6 160 110 3.9 2.88 17.0 0 1 ## 3 Datsun 7… 22.8 4 108 93 3.85 2.32 18.6 1 1 ## 4 Hornet 4… 21.4 6 258 110 3.08 3.22 19.4 1 0 ## 5 Hornet S… 18.7 8 360 175 3.15 3.44 17.0 0 0 ## 6 Valiant 18.1 NA 225 105 2.76 3.46 20.2 1 0 ## 7 Duster 3… 14.3 8 360 245 3.21 3.57 15.8 0 0 ## 8 Merc 240D 24.4 4 147. 62 3.69 3.19 20 1 0 ## 9 Merc 230 22.8 4 141. 95 3.92 3.15 22.9 1 0 ## 10 Merc 280 19.2 6 168. 123 3.92 3.44 18.3 1 0 ## # ℹ 22 more rows ## # ℹ 3 more variables: gear &lt;dbl&gt;, carb &lt;dbl&gt;, mileage_class &lt;chr&gt; Remove rows where any NA occurs: mtcars_NA %&gt;% na.omit() ## # A tibble: 29 × 13 ## CAR mpg cyl disp hp drat wt qsec vs am ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Mazda RX4 21 6 160 110 3.9 2.62 16.5 0 1 ## 2 Mazda RX… 21 6 160 110 3.9 2.88 17.0 0 1 ## 3 Datsun 7… 22.8 4 108 93 3.85 2.32 18.6 1 1 ## 4 Hornet 4… 21.4 6 258 110 3.08 3.22 19.4 1 0 ## 5 Hornet S… 18.7 8 360 175 3.15 3.44 17.0 0 0 ## 6 Duster 3… 14.3 8 360 245 3.21 3.57 15.8 0 0 ## 7 Merc 240D 24.4 4 147. 62 3.69 3.19 20 1 0 ## 8 Merc 230 22.8 4 141. 95 3.92 3.15 22.9 1 0 ## 9 Merc 280 19.2 6 168. 123 3.92 3.44 18.3 1 0 ## 10 Merc 280C 17.8 6 168. 123 3.92 3.44 18.9 1 0 ## # ℹ 19 more rows ## # ℹ 3 more variables: gear &lt;dbl&gt;, carb &lt;dbl&gt;, mileage_class &lt;chr&gt; Identify columns with NAs and the number of occurrences: vapply(mtcars_NA, function(x) sum(is.na(x)), double(1)) ## CAR mpg cyl disp ## 0 1 1 0 ## hp drat wt qsec ## 0 0 0 0 ## vs am gear carb ## 0 0 0 0 ## mileage_class ## 1 Remove columns with more than one missing value: mtcars_NA %&gt;% select(where(function(x) sum(is.na(x)) &lt; 1)) ## # A tibble: 32 × 10 ## CAR disp hp drat wt qsec vs am gear carb ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Mazda RX4 160 110 3.9 2.62 16.5 0 1 4 4 ## 2 Mazda RX… 160 110 3.9 2.88 17.0 0 1 4 4 ## 3 Datsun 7… 108 93 3.85 2.32 18.6 1 1 4 1 ## 4 Hornet 4… 258 110 3.08 3.22 19.4 1 0 3 1 ## 5 Hornet S… 360 175 3.15 3.44 17.0 0 0 3 2 ## 6 Valiant 225 105 2.76 3.46 20.2 1 0 3 1 ## 7 Duster 3… 360 245 3.21 3.57 15.8 0 0 3 4 ## 8 Merc 240D 147. 62 3.69 3.19 20 1 0 4 2 ## 9 Merc 230 141. 95 3.92 3.15 22.9 1 0 4 2 ## 10 Merc 280 168. 123 3.92 3.44 18.3 1 0 4 4 ## # ℹ 22 more rows Replace missing values with zero using tidyr::replace_na(): mtcars_NA %&gt;% map_dfc(~ replace_na(.x, 0)) Base R and using is.na() instead: mtcars_NA[is.na(mtcars_NA)] &lt;- 0 # not run 2.13 Joining dataframes Mutating joins in tidyverse are analogous to the inner and outer joins in SQL syntax: df1 &lt;- tibble( name = c(&#39;Brian&#39;, &#39;Connor&#39;, &#39;Jon&#39;), city = c(&#39;Tokyo&#39;, &#39;London&#39;, &#39;Milan&#39;), age = c(28, 25, 21) ) df2 &lt;- tibble( name = c(&#39;Brian&#39;, &#39;Connor&#39;), hair = c(&#39;black&#39;, &#39;brown&#39;), eyes = c(&#39;dark&#39;, &#39;hazel&#39;) ) df1 %&gt;% inner_join(df2, by = &#39;name&#39;) ## # A tibble: 2 × 5 ## name city age hair eyes ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Brian Tokyo 28 black dark ## 2 Connor London 25 brown hazel df1 %&gt;% left_join(df2, by = &#39;name&#39;) ## # A tibble: 3 × 5 ## name city age hair eyes ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Brian Tokyo 28 black dark ## 2 Connor London 25 brown hazel ## 3 Jon Milan 21 &lt;NA&gt; &lt;NA&gt; Base R uses merge() with the argument all.x = and all_y =: merge(df1, df2, by = &#39;name&#39;) ## name city age hair eyes ## 1 Brian Tokyo 28 black dark ## 2 Connor London 25 brown hazel merge(df1, df2, by = &#39;name&#39;, all.x = TRUE) ## name city age hair eyes ## 1 Brian Tokyo 28 black dark ## 2 Connor London 25 brown hazel ## 3 Jon Milan 21 &lt;NA&gt; &lt;NA&gt; "],["everyday-iterations.html", "Chapter 3 Everyday iterations 3.1 Iterating over multiple columns using apply(), dplyr, and purrr 3.2 Iterating over lists 3.3 Iterating over vectors 3.4 Iterating with two inputs 3.5 Iterating over indices and names 3.6 Handling errors within purrr", " Chapter 3 Everyday iterations In the previous chapter, I briefly covered writing an iterative function using purrr::map(). In a practical setting, iterations are most useful when we’ve split our data based on some sort of a variable of interest. For example, say we have a dataset containing patient outcome after a treatment with a novel therapeutic. We could split the data based on the type of the therapeutic and iterate a model to compare and contrast treatment effects. On the other hand, we could simply want to apply some sort of a function over multiple columns of a dataset to save the trouble of writing the same function over and over. A traditional iterative function involves explicitly writing a for loop. Though for loops are often misconstrued as being slower than the functional counterparts (e.g., the apply() family of base R functions), the real down-side of for loops, as Hadley argues in his book, ‘Advanced R’, is that for loops do a poor job in conveying what should be done with the results. library(tidyverse) 3.1 Iterating over multiple columns using apply(), dplyr, and purrr Returning to the familiar dataset mtcars, composed of 32 rows and 12 columns: data(mtcars) mtcars &lt;- as_tibble(mtcars, rownames = &#39;CAR&#39;) The basic apply() function needs to know whether you’re iterating over columns or over rows using the MARGIN = argument: apply(mtcars[-1], MARGIN = 2, function(x) x/2) %&gt;% head() ## mpg cyl disp hp drat wt qsec vs am gear carb ## [1,] 10.50 3 80.0 55.0 1.950 1.3100 8.230 0.0 0.5 2.0 2.0 ## [2,] 10.50 3 80.0 55.0 1.950 1.4375 8.510 0.0 0.5 2.0 2.0 ## [3,] 11.40 2 54.0 46.5 1.925 1.1600 9.305 0.5 0.5 2.0 0.5 ## [4,] 10.70 3 129.0 55.0 1.540 1.6075 9.720 0.5 0.0 1.5 0.5 ## [5,] 9.35 4 180.0 87.5 1.575 1.7200 8.510 0.0 0.0 1.5 1.0 ## [6,] 9.05 3 112.5 52.5 1.380 1.7300 10.110 0.5 0.0 1.5 0.5 This returns a matrix, as apply() is typically used for an array or a matrix. The one downside with apply() is that the user cannot define what form the output will be in. The side effect of this is that I’ve had to exclude the first column because it contains characters. Running cbind() afterwards to concatenate mtcars[1,] can do the job, though it’s cumbersome. Therefore, it’s often not advisable to run apply() on a dataframe or a tibble. A better method is the dplyr::mutate() solution: mtcars %&gt;% mutate(across(where(is.numeric), ~ .x/2)) ## # A tibble: 32 × 12 ## CAR mpg cyl disp hp drat wt qsec vs am ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Mazda RX4 10.5 3 80 55 1.95 1.31 8.23 0 0.5 ## 2 Mazda RX… 10.5 3 80 55 1.95 1.44 8.51 0 0.5 ## 3 Datsun 7… 11.4 2 54 46.5 1.92 1.16 9.30 0.5 0.5 ## 4 Hornet 4… 10.7 3 129 55 1.54 1.61 9.72 0.5 0 ## 5 Hornet S… 9.35 4 180 87.5 1.58 1.72 8.51 0 0 ## 6 Valiant 9.05 3 112. 52.5 1.38 1.73 10.1 0.5 0 ## 7 Duster 3… 7.15 4 180 122. 1.60 1.78 7.92 0 0 ## 8 Merc 240D 12.2 2 73.4 31 1.84 1.60 10 0.5 0 ## 9 Merc 230 11.4 2 70.4 47.5 1.96 1.58 11.4 0.5 0 ## 10 Merc 280 9.6 3 83.8 61.5 1.96 1.72 9.15 0.5 0 ## # ℹ 22 more rows ## # ℹ 2 more variables: gear &lt;dbl&gt;, carb &lt;dbl&gt; Note that instead of writing out function(x) x/2 I’m using the ~ .x notation which is often used in context of purrr:map(). Either works. The purrr:map() solution to this is: mtcars %&gt;% map_if(is.numeric, ~ .x/2) %&gt;% as_tibble() ## # A tibble: 32 × 12 ## CAR mpg cyl disp hp drat wt qsec vs am ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Mazda RX4 10.5 3 80 55 1.95 1.31 8.23 0 0.5 ## 2 Mazda RX… 10.5 3 80 55 1.95 1.44 8.51 0 0.5 ## 3 Datsun 7… 11.4 2 54 46.5 1.92 1.16 9.30 0.5 0.5 ## 4 Hornet 4… 10.7 3 129 55 1.54 1.61 9.72 0.5 0 ## 5 Hornet S… 9.35 4 180 87.5 1.58 1.72 8.51 0 0 ## 6 Valiant 9.05 3 112. 52.5 1.38 1.73 10.1 0.5 0 ## 7 Duster 3… 7.15 4 180 122. 1.60 1.78 7.92 0 0 ## 8 Merc 240D 12.2 2 73.4 31 1.84 1.60 10 0.5 0 ## 9 Merc 230 11.4 2 70.4 47.5 1.96 1.58 11.4 0.5 0 ## 10 Merc 280 9.6 3 83.8 61.5 1.96 1.72 9.15 0.5 0 ## # ℹ 22 more rows ## # ℹ 2 more variables: gear &lt;dbl&gt;, carb &lt;dbl&gt; Writing a for loop for something like this would have been less elegant: mtcars2 &lt;- mtcars for (i in 2:ncol(mtcars2)){ mtcars2[,i] &lt;- mtcars2[,i]/2 } mtcars2 ## # A tibble: 32 × 12 ## CAR mpg cyl disp hp drat wt qsec vs am ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Mazda RX4 10.5 3 80 55 1.95 1.31 8.23 0 0.5 ## 2 Mazda RX… 10.5 3 80 55 1.95 1.44 8.51 0 0.5 ## 3 Datsun 7… 11.4 2 54 46.5 1.92 1.16 9.30 0.5 0.5 ## 4 Hornet 4… 10.7 3 129 55 1.54 1.61 9.72 0.5 0 ## 5 Hornet S… 9.35 4 180 87.5 1.58 1.72 8.51 0 0 ## 6 Valiant 9.05 3 112. 52.5 1.38 1.73 10.1 0.5 0 ## 7 Duster 3… 7.15 4 180 122. 1.60 1.78 7.92 0 0 ## 8 Merc 240D 12.2 2 73.4 31 1.84 1.60 10 0.5 0 ## 9 Merc 230 11.4 2 70.4 47.5 1.96 1.58 11.4 0.5 0 ## 10 Merc 280 9.6 3 83.8 61.5 1.96 1.72 9.15 0.5 0 ## # ℹ 22 more rows ## # ℹ 2 more variables: gear &lt;dbl&gt;, carb &lt;dbl&gt; 3.2 Iterating over lists The usefulness of iteration is more apparent when we’re working with grouped data. I’ve covered a bit of this in the previous chapter, but this is when we’ve split the data based on some sort of a categorical or a grouping variable. mtcars_lst &lt;- mtcars %&gt;% group_split(cyl) The member of the apply() family suited for this task is lapply() which returns a list: lapply(mtcars_lst, function(x) cor(x$mpg, x$wt)) ## [[1]] ## [1] -0.7131848 ## ## [[2]] ## [1] -0.6815498 ## ## [[3]] ## [1] -0.650358 Of course, we can define our own function and then pass it over to lapply() instead: get_pval &lt;- function(x){ mod &lt;- cor.test(x$mpg, x$wt) pv &lt;- mod$p.value if (pv &lt; 0.05){ is_sig &lt;- TRUE } else { is_sig &lt;- FALSE } return(is_sig) } lapply(mtcars_lst, get_pval) ## [[1]] ## [1] TRUE ## ## [[2]] ## [1] FALSE ## ## [[3]] ## [1] TRUE Using vapply() instead allows you to define the class of what the expected output would be; in this case we obtain a logical vector rather than a list: vapply(mtcars_lst, get_pval, FUN.VALUE = logical(1)) ## [1] TRUE FALSE TRUE purrr::map() solution to iterating over list is as such: mtcars_lst %&gt;% map(~ cor(.x$mpg, .x$wt)) ## [[1]] ## [1] -0.7131848 ## ## [[2]] ## [1] -0.6815498 ## ## [[3]] ## [1] -0.650358 If we want the output to be a flat numeric vector instead of a list: mtcars_lst %&gt;% map_dbl(~ cor(.x$mpg, .x$wt)) ## [1] -0.7131848 -0.6815498 -0.6503580 Similarly, using map_lgl() instead would return a logical vector: mtcars_lst %&gt;% map_lgl(get_pval) ## [1] TRUE FALSE TRUE Iterations are also useful when we want to generate visualizations based on the grouped split: p &lt;- mtcars_lst %&gt;% map(~ ggplot(data = .x, aes(x = mpg, y = wt)) + geom_point() + geom_smooth(method = &#39;lm&#39;) + theme_bw()) p[[1]] p[[3]] Combining the package broom with iterative model fitting is particularly useful: library(broom) mtcars_lst %&gt;% map(~ lm(mpg ~ gear, data = .x)) %&gt;% map(glance) %&gt;% bind_rows() ## # A tibble: 3 × 12 ## r.squared adj.r.squared sigma statistic p.value df logLik ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.115 0.0163 4.47 1.17 0.308 1 -31.0 ## 2 0.0000902 -0.200 1.59 0.000451 0.984 1 -12.0 ## 3 0.00246 -0.0807 2.66 0.0297 0.866 1 -32.5 ## # ℹ 5 more variables: AIC &lt;dbl&gt;, BIC &lt;dbl&gt;, deviance &lt;dbl&gt;, ## # df.residual &lt;int&gt;, nobs &lt;int&gt; Using lapply() instead: lapply(mtcars_lst, function(x) lm(mpg ~ gear, data = x)) %&gt;% lapply(glance) %&gt;% bind_rows() ## # A tibble: 3 × 12 ## r.squared adj.r.squared sigma statistic p.value df logLik ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.115 0.0163 4.47 1.17 0.308 1 -31.0 ## 2 0.0000902 -0.200 1.59 0.000451 0.984 1 -12.0 ## 3 0.00246 -0.0807 2.66 0.0297 0.866 1 -32.5 ## # ℹ 5 more variables: AIC &lt;dbl&gt;, BIC &lt;dbl&gt;, deviance &lt;dbl&gt;, ## # df.residual &lt;int&gt;, nobs &lt;int&gt; The map() function can also be used to extract elements: mtcars_lst %&gt;% map(&#39;CAR&#39;) ## [[1]] ## [1] &quot;Datsun 710&quot; &quot;Merc 240D&quot; &quot;Merc 230&quot; ## [4] &quot;Fiat 128&quot; &quot;Honda Civic&quot; &quot;Toyota Corolla&quot; ## [7] &quot;Toyota Corona&quot; &quot;Fiat X1-9&quot; &quot;Porsche 914-2&quot; ## [10] &quot;Lotus Europa&quot; &quot;Volvo 142E&quot; ## ## [[2]] ## [1] &quot;Mazda RX4&quot; &quot;Mazda RX4 Wag&quot; &quot;Hornet 4 Drive&quot; ## [4] &quot;Valiant&quot; &quot;Merc 280&quot; &quot;Merc 280C&quot; ## [7] &quot;Ferrari Dino&quot; ## ## [[3]] ## [1] &quot;Hornet Sportabout&quot; &quot;Duster 360&quot; ## [3] &quot;Merc 450SE&quot; &quot;Merc 450SL&quot; ## [5] &quot;Merc 450SLC&quot; &quot;Cadillac Fleetwood&quot; ## [7] &quot;Lincoln Continental&quot; &quot;Chrysler Imperial&quot; ## [9] &quot;Dodge Challenger&quot; &quot;AMC Javelin&quot; ## [11] &quot;Camaro Z28&quot; &quot;Pontiac Firebird&quot; ## [13] &quot;Ford Pantera L&quot; &quot;Maserati Bora&quot; mtcars_lst %&gt;% map(~ lm(mpg ~ gear, data = .x)) %&gt;% map(coef) %&gt;% map_dbl(2) ## [1] 2.83125 -0.02000 0.17500 3.3 Iterating over vectors Let’s say we have a vector with missing values: x &lt;- c(3, 2, NA, 2, 1, 4, 2, NA, 2, NA) x ## [1] 3 2 NA 2 1 4 2 NA 2 NA Imputing the missing values is easy with iteration over the length of the vector: x %&gt;% map_dbl(~ replace(., is.na(.x), 0)) ## [1] 3 2 0 2 1 4 2 0 2 0 Or if we want to replace the missing values with the mean: x %&gt;% map_dbl(~ replace(., is.na(.x), mean(x, na.rm = TRUE))) ## [1] 3.000000 2.000000 2.285714 2.000000 1.000000 4.000000 ## [7] 2.000000 2.285714 2.000000 2.285714 This can of course be done with a for loop instead: for (i in seq_along(x)){ if (is.na(x[i]) == TRUE){ x[i] &lt;- mean(x, na.rm = TRUE) } } x ## [1] 3.000000 2.000000 2.285714 2.000000 1.000000 4.000000 ## [7] 2.000000 2.285714 2.000000 2.285714 Note that seq_along(x) prints the indices along the length of the vector, as if to write 1:length(x). Iterating over a vector of characters requires map_chr() to get the character vector back, but the syntax is the same: z &lt;- c(&#39;Brian&#39;, &#39;Connor&#39;, &#39;Harry&#39;, &#39;Sonny&#39;) map_chr(z, ~ paste0(.x, &#39;_NAME&#39;)) ## [1] &quot;Brian_NAME&quot; &quot;Connor_NAME&quot; &quot;Harry_NAME&quot; &quot;Sonny_NAME&quot; out &lt;- character(length(z)) for (i in seq_along(z)){ out[i] &lt;- paste0(z[i], &#39;_NAME&#39;) } out ## [1] &quot;Brian_NAME&quot; &quot;Connor_NAME&quot; &quot;Harry_NAME&quot; &quot;Sonny_NAME&quot; In particular cases where the output is printed out, as in the case of print() and cat(), we may end up echoing both the return values and the output list when using map(). To that end, walk() is used to avoid showing the result twice: walk(z, ~ print(paste0(.x, &#39;_NAME&#39;))) ## [1] &quot;Brian_NAME&quot; ## [1] &quot;Connor_NAME&quot; ## [1] &quot;Harry_NAME&quot; ## [1] &quot;Sonny_NAME&quot; 3.4 Iterating with two inputs In contrast to every solution so far, there’s a case to be made about iterating over multiple inputs. For that purpose, purrr:map2() does the job. x &lt;- c(2, 4, 2, 5) y &lt;- c(2, 6, 3, 1) map2_dbl(x, y, sum) ## [1] 4 10 5 6 map2_chr(x, y, ~ str_glue(&#39;The sum of {.x} and {.y} is {sum(.x, .y)}&#39;)) ## [1] &quot;The sum of 2 and 2 is 4&quot; &quot;The sum of 4 and 6 is 10&quot; ## [3] &quot;The sum of 2 and 3 is 5&quot; &quot;The sum of 5 and 1 is 6&quot; Note that the iteration occurs at the ith position of each vector. The intuition behind map2() is straightforward and is illustrated by the equivalent for loop: result &lt;- numeric(length(x)) for (i in seq_along(x)){ result[i] &lt;- sum(x[i], y[i]) } result ## [1] 4 10 5 6 One quirk with map2() is that it recycles the input: map2_chr(&#39;My name is &#39;, c(&#39;Brian&#39;, &#39;Connor&#39;), str_c) ## [1] &quot;My name is Brian&quot; &quot;My name is Connor&quot; Suppose a dataset like this one: z &lt;- tibble(A = x, B = y) z ## # A tibble: 4 × 2 ## A B ## &lt;dbl&gt; &lt;dbl&gt; ## 1 2 2 ## 2 4 6 ## 3 2 3 ## 4 5 1 There may be cases where we want to create a new column using mutate() which results from a transformation of the A and B columns at each row. First, see why the following does not work in creating a column C which takes the higher value between A and B at each row: z %&gt;% mutate(C = max(A, B)) ## # A tibble: 4 × 3 ## A B C ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2 2 6 ## 2 4 6 6 ## 3 2 3 6 ## 4 5 1 6 Using map2(), however, this does work: z %&gt;% mutate(C = map2_dbl(A, B, max)) ## # A tibble: 4 × 3 ## A B C ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2 2 2 ## 2 4 6 6 ## 3 2 3 3 ## 4 5 1 5 Other families of map2() works normally in this context, for example, if we want to check whether the sum of A and B at each row is an even number: z %&gt;% mutate(C = map2_lgl(A, B, ~ (.x + .y) %% 2 == 0)) ## # A tibble: 4 × 3 ## A B C ## &lt;dbl&gt; &lt;dbl&gt; &lt;lgl&gt; ## 1 2 2 TRUE ## 2 4 6 TRUE ## 3 2 3 FALSE ## 4 5 1 TRUE When there are more than 2 inputs, we can use pmap() instead; this function takes a list of the inputs instead: w &lt;- c(4, 2, 3, 1) pmap_dbl(list(x, y, w), sum) ## [1] 8 12 8 7 z &lt;- tibble(A = x, B = y, C = w) z ## # A tibble: 4 × 3 ## A B C ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2 2 4 ## 2 4 6 2 ## 3 2 3 3 ## 4 5 1 1 z %&gt;% mutate(v = pmap_lgl(list(A, B, C), ~ sum(.) %% 2 == 0)) ## # A tibble: 4 × 4 ## A B C v ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;lgl&gt; ## 1 2 2 4 TRUE ## 2 4 6 2 TRUE ## 3 2 3 3 TRUE ## 4 5 1 1 FALSE Using the notation for anonymous functions instead: z %&gt;% mutate(v = pmap_lgl(list(A, B, C), ~ sum(..1 + ..2 + ..3) %% 2 == 0)) ## # A tibble: 4 × 4 ## A B C v ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;lgl&gt; ## 1 2 2 4 TRUE ## 2 4 6 2 TRUE ## 3 2 3 3 TRUE ## 4 5 1 1 FALSE 3.5 Iterating over indices and names A related function is imap() and its variants, which is analogous to looping over numeric indices, such as in the case of for (i in seq_along(x)). That is, it applies a function over an input and its corresponding index. For example: x &lt;- c(2, 4, 2, 5) imap_chr(x, ~ paste0(&#39;The index &#39;, .y, &#39; number in x is &#39;, .x)) ## [1] &quot;The index 1 number in x is 2&quot; &quot;The index 2 number in x is 4&quot; ## [3] &quot;The index 3 number in x is 2&quot; &quot;The index 4 number in x is 5&quot; Without using purrr, this is equivalent to the for loop: out &lt;- character(length(x)) for (i in seq_along(x)){ out[i] &lt;- paste0(&#39;The index &#39;, i, &#39; number in x is &#39;, x[i]) } out ## [1] &quot;The index 1 number in x is 2&quot; &quot;The index 2 number in x is 4&quot; ## [3] &quot;The index 3 number in x is 2&quot; &quot;The index 4 number in x is 5&quot; imap() also works with names instead of indices, if required: names(x) &lt;- c(&#39;A&#39;, &#39;B&#39;, &#39;C&#39;, &#39;D&#39;) imap_lgl(x, ~ if(.y %in% c(&#39;A&#39;, &#39;C&#39;)) TRUE else FALSE) ## A B C D ## TRUE FALSE TRUE FALSE The equivalent expression in the form of a for loop is as follows: out &lt;- logical() for (i in names(x)){ if(i %in% c(&#39;A&#39;, &#39;C&#39;)){ out[i] &lt;- TRUE } else { out[i] &lt;- FALSE } } out ## A B C D ## TRUE FALSE TRUE FALSE Therefore, we see clearly that the two uses of imap() - iterating over indices and over names - is equivalent to for (i in seq_along(x)) and for (i in names(x)), respectively. 3.6 Handling errors within purrr Let’s return to the mtcars dataset, specifically after we split the tibble into a list based on group_split(cyl). mtcars_lst &lt;- mtcars %&gt;% group_split(cyl) For one of the elements of the list, I’m filling the gear column with missing values: mtcars_lst[[2]] &lt;- mtcars_lst[[2]] %&gt;% mutate(gear = NA) mtcars_lst[[2]] ## # A tibble: 7 × 12 ## CAR mpg cyl disp hp drat wt qsec vs am ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Mazda RX4 21 6 160 110 3.9 2.62 16.5 0 1 ## 2 Mazda RX4… 21 6 160 110 3.9 2.88 17.0 0 1 ## 3 Hornet 4 … 21.4 6 258 110 3.08 3.22 19.4 1 0 ## 4 Valiant 18.1 6 225 105 2.76 3.46 20.2 1 0 ## 5 Merc 280 19.2 6 168. 123 3.92 3.44 18.3 1 0 ## 6 Merc 280C 17.8 6 168. 123 3.92 3.44 18.9 1 0 ## 7 Ferrari D… 19.7 6 145 175 3.62 2.77 15.5 0 1 ## # ℹ 2 more variables: gear &lt;dbl&gt;, carb &lt;dbl&gt; Now, if I were to try and fit a linear model using lm(mpg ~ wt) iteratively over the mtcars_lst, it will fail at the second element as the wt values are all missing. mtcars_lst %&gt;% map(~ lm(mpg ~ gear, data = .x)) %&gt;% map(coef) %&gt;% map_dbl(2) # returns an error This is inconvenient in many cases as ideally we’d want to skip over the iteration at which the function fails and get the rest of the results. Thankfully this can be done using possibly(); note the second element in the output where the lm() function would’ve failed: map(mtcars_lst, possibly(~ lm(mpg ~ gear, data = .x), otherwise = NA)) ## [[1]] ## ## Call: ## lm(formula = mpg ~ gear, data = .x) ## ## Coefficients: ## (Intercept) gear ## 15.081 2.831 ## ## ## [[2]] ## [1] NA ## ## [[3]] ## ## Call: ## lm(formula = mpg ~ gear, data = .x) ## ## Coefficients: ## (Intercept) gear ## 14.525 0.175 The second argument within otherwise = argument within possibly(), where we wrapped the iterative function, provides an alternative solution in case the function fails. As we can see above, the second element of the output corresponds to NA and the iteration continued after. Using purrr::keep() I can select for the elements that did not fail: map(mtcars_lst, possibly(~ lm(mpg ~ gear, data = .x), otherwise = NA)) %&gt;% keep(~ !is.na(.x) %&gt;% all()) ## [[1]] ## ## Call: ## lm(formula = mpg ~ gear, data = .x) ## ## Coefficients: ## (Intercept) gear ## 15.081 2.831 ## ## ## [[2]] ## ## Call: ## lm(formula = mpg ~ gear, data = .x) ## ## Coefficients: ## (Intercept) gear ## 14.525 0.175 Of course this is the same as running the first map() function wrapped around possibly() and then running result[-which(is.na(result)]. Sometimes it’s not useful to keep the failed element in the first place, so setting otherwise = NULL within possibly() works too. Afterwards, removing the empty element (i.e., NULL) is done using purrr::compact(). map(mtcars_lst, possibly(~ lm(mpg ~ gear, data = .x), otherwise = NULL)) %&gt;% compact() ## [[1]] ## ## Call: ## lm(formula = mpg ~ gear, data = .x) ## ## Coefficients: ## (Intercept) gear ## 15.081 2.831 ## ## ## [[2]] ## ## Call: ## lm(formula = mpg ~ gear, data = .x) ## ## Coefficients: ## (Intercept) gear ## 14.525 0.175 Instead of discarding the iteration where the function failed, we could also catch the error by using safely() instead. This returns a nested list as such: map(mtcars_lst, safely(~ lm(mpg ~ gear, data = .x))) ## [[1]] ## [[1]]$result ## ## Call: ## lm(formula = mpg ~ gear, data = .x) ## ## Coefficients: ## (Intercept) gear ## 15.081 2.831 ## ## ## [[1]]$error ## NULL ## ## ## [[2]] ## [[2]]$result ## NULL ## ## [[2]]$error ## &lt;simpleError in lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...): 0 (non-NA) cases&gt; ## ## ## [[3]] ## [[3]]$result ## ## Call: ## lm(formula = mpg ~ gear, data = .x) ## ## Coefficients: ## (Intercept) gear ## 14.525 0.175 ## ## ## [[3]]$error ## NULL We could also just pull the error terms and throw away the empty NULL elements: map(mtcars_lst, safely(~ lm(mpg ~ gear, data = .x))) %&gt;% map(&#39;error&#39;) %&gt;% compact() ## [[1]] ## &lt;simpleError in lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...): 0 (non-NA) cases&gt; In a traditional for loop without purrr, a solution could be to use tryCatch() with next. Below returns the iteration with the error as NULL as was the case with possibly(..., otherwise = NULL). mod &lt;- list() for (i in seq_along(mtcars_lst)){ err &lt;- tryCatch( mod[[i]] &lt;- lm(mpg ~ gear, data = mtcars_lst[[i]]), error = function(e) e ) if (inherits(err, &#39;error&#39;)) next mod[[i]] &lt;- lm(mpg ~ gear, data = mtcars_lst[[i]]) } mod ## [[1]] ## ## Call: ## lm(formula = mpg ~ gear, data = mtcars_lst[[i]]) ## ## Coefficients: ## (Intercept) gear ## 15.081 2.831 ## ## ## [[2]] ## NULL ## ## [[3]] ## ## Call: ## lm(formula = mpg ~ gear, data = mtcars_lst[[i]]) ## ## Coefficients: ## (Intercept) gear ## 14.525 0.175 "],["interlude-i-a-brief-glimpse-into-data.html", "Chapter 4 Interlude I: A brief glimpse into data.table 4.1 Data wrangling operations 4.2 .SD, .SDcols, and := 4.3 Reshaping data using melt and dcast", " Chapter 4 Interlude I: A brief glimpse into data.table In the first chapter, we saw various practical solutions in data wrangling using tidyverse and base R. One topic that has not been discussed is the idea of computational efficiency and runtime - this matter has been trivial so far since the datasets have been considerably tiny. However, when working with large data, it may be in the user’s interest to try using an alternative package designed for reducing programming and computation time - data.table. The vignette is available here. library(data.table) options(datatable.print.nrows=10) Similarly to how a tibble is an enhanced form of a data.frame in tidyverse, data.table uses an object class called a data.table. Using fread() to read in data - whether the argument corresponds to a local file or a URL pointing to a dataset - automatically generates a data.table object. Converting a regular data.frame to a data.table is done with setDT(). data(mtcars) setDT(mtcars) head(mtcars, 10) ## mpg cyl disp hp drat wt qsec vs am gear carb ## 1: 21.0 6 160.0 110 3.90 2.620 16.46 0 1 4 4 ## 2: 21.0 6 160.0 110 3.90 2.875 17.02 0 1 4 4 ## 3: 22.8 4 108.0 93 3.85 2.320 18.61 1 1 4 1 ## 4: 21.4 6 258.0 110 3.08 3.215 19.44 1 0 3 1 ## 5: 18.7 8 360.0 175 3.15 3.440 17.02 0 0 3 2 ## 6: 18.1 6 225.0 105 2.76 3.460 20.22 1 0 3 1 ## 7: 14.3 8 360.0 245 3.21 3.570 15.84 0 0 3 4 ## 8: 24.4 4 146.7 62 3.69 3.190 20.00 1 0 4 2 ## 9: 22.8 4 140.8 95 3.92 3.150 22.90 1 0 4 2 ## 10: 19.2 6 167.6 123 3.92 3.440 18.30 1 0 4 4 class(mtcars)[1] ## [1] &quot;data.table&quot; 4.1 Data wrangling operations The vignette does a great job explaining the syntax of data.table in detail, but the takeaway message is that it subsets the data by i, performs an operation according to j, then groups it using by =. That is, DT[i, j, by]. mtcars[cyl == 6, .(mean_mileage = mean(mpg))] ## mean_mileage ## 1: 19.74286 Above, data.table has subsetted the object based on cyl == 6 then calculated the mean mileage. The j is wrapped around .(), which is equivalent to list() - this is because the columns in a table are analogous to a list object and we want to return a data.table as our output rather than an atomic vector. class(mtcars[cyl == 6, .(mean_mileage = mean(mpg))]) ## [1] &quot;data.table&quot; &quot;data.frame&quot; Multiple calculations can be performed in j: mtcars[cyl == 6 &amp; gear == 4, .(mean_mileage = mean(mpg), median_wt = median(wt))] ## mean_mileage median_wt ## 1: 19.75 3.1575 Calculating the number of rows in j uses a special variable .N. mtcars[cyl == 6 &amp; gear == 4, .N] ## [1] 4 The j argument can be used to select columns after subsetting rows with i; this is analogous to filter() and select() in dplyr: mtcars[, .(mpg, wt, gear)][1:10] ## mpg wt gear ## 1: 21.0 2.620 4 ## 2: 21.0 2.875 4 ## 3: 22.8 2.320 4 ## 4: 21.4 3.215 3 ## 5: 18.7 3.440 3 ## 6: 18.1 3.460 3 ## 7: 14.3 3.570 3 ## 8: 24.4 3.190 4 ## 9: 22.8 3.150 4 ## 10: 19.2 3.440 4 my_cols &lt;- c(&#39;mpg&#39;, &#39;wt&#39;, &#39;gear&#39;) mtcars[, ..my_cols][1:10] ## mpg wt gear ## 1: 21.0 2.620 4 ## 2: 21.0 2.875 4 ## 3: 22.8 2.320 4 ## 4: 21.4 3.215 3 ## 5: 18.7 3.440 3 ## 6: 18.1 3.460 3 ## 7: 14.3 3.570 3 ## 8: 24.4 3.190 4 ## 9: 22.8 3.150 4 ## 10: 19.2 3.440 4 Using by = argument is similar to group_by() in dplyr: mtcars[, .(mean_mileage = mean(mpg), median_wt = median(wt)), by = cyl] ## cyl mean_mileage median_wt ## 1: 6 19.74286 3.215 ## 2: 4 26.66364 2.200 ## 3: 8 15.10000 3.755 mtcars[, .N, by = cyl] ## cyl N ## 1: 6 7 ## 2: 4 11 ## 3: 8 14 mtcars[vs == 0, .N, by = .(cyl, gear)] ## cyl gear N ## 1: 6 4 2 ## 2: 8 3 12 ## 3: 4 5 1 ## 4: 8 5 2 ## 5: 6 5 1 Piping multiple operations together in data.table is straightforward: mtcars[vs == 0, .(mpg, cyl, gear)][,.(mean_mpg = mean(mpg)), by = .(cyl, gear)] ## cyl gear mean_mpg ## 1: 6 4 21.00 ## 2: 8 3 15.05 ## 3: 4 5 26.00 ## 4: 8 5 15.40 ## 5: 6 5 19.70 4.2 .SD, .SDcols, and := For slightly more difficult operations, we need to define three new concepts: firstly, the .SD variable points to the current subset of data. mtcars[cyl == 6, .SD] ## mpg cyl disp hp drat wt qsec vs am gear carb ## 1: 21.0 6 160.0 110 3.90 2.620 16.46 0 1 4 4 ## 2: 21.0 6 160.0 110 3.90 2.875 17.02 0 1 4 4 ## 3: 21.4 6 258.0 110 3.08 3.215 19.44 1 0 3 1 ## 4: 18.1 6 225.0 105 2.76 3.460 20.22 1 0 3 1 ## 5: 19.2 6 167.6 123 3.92 3.440 18.30 1 0 4 4 ## 6: 17.8 6 167.6 123 3.92 3.440 18.90 1 0 4 4 ## 7: 19.7 6 145.0 175 3.62 2.770 15.50 0 1 5 6 In above context, the .SD doesn’t do much. but this special variable is useful when you’re doing operations over multiple columns. Using .SDcols with .SD allows user to specifically point to columns across the current subset of data. mtcars[cyl == 6, .SD, .SDcols = c(&#39;disp&#39;, &#39;hp&#39;, &#39;drat&#39;)] ## disp hp drat ## 1: 160.0 110 3.90 ## 2: 160.0 110 3.90 ## 3: 258.0 110 3.08 ## 4: 225.0 105 2.76 ## 5: 167.6 123 3.92 ## 6: 167.6 123 3.92 ## 7: 145.0 175 3.62 This means we can easily perform operations across a subset of columns: mtcars[, lapply(.SD, mean), by = cyl, .SDcols = c(&#39;disp&#39;, &#39;hp&#39;, &#39;drat&#39;)] ## cyl disp hp drat ## 1: 6 183.3143 122.28571 3.585714 ## 2: 4 105.1364 82.63636 4.070909 ## 3: 8 353.1000 209.21429 3.229286 .SDcols is flexible because it also accepts indices: col_idx &lt;- colnames(mtcars) %in% c(&#39;disp&#39;, &#39;hp&#39;, &#39;drat&#39;) mtcars[, lapply(.SD, mean), by = cyl, .SDcols = col_idx] ## cyl disp hp drat ## 1: 6 183.3143 122.28571 3.585714 ## 2: 4 105.1364 82.63636 4.070909 ## 3: 8 353.1000 209.21429 3.229286 Using the := operator allows user to define new columns in one of two ways: firstly, in a simple LHS := RHS syntax; this creates a new column but does not print the result to the console. mtcars[, HpPerMpg := .(hp/mpg)] head(mtcars) ## mpg cyl disp hp drat wt qsec vs am gear carb HpPerMpg ## 1: 21.0 6 160 110 3.90 2.620 16.46 0 1 4 4 5.238095 ## 2: 21.0 6 160 110 3.90 2.875 17.02 0 1 4 4 5.238095 ## 3: 22.8 4 108 93 3.85 2.320 18.61 1 1 4 1 4.078947 ## 4: 21.4 6 258 110 3.08 3.215 19.44 1 0 3 1 5.140187 ## 5: 18.7 8 360 175 3.15 3.440 17.02 0 0 3 2 9.358289 ## 6: 18.1 6 225 105 2.76 3.460 20.22 1 0 3 1 5.801105 This allows users to remove columns by setting the RHS to NULL: # not run mtcars[, HpPerMpg := NULL] Subsetting using i allows for condition-based operations, similar to mutate(case_when()) in dplyr: mtcars[cyl == 6, CylThreshold := &#39;Over 6&#39;][cyl != 6, CylThreshold := &#39;Under 6&#39;] head(mtcars) ## mpg cyl disp hp drat wt qsec vs am gear carb HpPerMpg ## 1: 21.0 6 160 110 3.90 2.620 16.46 0 1 4 4 5.238095 ## 2: 21.0 6 160 110 3.90 2.875 17.02 0 1 4 4 5.238095 ## 3: 22.8 4 108 93 3.85 2.320 18.61 1 1 4 1 4.078947 ## 4: 21.4 6 258 110 3.08 3.215 19.44 1 0 3 1 5.140187 ## 5: 18.7 8 360 175 3.15 3.440 17.02 0 0 3 2 9.358289 ## 6: 18.1 6 225 105 2.76 3.460 20.22 1 0 3 1 5.801105 ## CylThreshold ## 1: Over 6 ## 2: Over 6 ## 3: Under 6 ## 4: Over 6 ## 5: Under 6 ## 6: Over 6 Secondly, := can be used in a functional form: mtcars[, `:=`(HpPerMpg = hp/mpg, MpgXCyl = mpg*cyl)] head(mtcars) ## mpg cyl disp hp drat wt qsec vs am gear carb HpPerMpg ## 1: 21.0 6 160 110 3.90 2.620 16.46 0 1 4 4 5.238095 ## 2: 21.0 6 160 110 3.90 2.875 17.02 0 1 4 4 5.238095 ## 3: 22.8 4 108 93 3.85 2.320 18.61 1 1 4 1 4.078947 ## 4: 21.4 6 258 110 3.08 3.215 19.44 1 0 3 1 5.140187 ## 5: 18.7 8 360 175 3.15 3.440 17.02 0 0 3 2 9.358289 ## 6: 18.1 6 225 105 2.76 3.460 20.22 1 0 3 1 5.801105 ## CylThreshold MpgXCyl ## 1: Over 6 126.0 ## 2: Over 6 126.0 ## 3: Under 6 91.2 ## 4: Over 6 128.4 ## 5: Under 6 149.6 ## 6: Over 6 108.6 Combining the := with by =: mtcars[, `:=`(mean_mileage = mean(mpg)), by = .(cyl, vs)] head(mtcars) ## mpg cyl disp hp drat wt qsec vs am gear carb HpPerMpg ## 1: 21.0 6 160 110 3.90 2.620 16.46 0 1 4 4 5.238095 ## 2: 21.0 6 160 110 3.90 2.875 17.02 0 1 4 4 5.238095 ## 3: 22.8 4 108 93 3.85 2.320 18.61 1 1 4 1 4.078947 ## 4: 21.4 6 258 110 3.08 3.215 19.44 1 0 3 1 5.140187 ## 5: 18.7 8 360 175 3.15 3.440 17.02 0 0 3 2 9.358289 ## 6: 18.1 6 225 105 2.76 3.460 20.22 1 0 3 1 5.801105 ## CylThreshold MpgXCyl mean_mileage ## 1: Over 6 126.0 20.56667 ## 2: Over 6 126.0 20.56667 ## 3: Under 6 91.2 26.73000 ## 4: Over 6 128.4 19.12500 ## 5: Under 6 149.6 15.10000 ## 6: Over 6 108.6 19.12500 Combining .SD with the := operator: mtcars[, c(&#39;max_disp&#39;, &#39;max_hp&#39;, &#39;max_wt&#39;) := lapply(.SD, max), by = cyl, .SDcols = c(&#39;disp&#39;, &#39;hp&#39;, &#39;wt&#39;)] head(mtcars) ## mpg cyl disp hp drat wt qsec vs am gear carb HpPerMpg ## 1: 21.0 6 160 110 3.90 2.620 16.46 0 1 4 4 5.238095 ## 2: 21.0 6 160 110 3.90 2.875 17.02 0 1 4 4 5.238095 ## 3: 22.8 4 108 93 3.85 2.320 18.61 1 1 4 1 4.078947 ## 4: 21.4 6 258 110 3.08 3.215 19.44 1 0 3 1 5.140187 ## 5: 18.7 8 360 175 3.15 3.440 17.02 0 0 3 2 9.358289 ## 6: 18.1 6 225 105 2.76 3.460 20.22 1 0 3 1 5.801105 ## CylThreshold MpgXCyl mean_mileage max_disp max_hp max_wt ## 1: Over 6 126.0 20.56667 258.0 175 3.460 ## 2: Over 6 126.0 20.56667 258.0 175 3.460 ## 3: Under 6 91.2 26.73000 146.7 113 3.190 ## 4: Over 6 128.4 19.12500 258.0 175 3.460 ## 5: Under 6 149.6 15.10000 472.0 335 5.424 ## 6: Over 6 108.6 19.12500 258.0 175 3.460 Finally, a strange behaviour is observed when we start making copies of data; for example: data(iris) setDT(iris) iris2 &lt;- iris identical(iris, iris2) ## [1] TRUE Now see what happens when we change one of the columns in iris2 using :=: iris2[, Petal.Width := Petal.Width/100] iris2 ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1: 5.1 3.5 1.4 0.002 setosa ## 2: 4.9 3.0 1.4 0.002 setosa ## 3: 4.7 3.2 1.3 0.002 setosa ## 4: 4.6 3.1 1.5 0.002 setosa ## 5: 5.0 3.6 1.4 0.002 setosa ## --- ## 146: 6.7 3.0 5.2 0.023 virginica ## 147: 6.3 2.5 5.0 0.019 virginica ## 148: 6.5 3.0 5.2 0.020 virginica ## 149: 6.2 3.4 5.4 0.023 virginica ## 150: 5.9 3.0 5.1 0.018 virginica It turns out that changing iris2 has also changed the original data iris: head(iris) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1: 5.1 3.5 1.4 0.002 setosa ## 2: 4.9 3.0 1.4 0.002 setosa ## 3: 4.7 3.2 1.3 0.002 setosa ## 4: 4.6 3.1 1.5 0.002 setosa ## 5: 5.0 3.6 1.4 0.002 setosa ## 6: 5.4 3.9 1.7 0.004 setosa identical(iris, iris2) ## [1] TRUE However, if we use &lt;- to change one of the columns of iris2, the original iris data does not change: iris2$Petal.Length &lt;- iris2$Petal.Length/100 identical(iris, iris2) ## [1] FALSE The rationale for this behaviour is well-explained in this stackoverflow post, but essentially what happens is that := operator modifies by reference. Both iris2 and iris are pointing to the same location after copying initially with &lt;-. Thus when we modify the copy of iris by reference, there is no need to copy the entire dataset iris to alter its copy. On the other hand, changing iris2 using &lt;- will copy the entire thing even if we’re only changing just one column. This behaviour is undesirable when we’re working with very large data. To avoid changing the original dataset but still use := to update a copy, data.table uses the copy() function: iris3 &lt;- copy(iris) iris3[, Petal.Width := Petal.Width/100] identical(iris, iris3) # only the iris3 object was changed here ## [1] FALSE 4.3 Reshaping data using melt and dcast In the first chapter, I briefly touched on melt() as an alternative to tidr::pivot_longer(). Base R’s equivalent reshape() is rather clunky to use, so I much prefer the tidyr or data.table solutions. DT &lt;- data.table( Team = c(&#39;Tottenham&#39;, &#39;Arsenal&#39;, &#39;Chelsea&#39;, &#39;ManUnited&#39;), Wins = c(7, 3, 4, 6), Goals = c(29, 18, 22, 26), CleanSheets = c(3, 1, 2, 3) ) DT ## Team Wins Goals CleanSheets ## 1: Tottenham 7 29 3 ## 2: Arsenal 3 18 1 ## 3: Chelsea 4 22 2 ## 4: ManUnited 6 26 3 DT_long &lt;- melt(DT, id.vars = &#39;Team&#39;, measure.vars = c(&#39;Wins&#39;, &#39;Goals&#39;, &#39;CleanSheets&#39;), variable.name = &#39;Stat&#39;, value.name = &#39;Value&#39;) DT_long ## Team Stat Value ## 1: Tottenham Wins 7 ## 2: Arsenal Wins 3 ## 3: Chelsea Wins 4 ## 4: ManUnited Wins 6 ## 5: Tottenham Goals 29 ## --- ## 8: ManUnited Goals 26 ## 9: Tottenham CleanSheets 3 ## 10: Arsenal CleanSheets 1 ## 11: Chelsea CleanSheets 2 ## 12: ManUnited CleanSheets 3 The data.table equivalent to tidyr::pivot_wider() is dcast(); this function takes in a formula as an argument (~) where the LHS corresponds to the id.vars and the RHS corresponds to the column that originated from the measure.vars. Running this yields the original dataset: dcast(DT_long, Team ~ Stat, value.var = &#39;Value&#39;) ## Team Wins Goals CleanSheets ## 1: Arsenal 3 18 1 ## 2: Chelsea 4 22 2 ## 3: ManUnited 6 26 3 ## 4: Tottenham 7 29 3 "],["everyday-exploratory-data-analysis.html", "Chapter 5 Everyday exploratory data analysis 5.1 Workflow 1: continuous data 5.2 Workflow 2: dates and ordinal data 5.3 Visualization of clusters", " Chapter 5 Everyday exploratory data analysis Before diving straight into complex modeling tasks, many downstream failures and oversights can be avoided with a broad, preliminary look at the data. Exploratory data analysis (EDA) doesn’t aim to answer a specific question or a hypothesis, but seeks to find general, interesting trends and quirks in the data. Finding and identifying outliers, understanding data formats, and investigating the distribution of the data are all components of EDA. There is no one set rule or protocol, but this chapter aims to follow a typical EDA workflow working with continuous, categorical, and ordinal data. library(tidyverse) library(mlbench) library(skimr) 5.1 Workflow 1: continuous data For this exercise I will use the Boston Housing dataset from mlbench, which is a common dataset used in ML tutorials. It consists of numerical features (except one column chas, which is a binary dummy variable) and a continuous target variable medv - median value of homes. data(&quot;BostonHousing&quot;) df &lt;- as_tibble(BostonHousing) The function str() conveniently returns the structure of the dataset and as we can see, it returns the nature of each column. str(df) ## tibble [506 × 14] (S3: tbl_df/tbl/data.frame) ## $ crim : num [1:506] 0.00632 0.02731 0.02729 0.03237 0.06905 ... ## $ zn : num [1:506] 18 0 0 0 0 0 12.5 12.5 12.5 12.5 ... ## $ indus : num [1:506] 2.31 7.07 7.07 2.18 2.18 2.18 7.87 7.87 7.87 7.87 ... ## $ chas : Factor w/ 2 levels &quot;0&quot;,&quot;1&quot;: 1 1 1 1 1 1 1 1 1 1 ... ## $ nox : num [1:506] 0.538 0.469 0.469 0.458 0.458 0.458 0.524 0.524 0.524 0.524 ... ## $ rm : num [1:506] 6.58 6.42 7.18 7 7.15 ... ## $ age : num [1:506] 65.2 78.9 61.1 45.8 54.2 58.7 66.6 96.1 100 85.9 ... ## $ dis : num [1:506] 4.09 4.97 4.97 6.06 6.06 ... ## $ rad : num [1:506] 1 2 2 3 3 3 5 5 5 5 ... ## $ tax : num [1:506] 296 242 242 222 222 222 311 311 311 311 ... ## $ ptratio: num [1:506] 15.3 17.8 17.8 18.7 18.7 18.7 15.2 15.2 15.2 15.2 ... ## $ b : num [1:506] 397 397 393 395 397 ... ## $ lstat : num [1:506] 4.98 9.14 4.03 2.94 5.33 ... ## $ medv : num [1:506] 24 21.6 34.7 33.4 36.2 28.7 22.9 27.1 16.5 18.9 ... For a neat summary of a dataset, we can also use skim() from the package skimr, which behaves nicely in a typical tidyverse pipe style: skim(df) %&gt;% summary() Table 5.1: Data summary Name df Number of rows 506 Number of columns 14 _______________________ Column type frequency: factor 1 numeric 13 ________________________ Group variables None Immediately we see that the column chas is a factor. As previously iterated, this column contains a dummy variable which indicates 1 if tract bounds river and 0 if otherwise. As expected, vapply() output says this is the only non-numerical column. We will have to be aware of this. vapply(df, is.numeric, logical(1)) ## crim zn indus chas nox rm age dis ## TRUE TRUE TRUE FALSE TRUE TRUE TRUE TRUE ## rad tax ptratio b lstat medv ## TRUE TRUE TRUE TRUE TRUE TRUE Generally, features (also as known as predictors) tend to show some degree of correlations with one another. This can be identified with cor(), which returns a correlation matrix. cor_res &lt;- cor(df %&gt;% select(-c(medv, chas))) head(cor_res) ## crim zn indus nox rm ## crim 1.0000000 -0.2004692 0.4065834 0.4209717 -0.2192467 ## zn -0.2004692 1.0000000 -0.5338282 -0.5166037 0.3119906 ## indus 0.4065834 -0.5338282 1.0000000 0.7636514 -0.3916759 ## nox 0.4209717 -0.5166037 0.7636514 1.0000000 -0.3021882 ## rm -0.2192467 0.3119906 -0.3916759 -0.3021882 1.0000000 ## age 0.3527343 -0.5695373 0.6447785 0.7314701 -0.2402649 ## age dis rad tax ptratio ## crim 0.3527343 -0.3796701 0.6255051 0.5827643 0.2899456 ## zn -0.5695373 0.6644082 -0.3119478 -0.3145633 -0.3916785 ## indus 0.6447785 -0.7080270 0.5951293 0.7207602 0.3832476 ## nox 0.7314701 -0.7692301 0.6114406 0.6680232 0.1889327 ## rm -0.2402649 0.2052462 -0.2098467 -0.2920478 -0.3555015 ## age 1.0000000 -0.7478805 0.4560225 0.5064556 0.2615150 ## b lstat ## crim -0.3850639 0.4556215 ## zn 0.1755203 -0.4129946 ## indus -0.3569765 0.6037997 ## nox -0.3800506 0.5908789 ## rm 0.1280686 -0.6138083 ## age -0.2735340 0.6023385 The package corrplot elegantly outputs a visualization using the correlation matrix: library(corrplot) corrplot(cor_res, method = &#39;color&#39;) Now it’s time to check the distribution of the features. For this I want to make a boxplot, but for that I need to reshape the dataset first. As seen in Chapter 1, tidyr::pivot_longer() can be used to convert the data into long format. dfm &lt;- df %&gt;% select(-c(medv, chas)) %&gt;% pivot_longer(everything(), names_to = &#39;Feature&#39;, values_to = &#39;Value&#39;) dfm ## # A tibble: 6,072 × 2 ## Feature Value ## &lt;chr&gt; &lt;dbl&gt; ## 1 crim 0.00632 ## 2 zn 18 ## 3 indus 2.31 ## 4 nox 0.538 ## 5 rm 6.58 ## 6 age 65.2 ## 7 dis 4.09 ## 8 rad 1 ## 9 tax 296 ## 10 ptratio 15.3 ## # ℹ 6,062 more rows Now it’s trivial to use ggplot() to make the boxplot: ggplot(dfm, aes(x = Feature, y = Value)) + geom_boxplot(aes(fill = Feature), alpha = .6) + theme_bw() + theme(legend.position = &#39;none&#39;) It looks like the scales for the features are not consistent. This is common in real-life data and for most ML algorithms, features need to be preprocessed. I will cover this in the future chapters, but in brief, we can calculate the z-score as such for each predictor: zs &lt;- function(x){ (x - mean(x)) / sd(x) } df_s &lt;- df %&gt;% select(-c(medv, chas)) %&gt;% mutate(across(everything(), zs)) df_s ## # A tibble: 506 × 12 ## crim zn indus nox rm age dis rad tax ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 -0.419 0.285 -1.29 -0.144 0.413 -0.120 0.140 -0.982 -0.666 ## 2 -0.417 -0.487 -0.593 -0.740 0.194 0.367 0.557 -0.867 -0.986 ## 3 -0.417 -0.487 -0.593 -0.740 1.28 -0.266 0.557 -0.867 -0.986 ## 4 -0.416 -0.487 -1.31 -0.834 1.02 -0.809 1.08 -0.752 -1.11 ## 5 -0.412 -0.487 -1.31 -0.834 1.23 -0.511 1.08 -0.752 -1.11 ## 6 -0.417 -0.487 -1.31 -0.834 0.207 -0.351 1.08 -0.752 -1.11 ## 7 -0.410 0.0487 -0.476 -0.265 -0.388 -0.0702 0.838 -0.522 -0.577 ## 8 -0.403 0.0487 -0.476 -0.265 -0.160 0.978 1.02 -0.522 -0.577 ## 9 -0.396 0.0487 -0.476 -0.265 -0.930 1.12 1.09 -0.522 -0.577 ## 10 -0.400 0.0487 -0.476 -0.265 -0.399 0.615 1.33 -0.522 -0.577 ## # ℹ 496 more rows ## # ℹ 3 more variables: ptratio &lt;dbl&gt;, b &lt;dbl&gt;, lstat &lt;dbl&gt; Now the boxplots look more uniform: df_sm &lt;- df_s %&gt;% pivot_longer(everything(), names_to = &#39;Feature&#39;, values_to = &#39;Value&#39;) ggplot(df_sm, aes(x = Feature, y = Value)) + geom_boxplot(aes(fill = Feature), alpha = .6) + theme_bw() + theme(legend.position = &#39;none&#39;) How about our target variable - the housing price? This column is also continuous so let’s check the shape of this distribution using a histogram: hist(df$medv) It looks like the mean value for medv is at around 22 to 23. Changing the number of bins can give us a better look, if needed. Since the target variable is continuous, we can easily fit a simple linear model to check for relationships between the predictors and the target. This part may be delving a bit deeper than our initial goal of EDA, but it’s still useful to make us aware of possible relationships in our data. df_num &lt;- df %&gt;% select(-chas) lm_mod &lt;- lm(medv ~ ., data = df_num) summary(lm_mod) ## ## Call: ## lm(formula = medv ~ ., data = df_num) ## ## Residuals: ## Min 1Q Median 3Q Max ## -13.3968 -2.8103 -0.6455 1.9141 26.3755 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 36.891960 5.146516 7.168 2.79e-12 *** ## crim -0.113139 0.033113 -3.417 0.000686 *** ## zn 0.047052 0.013847 3.398 0.000734 *** ## indus 0.040311 0.061707 0.653 0.513889 ## nox -17.366999 3.851224 -4.509 8.13e-06 *** ## rm 3.850492 0.421402 9.137 &lt; 2e-16 *** ## age 0.002784 0.013309 0.209 0.834407 ## dis -1.485374 0.201187 -7.383 6.64e-13 *** ## rad 0.328311 0.066542 4.934 1.10e-06 *** ## tax -0.013756 0.003766 -3.653 0.000287 *** ## ptratio -0.990958 0.131399 -7.542 2.25e-13 *** ## b 0.009741 0.002706 3.600 0.000351 *** ## lstat -0.534158 0.051072 -10.459 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 4.787 on 493 degrees of freedom ## Multiple R-squared: 0.7355, Adjusted R-squared: 0.7291 ## F-statistic: 114.3 on 12 and 493 DF, p-value: &lt; 2.2e-16 An ANOVA table on the fitted model gives us additional info such as the mean sum of squares: anova(lm_mod) ## Analysis of Variance Table ## ## Response: medv ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## crim 1 6440.8 6440.8 281.0564 &lt; 2.2e-16 *** ## zn 1 3554.3 3554.3 155.1005 &lt; 2.2e-16 *** ## indus 1 2551.2 2551.2 111.3283 &lt; 2.2e-16 *** ## nox 1 28.7 28.7 1.2507 0.26397 ## rm 1 11794.6 11794.6 514.6823 &lt; 2.2e-16 *** ## age 1 74.1 74.1 3.2330 0.07278 . ## dis 1 1858.3 1858.3 81.0890 &lt; 2.2e-16 *** ## rad 1 46.9 46.9 2.0447 0.15337 ## tax 1 454.1 454.1 19.8158 1.055e-05 *** ## ptratio 1 1458.7 1458.7 63.6538 1.046e-14 *** ## b 1 650.0 650.0 28.3656 1.529e-07 *** ## lstat 1 2506.8 2506.8 109.3905 &lt; 2.2e-16 *** ## Residuals 493 11297.8 22.9 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 The broom package is useful in converting summaries of model objects into workable tibbles: library(broom) tidy(lm_mod) ## # A tibble: 13 × 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 36.9 5.15 7.17 2.79e-12 ## 2 crim -0.113 0.0331 -3.42 6.86e- 4 ## 3 zn 0.0471 0.0138 3.40 7.34e- 4 ## 4 indus 0.0403 0.0617 0.653 5.14e- 1 ## 5 nox -17.4 3.85 -4.51 8.13e- 6 ## 6 rm 3.85 0.421 9.14 1.66e-18 ## 7 age 0.00278 0.0133 0.209 8.34e- 1 ## 8 dis -1.49 0.201 -7.38 6.64e-13 ## 9 rad 0.328 0.0665 4.93 1.10e- 6 ## 10 tax -0.0138 0.00377 -3.65 2.87e- 4 ## 11 ptratio -0.991 0.131 -7.54 2.25e-13 ## 12 b 0.00974 0.00271 3.60 3.51e- 4 ## 13 lstat -0.534 0.0511 -10.5 2.94e-23 Since we seem to have linear relationships across our dataset, we can use scatterplots in combination with correlation analysis to generate useful visualizations: library(ggpubr) ggplot(df_num, aes(x = rm, y = medv)) + geom_point() + geom_smooth(method = &#39;lm&#39;) + theme_bw() + stat_cor(method = &#39;pearson&#39;) Oops! Even though it’s clear there is indeed a linear relationship between the number of rooms rm and the housing price medv, it looks like there is a strange behaviour at medv == 50. Indeed, it looks like the measurement was artificially capped at 50 and there are 16 instances where this value is found: length(df$medv[df$medv == 50]) ## [1] 16 Since we’re only concerned with EDA for now, we won’t delve further into how we’re going to tackle this. Of course, if we are training a prediction model, we probably shouldn’t leave the values capped like that as is. EDA have made us aware of this before we started high-level modeling tasks, and that’s good. Let’s circle back to the dummy variable chas. Since this is a factor, let’s treat them as groups and compare the distribution of medv using a Wilcoxon test: df_chas &lt;- df %&gt;% select(medv, chas) ggplot(df_chas, aes(x = chas, y = medv)) + geom_boxplot(aes(fill = chas), alpha = .6) + theme_bw() + theme(legend.position = &#39;none&#39;) + stat_compare_means(method = &#39;wilcox&#39;) A quick summary table can be accessed using skimr::skim() on the grouped data: df_chas %&gt;% group_by(chas) %&gt;% skim() Table 5.2: Data summary Name Piped data Number of rows 506 Number of columns 2 _______________________ Column type frequency: numeric 1 ________________________ Group variables chas Variable type: numeric skim_variable chas n_missing complete_rate mean sd p0 p25 p50 p75 p100 hist medv 0 0 1 22.09 8.83 5.0 16.6 20.9 24.80 50 ▃▇▅▁▁ medv 1 0 1 28.44 11.82 13.4 21.1 23.3 33.15 50 ▅▇▃▁▃ Boxplots are nice but violin plots give us a further look at the shape of the distributions: this way we can actually see that medv values are capped at 50. ggplot(df_chas, aes(x = chas, y = medv)) + geom_violin(aes(fill = chas), alpha = .6) + theme_bw() + theme(legend.position = &#39;none&#39;) + stat_compare_means(method = &#39;wilcox&#39;) 5.2 Workflow 2: dates and ordinal data For this part I will pull the Ozone data from mlbench which has the following three columns as the first three: integers coding for the month, integers coding for the day of the month, and integers coding for the day of the week, with Monday coded as the first day (i.e., 1 = Mon., 2 = Tues.,…). The rest of the columns correspond to various weather measurements as continuous values such as the temperature, humidity, and visibility. data(&#39;Ozone&#39;) df &lt;- as_tibble(Ozone) df ## # A tibble: 366 × 13 ## V1 V2 V3 V4 V5 V6 V7 V8 V9 V10 ## &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 1 4 3 5480 8 20 NA NA 5000 ## 2 1 2 5 3 5660 6 NA 38 NA NA ## 3 1 3 6 3 5710 4 28 40 NA 2693 ## 4 1 4 7 5 5700 3 37 45 NA 590 ## 5 1 5 1 5 5760 3 51 54 45.3 1450 ## 6 1 6 2 6 5720 4 69 35 49.6 1568 ## 7 1 7 3 4 5790 6 19 45 46.4 2631 ## 8 1 8 4 4 5790 3 25 55 52.7 554 ## 9 1 9 5 6 5700 3 73 41 48.0 2083 ## 10 1 10 6 7 5700 3 59 44 NA 2654 ## # ℹ 356 more rows ## # ℹ 3 more variables: V11 &lt;dbl&gt;, V12 &lt;dbl&gt;, V13 &lt;dbl&gt; It’s not necessary in this case, but since we are working with dates let’s make the date labels easier to read. Using lubridate I will convert the month labels into a factor with character levels. Then using base R’s weekdays() I will convert the days of the week to characters as well. library(lubridate) df &lt;- df %&gt;% mutate(V1 = lubridate::month(as.numeric(V1), label = TRUE)) %&gt;% mutate(V3 = weekdays(.Date(4:10))[df$V3]) df ## # A tibble: 366 × 13 ## V1 V2 V3 V4 V5 V6 V7 V8 V9 V10 ## &lt;ord&gt; &lt;fct&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Jan 1 Thursday 3 5480 8 20 NA NA 5000 ## 2 Jan 2 Friday 3 5660 6 NA 38 NA NA ## 3 Jan 3 Saturday 3 5710 4 28 40 NA 2693 ## 4 Jan 4 Sunday 5 5700 3 37 45 NA 590 ## 5 Jan 5 Monday 5 5760 3 51 54 45.3 1450 ## 6 Jan 6 Tuesday 6 5720 4 69 35 49.6 1568 ## 7 Jan 7 Wednesday 4 5790 6 19 45 46.4 2631 ## 8 Jan 8 Thursday 4 5790 3 25 55 52.7 554 ## 9 Jan 9 Friday 6 5700 3 73 41 48.0 2083 ## 10 Jan 10 Saturday 7 5700 3 59 44 NA 2654 ## # ℹ 356 more rows ## # ℹ 3 more variables: V11 &lt;dbl&gt;, V12 &lt;dbl&gt;, V13 &lt;dbl&gt; df &lt;- df %&gt;% mutate(V3 = factor(V3, levels= weekdays(.Date(4:10)))) Another thing to note - since this data is temporal data, there’s a big chance there are many missing values due to external factors. Let’s see: vapply(df, function(x) sum(is.na(x)), double(1)) ## V1 V2 V3 V4 V5 V6 V7 V8 V9 V10 V11 V12 V13 ## 0 0 0 5 12 0 15 2 139 15 1 14 0 Column V9, which correspond to temperature measured in El Monte, CA has 139 missing values! Immediately you could argue we can replace these with 0s but remember the nature of this data - a 0 degree weather has actual meaning. Imputation cases like this are tricky and that will be important during modeling tasks. Since we are working with ordinal data - in our case, data points over time, it makes sense to make a trendline. Using facet_wrap(), in ggplot(), I can make a grid based on the time of the month; here I am plotting V8 - temperature measured at Sandburg, CA - versus V2 - day of the month. ggplot(df, aes(x = V2, y = V8)) + geom_point() + geom_line(aes(group = 1)) + facet_wrap(~ V1) + theme_bw() + theme(axis.text.x = element_blank()) For a visual reference, let’s see what happens when we plot the temperature at El Monte instead, with all those missing values: ggplot(df, aes(x = V2, y = V9)) + geom_point() + geom_line(aes(group = 1)) + facet_wrap(~ V1) + theme_bw() + theme(axis.text.x = element_blank()) Adding multiple trendlines is easy using the group = aesthetic within geom_line(); here I will plot V7 and V11 together - humidity and pressure gradient measured at LAX, respectively: df_2 &lt;- df %&gt;% select(V1, V2, V7, V11) %&gt;% pivot_longer(c(V7, V11), names_to = &#39;Measurement&#39;, values_to = &#39;Values&#39;) ggplot(df_2, aes(x = V2, y = Values)) + geom_point() + geom_line(aes(group = Measurement, color = Measurement)) + facet_wrap(~ V1) + theme_bw() + theme(legend.position = &#39;bottom&#39;, axis.text.x = element_blank()) Working with grouped data such as this means an ANOVA tells us whether there is a significant variation across the group means relative to the within-group means: aov_mod &lt;- aov(V8 ~ V1, data = df) summary(aov_mod) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## V1 11 43504 3955 45.67 &lt;2e-16 *** ## Residuals 352 30483 87 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## 2 observations deleted due to missingness A nice way to visualize an ANOVA result is by using grouped boxplots; here I am adding the Kruskal-Wallis ANOVA result from ggpubr(): ggplot(df, aes(x = V1, y = V8)) + geom_boxplot(aes(fill = V1), alpha = .6) + theme_bw() + xlab(&#39;&#39;) + theme(legend.position = &#39;none&#39;) + stat_compare_means(method = &#39;kruskal&#39;) Instead of an ANOVA, I can also run pairwise Wilcoxon tests against a reference group. Here I will make January the reference group: ggplot(df, aes(x = V1, y = V8)) + geom_boxplot(aes(fill = V1), alpha = .6) + theme_bw() + xlab(&#39;&#39;) + theme(legend.position = &#39;none&#39;) + stat_compare_means(method = &#39;wilcox&#39;, ref.group = &#39;Jan&#39;, label = &#39;p.signif&#39;) ## Warning: Removed 2 rows containing non-finite values ## (`stat_boxplot()`). ## Warning: Removed 2 rows containing non-finite values ## (`stat_compare_means()`). If we want to calculate values based on groups, dplyr’s group_by() is useful, as seen in Chapter 1: df %&gt;% group_by(V1) %&gt;% summarise(across(V4:V13, ~ mean(.x, na.rm = TRUE), .names = &#39;mean_{col}&#39;)) ## # A tibble: 12 × 11 ## V1 mean_V4 mean_V5 mean_V6 mean_V7 mean_V8 mean_V9 mean_V10 ## &lt;ord&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Jan 5.45 5745. 4.23 37.4 53.7 54.6 2355. ## 2 Feb 7.14 5659. 5.69 56.9 47.0 45.9 3339. ## 3 Mar 8.84 5656. 5.03 49.8 49.5 45.3 2703. ## 4 Apr 10 5637. 5.97 58.2 51.7 47.2 3101. ## 5 May 15.8 5753. 5.48 70.9 68.4 56.9 2199. ## 6 Jun 17.1 5792 5.2 63.3 73.6 62.8 1651. ## 7 Jul 20.2 5862. 5.29 72.8 80.5 71.2 1824. ## 8 Aug 17.5 5830. 5.23 67.6 74.4 66.7 2113. ## 9 Sep 12.4 5797. 5.77 73.6 69.2 63.5 2794. ## 10 Oct 12.6 5796. 4.90 62.7 64.5 62.4 2728. ## 11 Nov 7.67 5790. 3.3 50.1 58.8 58.6 2834. ## 12 Dec 4.06 5725 2.42 36.1 50.2 51.9 3487. ## # ℹ 3 more variables: mean_V11 &lt;dbl&gt;, mean_V12 &lt;dbl&gt;, ## # mean_V13 &lt;dbl&gt; df %&gt;% group_by(V1, V3) %&gt;% summarise(mean_hum_LAX = mean(V7, na.rm=T)) ## `summarise()` has grouped output by &#39;V1&#39;. You can override using ## the `.groups` argument. ## # A tibble: 84 × 3 ## # Groups: V1 [12] ## V1 V3 mean_hum_LAX ## &lt;ord&gt; &lt;fct&gt; &lt;dbl&gt; ## 1 Jan Monday 56 ## 2 Jan Tuesday 35 ## 3 Jan Wednesday 19 ## 4 Jan Thursday 21.8 ## 5 Jan Friday 44.8 ## 6 Jan Saturday 41.6 ## 7 Jan Sunday 46.8 ## 8 Feb Monday 58.2 ## 9 Feb Tuesday 69.2 ## 10 Feb Wednesday 63.2 ## # ℹ 74 more rows 5.3 Visualization of clusters Clustering and dimensionality reduction tasks can give us a visual look at groupings in the data. The concept of unsupervised clustering and dimensionality reduction techniques will be covered in one of the future chapters, but this is a high-level glance that will be useful in quickly identifying clusters: data(&quot;iris&quot;) df &lt;- as_tibble(iris) For PCA, I will use the useful factoextra package for visualization. The first step is to make sure that the input for PCA is numeric; this means that, for example, in the iris dataset, I need to exclude the column containing the target labels. Additionally, I am declaring the target label column as a factor, since I want to label the data points with these labels in the PCA plot. The fviz_pca_ind() function draws the PCA plot: library(factoextra) pc_res &lt;- prcomp( df %&gt;% select(-Species), scale = TRUE ) groups &lt;- factor(df$Species) fviz_pca_ind(pc_res, col.ind = groups, addEllipses = TRUE, legend.title = &#39;Species&#39;) Since this is a tiny and simple dataset frequently used in these types of tutorials, it’s no surprise that we get nice ellipses in the data. The data points separate well across the first PC (the x-axis). The fviz_contrib() plot shows the contribution of each variable. The horizontal red line here shows the expected level of contribution if the contributions were uniform. The axes = 1 argument states that I want to check for the contribution in the first PC only. fviz_contrib(pc_res, axes = 1, choice = &#39;var&#39;) Screeplot shows the level of covariance explained by each PC: fviz_screeplot(pc_res) Alternatively, dendrograms are a commonly used tools to visualize hierarchical clustering in the data. Again, the concept behind the clustering method will be explained in greater detail in future chapters, but essentially we need to generate a distance matrix (using some sort of a distance metric, in this case Euclidean distance) and then calculate linkage between each instance. The dist() and hclust() from base R handles this nicely: data(mtcars) dm &lt;- dist(mtcars, method = &#39;euclidean&#39;) hc &lt;- hclust(dm, method = &#39;complete&#39;) The dendextend package can accept the output from hclust to make customizable dendrograms: library(dendextend) den &lt;- as.dendrogram(hc) den %&gt;% plot() When we know the number of clusters present, we can color them differently with color_branches(). For an alternative look at dendrograms, we can circlize them using circlize_dendrogram(): den %&gt;% color_branches(k = 4) %&gt;% circlize_dendrogram() The cutree() function accepts the hclust() output and assigns a cluster label, depending on the number of clusters defined: cutree(hc, k = 4) %&gt;% head() ## Mazda RX4 Mazda RX4 Wag Datsun 710 ## 1 1 1 ## Hornet 4 Drive Hornet Sportabout Valiant ## 2 3 2 Finally, identifying the optimal number of clusters in the data can mainly be done using the silhouette method or the elbow (also called the wss) method: fviz_nbclust(mtcars, FUNcluster = hcut, method = &#39;silhouette&#39;, diss = dm) fviz_nbclust(mtcars, FUNcluster = hcut, method = &#39;wss&#39;, diss = dm) "],["everyday-ml-classification.html", "Chapter 6 Everyday ML: Classification 6.1 Model training and predictions 6.2 Feature selection using univariate filters 6.3 Feature selection using recursive feature elimination 6.4 Feature selection for correlated and low-variance predictors 6.5 Hyperparameter tuning 6.6 ROC and precision-recall curves 6.7 Model comparisons", " Chapter 6 Everyday ML: Classification In the preceding chapters, I reviewed the fundamentals of wrangling data as well as running some exploratory data analysis to get a feel for the data at hand. In data science projects, it is often typical to frame problems in context of a model - how does a variable y behave according to some other variable x? For example, how does the pricing of a residential property behave according to the square footage? Is the relationship linear? Are there confounding variables that affect this relationship we have not accounted for? In the simplest sense, fitting a linear model using ordinary least squares using lm() in R provide us with two parameters: the coefficient and the intercept. We can use these parameters to predict the housing price of a property based on the input feature - or features most likely - of that particular instance. This is the fundamental concept at the core of supervised learning. This example is a type of a regression as the target variable (i.e., the housing price) is a continuous variable. However, if the variable we were trying to predict is categorical (e.g., bins based on the bracket of housing price) the task would be classification. The digression into concepts in ML and the details into each algorithm is beyond the scope of this book, but more details around specific topics are available on my blog as well as documentation for popular ML packages such as Python’s Scikit-Learn. In R, the workhorse of supervised learning models, whether it’s classification or regression, is the caret package. Recently, the development of the package tidymodels has made implementation of ML much easier, with incorporation of packages such as parsnip. Tidymodels is especially convenient as it aims to remain consistent with the syntax from the tidyverse suite of data science packages. In this chapter however, I will use caret as I believe it is still very commonly used today and retains a backlog of useful related links on public forums such as Stack Overflow. library(tidyverse) library(caret) 6.1 Model training and predictions For the first exercise I will use a dataset from Kaggle here which I also uploaded onto my GitHub for reference: url &lt;- &#39;https://raw.githubusercontent.com/snowoflondon/everyday-r/main/datasets/Class_Winequality.csv&#39; df &lt;- read_delim(url, delim = &#39;;&#39;) ## Rows: 4898 Columns: 12 ## ── Column specification ────────────────────────────────────────── ## Delimiter: &quot;;&quot; ## dbl (12): fixed acidity, volatile acidity, citric acid, residu... ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. head(df) ## # A tibble: 6 × 12 ## `fixed acidity` `volatile acidity` `citric acid` ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 7 0.27 0.36 ## 2 6.3 0.3 0.34 ## 3 8.1 0.28 0.4 ## 4 7.2 0.23 0.32 ## 5 7.2 0.23 0.32 ## 6 8.1 0.28 0.4 ## # ℹ 9 more variables: `residual sugar` &lt;dbl&gt;, chlorides &lt;dbl&gt;, ## # `free sulfur dioxide` &lt;dbl&gt;, `total sulfur dioxide` &lt;dbl&gt;, ## # density &lt;dbl&gt;, pH &lt;dbl&gt;, sulphates &lt;dbl&gt;, alcohol &lt;dbl&gt;, ## # quality &lt;dbl&gt; This dataset has 11 features and a target label column called quality. Firstly, I convert the quality column into factors to reiterate the fact that we are working with a categorical column with defined levels. df &lt;- df %&gt;% mutate(quality = factor(quality)) %&gt;% relocate(quality) A glimpse into the 11 features shows us that the values are heterogenous in scale: summary(df %&gt;% select(-quality)) ## fixed acidity volatile acidity citric acid ## Min. : 3.800 Min. :0.0800 Min. :0.0000 ## 1st Qu.: 6.300 1st Qu.:0.2100 1st Qu.:0.2700 ## Median : 6.800 Median :0.2600 Median :0.3200 ## Mean : 6.855 Mean :0.2782 Mean :0.3342 ## 3rd Qu.: 7.300 3rd Qu.:0.3200 3rd Qu.:0.3900 ## Max. :14.200 Max. :1.1000 Max. :1.6600 ## residual sugar chlorides free sulfur dioxide ## Min. : 0.600 Min. :0.00900 Min. : 2.00 ## 1st Qu.: 1.700 1st Qu.:0.03600 1st Qu.: 23.00 ## Median : 5.200 Median :0.04300 Median : 34.00 ## Mean : 6.391 Mean :0.04577 Mean : 35.31 ## 3rd Qu.: 9.900 3rd Qu.:0.05000 3rd Qu.: 46.00 ## Max. :65.800 Max. :0.34600 Max. :289.00 ## total sulfur dioxide density pH ## Min. : 9.0 Min. :0.9871 Min. :2.720 ## 1st Qu.:108.0 1st Qu.:0.9917 1st Qu.:3.090 ## Median :134.0 Median :0.9937 Median :3.180 ## Mean :138.4 Mean :0.9940 Mean :3.188 ## 3rd Qu.:167.0 3rd Qu.:0.9961 3rd Qu.:3.280 ## Max. :440.0 Max. :1.0390 Max. :3.820 ## sulphates alcohol ## Min. :0.2200 Min. : 8.00 ## 1st Qu.:0.4100 1st Qu.: 9.50 ## Median :0.4700 Median :10.40 ## Mean :0.4898 Mean :10.51 ## 3rd Qu.:0.5500 3rd Qu.:11.40 ## Max. :1.0800 Max. :14.20 For a quick exploratory analysis, take a look at the distribution of the features and their scales (i.e., y-axis). Typically in ML tasks, the scales need to be preprocessed prior to model training. This isn’t necessarily the case in models like the random forest, for example, but it is good practice regardless. I will circle back to this in a bit. library(RColorBrewer) dfm &lt;- df %&gt;% pivot_longer(-quality, names_to = &#39;feature&#39;, values_to = &#39;values&#39;) dfm %&gt;% ggplot(aes(x = quality, y = values)) + geom_boxplot(aes(fill = quality), alpha = .6) + facet_wrap(~ feature, scales = &#39;free&#39;) + theme_bw() + theme(legend.position = &#39;none&#39;) + scale_fill_brewer(palette = &#39;Set1&#39;) Before doing any kind of pre-processing or normalization, it is imperative to split the data into training and testing to prevent information leak. The createDataPartition() function accepts the p = argument which defines the split fraction. Here I use 80/20 split. set.seed(42) df &lt;- df %&gt;% mutate(quality = paste0(&#39;Group_&#39;, quality)) %&gt;% mutate(quality = factor(quality)) idx &lt;- createDataPartition(y = df$quality, p = .8, list = FALSE, times = 1) df_train &lt;- df[idx,] df_test &lt;- df[-idx,] The createDataPartition() outputs an array of indices which can be used to split the original data. Going back to the talk of scaling and pre-processing the data: a common procedure is to center and scale, that is - subtract the mean and divide by the standard deviation. If you’re familiar with scikit-learn in Python, this is analogous to running StandardScaler(). preProcObj &lt;- preProcess(df_train, method = c(&#39;center&#39;, &#39;scale&#39;)) preProcObj ## Created from 3920 samples and 12 variables ## ## Pre-processing: ## - centered (11) ## - ignored (1) ## - scaled (11) Evidently, the preProcess() function recognized the column containing the target labels and ignored it for pre-processing. Pre-processing is done on the training data and the learned object is applied to both the training and testing data: df_train &lt;- predict(preProcObj, df_train) df_test &lt;- predict(preProcObj, df_test) Revisiting the features now shows the effect of the preprocessing step: summary(df_train %&gt;% select(-quality)) ## fixed acidity volatile acidity citric acid ## Min. :-3.63462 Min. :-1.9812 Min. :-2.7409 ## 1st Qu.:-0.66724 1st Qu.:-0.6815 1st Qu.:-0.5247 ## Median :-0.07376 Median :-0.1816 Median :-0.1143 ## Mean : 0.00000 Mean : 0.0000 Mean : 0.0000 ## 3rd Qu.: 0.63841 3rd Qu.: 0.4182 3rd Qu.: 0.4602 ## Max. : 8.70968 Max. : 7.2663 Max. :10.8844 ## residual sugar chlorides free sulfur dioxide ## Min. :-1.1408 Min. :-1.6600 Min. :-1.89691 ## 1st Qu.:-0.9250 1st Qu.:-0.4457 1st Qu.:-0.72325 ## Median :-0.2384 Median :-0.1309 Median :-0.07773 ## Mean : 0.0000 Mean : 0.0000 Mean : 0.00000 ## 3rd Qu.: 0.6835 3rd Qu.: 0.1840 3rd Qu.: 0.62647 ## Max. :11.6490 Max. :13.4966 Max. :14.88647 ## total sulfur dioxide density pH ## Min. :-3.0417 Min. :-2.2943 Min. :-2.99893 ## 1st Qu.:-0.6983 1st Qu.:-0.7665 1st Qu.:-0.65873 ## Median :-0.1065 Median :-0.1122 Median :-0.05697 ## Mean : 0.0000 Mean : 0.0000 Mean : 0.00000 ## 3rd Qu.: 0.6746 3rd Qu.: 0.7082 3rd Qu.: 0.61166 ## Max. : 7.1367 Max. :14.9270 Max. : 4.22224 ## sulphates alcohol ## Min. :-2.4096 Min. :-2.05259 ## 1st Qu.:-0.7084 1st Qu.:-0.83024 ## Median :-0.1711 Median :-0.09683 ## Mean : 0.0000 Mean : 0.00000 ## 3rd Qu.: 0.5452 3rd Qu.: 0.71807 ## Max. : 4.4850 Max. : 2.99978 The scales have been normalized, as evident here: df_train %&gt;% pivot_longer(-quality, names_to = &#39;feature&#39;, values_to = &#39;values&#39;) %&gt;% ggplot(aes(x = quality, y = values)) + geom_boxplot(aes(fill = quality), alpha = .6) + facet_wrap(~ feature, scales = &#39;free&#39;) + theme_bw() + theme(legend.position = &#39;none&#39;) + scale_fill_brewer(palette = &#39;Set1&#39;) Once we’re ready to train the model, an important function is trainControl(). Here, typically we define the sampling method for the model training. I am using method = cv with number = 5 for k-fold cross-validation with 5 folds. Alternatively, I could use method = repeatedcv with number = 5 and repeats = 5 for repeated cross-validation with 5 iterations, but for this exercise I will settle with the simple 5-fold cross validation. tr &lt;- trainControl(method = &#39;cv&#39;, number = 5, classProbs = TRUE) model &lt;- train(quality ~ ., data = df_train, method = &#39;ranger&#39;, importance = &#39;impurity&#39;, trControl = tr) Above, I defined method = ranger within train(), which is a wrapper for training a random forest model. For all available methods for train(), see caret’s documentation here. The importance = 'impurity' asks the model to use the Gini impurity method to rank variable importance. This will be useful later. Calling the model object summarizes the model’s performance on the validation set (i.e., hold-out sets during k-fold cross validation). model ## Random Forest ## ## 3920 samples ## 11 predictor ## 7 classes: &#39;Group_3&#39;, &#39;Group_4&#39;, &#39;Group_5&#39;, &#39;Group_6&#39;, &#39;Group_7&#39;, &#39;Group_8&#39;, &#39;Group_9&#39; ## ## No pre-processing ## Resampling: Cross-Validated (5 fold) ## Summary of sample sizes: 3136, 3135, 3136, 3137, 3136 ## Resampling results across tuning parameters: ## ## mtry splitrule Accuracy Kappa ## 2 gini 0.6673350 0.4788991 ## 2 extratrees 0.6691211 0.4770672 ## 6 gini 0.6576428 0.4669154 ## 6 extratrees 0.6668264 0.4778833 ## 11 gini 0.6558499 0.4648450 ## 11 extratrees 0.6647859 0.4756944 ## ## Tuning parameter &#39;min.node.size&#39; was held constant at a value of 1 ## Accuracy was used to select the optimal model using the ## largest value. ## The final values used for the model were mtry = 2, splitrule ## = extratrees and min.node.size = 1. Various hyperparametes were tested and the combination with the highest validation accuracy was chosen: model$bestTune ## mtry splitrule min.node.size ## 2 2 extratrees 1 The performance on the resamples during the cross validation process can be found here: model$resample ## Accuracy Kappa Resample ## 1 0.6530612 0.4487323 Fold1 ## 2 0.6811224 0.4989916 Fold3 ## 3 0.6942675 0.5131632 Fold2 ## 4 0.6683673 0.4796363 Fold5 ## 5 0.6487867 0.4448126 Fold4 The testing dataset has not been touched at all during model training. For model evaluation, above model is tested on this hold-out set using predict(): pred &lt;- predict(model, df_test) For a clean summary of model evaluation, use confusionMatrix(): confusionMatrix(data = pred, reference = df_test$quality, mode = &#39;prec_recall&#39;) ## Confusion Matrix and Statistics ## ## Reference ## Prediction Group_3 Group_4 Group_5 Group_6 Group_7 Group_8 ## Group_3 0 0 0 0 0 0 ## Group_4 0 7 1 0 0 0 ## Group_5 3 14 217 55 3 1 ## Group_6 1 11 71 360 82 15 ## Group_7 0 0 2 23 90 7 ## Group_8 0 0 0 1 1 12 ## Group_9 0 0 0 0 0 0 ## Reference ## Prediction Group_9 ## Group_3 0 ## Group_4 0 ## Group_5 0 ## Group_6 1 ## Group_7 0 ## Group_8 0 ## Group_9 0 ## ## Overall Statistics ## ## Accuracy : 0.7014 ## 95% CI : (0.6717, 0.73) ## No Information Rate : 0.4489 ## P-Value [Acc &gt; NIR] : &lt; 2.2e-16 ## ## Kappa : 0.533 ## ## Mcnemar&#39;s Test P-Value : NA ## ## Statistics by Class: ## ## Class: Group_3 Class: Group_4 Class: Group_5 ## Precision NA 0.875000 0.7406 ## Recall 0.00000 0.218750 0.7457 ## F1 NA 0.350000 0.7432 ## Prevalence 0.00409 0.032720 0.2975 ## Detection Rate 0.00000 0.007157 0.2219 ## Detection Prevalence 0.00000 0.008180 0.2996 ## Balanced Accuracy 0.50000 0.608846 0.8175 ## Class: Group_6 Class: Group_7 Class: Group_8 ## Precision 0.6654 0.73770 0.85714 ## Recall 0.8200 0.51136 0.34286 ## F1 0.7347 0.60403 0.48980 ## Prevalence 0.4489 0.17996 0.03579 ## Detection Rate 0.3681 0.09202 0.01227 ## Detection Prevalence 0.5532 0.12474 0.01431 ## Balanced Accuracy 0.7421 0.73573 0.67037 ## Class: Group_9 ## Precision NA ## Recall 0.000000 ## F1 NA ## Prevalence 0.001022 ## Detection Rate 0.000000 ## Detection Prevalence 0.000000 ## Balanced Accuracy 0.500000 Certain models such as the random forest have built-in feature importance. During model training, I defined importance = 'impurity', which means that the feature importance is calculated using the mean decrease in impurity after permutation of a given feature. Accessing this information is useful when we want to know which variables have the greatest influence on model performance and conversely, which ones have the least. varImp(model)$importance ## Overall ## `fixed acidity` 0.000000 ## `volatile acidity` 35.165026 ## `citric acid` 8.761852 ## `residual sugar` 11.579565 ## chlorides 4.090038 ## `free sulfur dioxide` 14.832990 ## `total sulfur dioxide` 12.411664 ## density 38.947736 ## pH 7.484357 ## sulphates 2.801255 ## alcohol 100.000000 The variable importance score is automatically scaled so that the highest score is set to 100. This can be turned off using scale = FALSE within varImp(). A quick visualization of variable importance is useful: df_imp &lt;- varImp(model)$importance %&gt;% rownames_to_column(var = &#39;Var&#39;) %&gt;% as_tibble() %&gt;% arrange(desc(Overall)) ggplot(df_imp, aes(x = reorder(Var, Overall), y = Overall)) + geom_point(stat = &#39;identity&#39;, color = &#39;red&#39;) + geom_segment(aes(x = reorder(Var, Overall), xend = reorder(Var, Overall), y = 0, yend = Overall)) + theme_classic() + coord_flip() + xlab(&#39;&#39;) + ylab(&#39;Var. Imp.&#39;) + theme(text = element_text(size = 14)) 6.2 Feature selection using univariate filters When the dataset is considerably larger, the number of n features may grow extremely large. In these scenarios, it may be advisable to reduce the number of features to save computation time but also to reduce model complexity. Of course, dimensionality reduction is possible, though this transforms the data and the original meaning of the features is lost. An alternative method is feature selection - selecting important features and discarding unimportant ones. This relates specifically to the concept of feature importance in the previous section. caret offers a simple way to rank features when built-in feature importance measures are not available. This is by using univariate filters, which are essentially fitting n individual models (where n is the number of features) against the target label and ranking them based on their statistical significance. anoveScores() is used for classification models and fits an ANOVA for each feature against the label. The null hypothesis here assumes the mean values for each feature is equal for all labels. gamScores() is used for regression models and uses a generalized additive model to look for functional relationships between the features and the label. In both cases, each feature in the predictor set is passed individually. For this part I will use the Sonar dataset from mlbench. library(mlbench) data(Sonar) Sonar &lt;- as_tibble(Sonar) Sonar ## # A tibble: 208 × 61 ## V1 V2 V3 V4 V5 V6 V7 V8 V9 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.02 0.0371 0.0428 0.0207 0.0954 0.0986 0.154 0.160 0.311 ## 2 0.0453 0.0523 0.0843 0.0689 0.118 0.258 0.216 0.348 0.334 ## 3 0.0262 0.0582 0.110 0.108 0.0974 0.228 0.243 0.377 0.560 ## 4 0.01 0.0171 0.0623 0.0205 0.0205 0.0368 0.110 0.128 0.0598 ## 5 0.0762 0.0666 0.0481 0.0394 0.059 0.0649 0.121 0.247 0.356 ## 6 0.0286 0.0453 0.0277 0.0174 0.0384 0.099 0.120 0.183 0.210 ## 7 0.0317 0.0956 0.132 0.141 0.167 0.171 0.0731 0.140 0.208 ## 8 0.0519 0.0548 0.0842 0.0319 0.116 0.0922 0.103 0.0613 0.146 ## 9 0.0223 0.0375 0.0484 0.0475 0.0647 0.0591 0.0753 0.0098 0.0684 ## 10 0.0164 0.0173 0.0347 0.007 0.0187 0.0671 0.106 0.0697 0.0962 ## # ℹ 198 more rows ## # ℹ 52 more variables: V10 &lt;dbl&gt;, V11 &lt;dbl&gt;, V12 &lt;dbl&gt;, ## # V13 &lt;dbl&gt;, V14 &lt;dbl&gt;, V15 &lt;dbl&gt;, V16 &lt;dbl&gt;, V17 &lt;dbl&gt;, ## # V18 &lt;dbl&gt;, V19 &lt;dbl&gt;, V20 &lt;dbl&gt;, V21 &lt;dbl&gt;, V22 &lt;dbl&gt;, ## # V23 &lt;dbl&gt;, V24 &lt;dbl&gt;, V25 &lt;dbl&gt;, V26 &lt;dbl&gt;, V27 &lt;dbl&gt;, ## # V28 &lt;dbl&gt;, V29 &lt;dbl&gt;, V30 &lt;dbl&gt;, V31 &lt;dbl&gt;, V32 &lt;dbl&gt;, ## # V33 &lt;dbl&gt;, V34 &lt;dbl&gt;, V35 &lt;dbl&gt;, V36 &lt;dbl&gt;, V37 &lt;dbl&gt;, … The target labels in Sonar has two classes: Sonar$Class %&gt;% str() ## Factor w/ 2 levels &quot;M&quot;,&quot;R&quot;: 2 2 2 2 2 2 2 2 2 2 ... Since this is a classification task, I will use anovaScores() to output a score for each feature. fit_anova &lt;- function(x, y) { anova_res &lt;- apply(x, 2, function(f) {anovaScores(f, y)}) return(anova_res) } aov_res &lt;- fit_anova(x = select(Sonar, -Class), y = Sonar$Class) aov_res &lt;- as.data.frame(aov_res) head(aov_res) ## aov_res ## V1 0.0000719490 ## V2 0.0007779402 ## V3 0.0054167746 ## V4 0.0002607819 ## V5 0.0012544689 ## V6 0.0567376774 The output for each feature is the p-value for the whole model F-test. These can be ranked to find the features with the greatest degree of relationship with the target labels: aov_res &lt;- aov_res %&gt;% rownames_to_column(var = &#39;Var&#39;) %&gt;% as_tibble() %&gt;% rename(pVal = aov_res) %&gt;% arrange(aov_res) aov_res ## # A tibble: 60 × 2 ## Var pVal ## &lt;chr&gt; &lt;dbl&gt; ## 1 V11 6.59e-11 ## 2 V12 4.64e- 9 ## 3 V49 1.96e- 7 ## 4 V10 4.60e- 7 ## 5 V45 5.30e- 7 ## 6 V48 1.19e- 6 ## 7 V9 2.20e- 6 ## 8 V13 4.22e- 6 ## 9 V46 7.16e- 6 ## 10 V47 9.49e- 6 ## # ℹ 50 more rows 6.3 Feature selection using recursive feature elimination An alternative method for feature selection is recursive feature elimination (RFE). RFE is a wrapper method that uses another model to rank features based on variable importance. This model does not have to be the same model used in the downstream model prediction task. The feature importance ranking method depends which model the RFE wrapper uses. Tree models such as the random forest, as previous mentioned, can use impurity scores or mean accuracy decrease to calculate this. The rfeControl() function specifies the RFE model as well as the resampling method. Then rfe() runs the algorithm to identify important features as well as the model accuracy as the RFE recursively removes the less important features and trains the model. rfec &lt;- rfeControl(functions = rfFuncs, method = &#39;cv&#39;, number = 5) rfeObj &lt;- rfe(x = select(Sonar, -Class), y = Sonar$Class, rfeControl = rfec) Calling the output shows that the top 5 most important features show overlap with the result from anovaScores() from the previous section, which is good. It also shows that keeping the original 60 features here shows the best model accuracy, which is fine. This won’t always be the case with increasing number of dimensions in the data. rfeObj ## ## Recursive feature selection ## ## Outer resampling method: Cross-Validated (5 fold) ## ## Resampling performance over subset size: ## ## Variables Accuracy Kappa AccuracySD KappaSD Selected ## 4 0.7212 0.4398 0.05514 0.11064 ## 8 0.7308 0.4557 0.04581 0.09426 ## 16 0.7790 0.5539 0.05389 0.10782 ## 60 0.8176 0.6290 0.06359 0.12886 * ## ## The top 5 variables (out of 60): ## V11, V12, V9, V10, V48 The fitted model and its performance can be retrieved as such: rfeObj$fit ## ## Call: ## randomForest(x = x, y = y, importance = TRUE) ## Type of random forest: classification ## Number of trees: 500 ## No. of variables tried at each split: 7 ## ## OOB estimate of error rate: 15.87% ## Confusion matrix: ## M R class.error ## M 101 10 0.09009009 ## R 23 74 0.23711340 The ranking of the features can be retrieved here, which is useful if we were to select the first few and subset our original dataset: rfeObj$optVariables ## [1] &quot;V11&quot; &quot;V12&quot; &quot;V9&quot; &quot;V10&quot; &quot;V48&quot; &quot;V47&quot; &quot;V36&quot; &quot;V49&quot; &quot;V28&quot; &quot;V45&quot; ## [11] &quot;V37&quot; &quot;V21&quot; &quot;V46&quot; &quot;V16&quot; &quot;V13&quot; &quot;V17&quot; &quot;V51&quot; &quot;V27&quot; &quot;V4&quot; &quot;V15&quot; ## [21] &quot;V20&quot; &quot;V18&quot; &quot;V52&quot; &quot;V5&quot; &quot;V44&quot; &quot;V23&quot; &quot;V1&quot; &quot;V31&quot; &quot;V43&quot; &quot;V26&quot; ## [31] &quot;V22&quot; &quot;V35&quot; &quot;V19&quot; &quot;V30&quot; &quot;V14&quot; &quot;V8&quot; &quot;V39&quot; &quot;V34&quot; &quot;V32&quot; &quot;V24&quot; ## [41] &quot;V54&quot; &quot;V42&quot; &quot;V29&quot; &quot;V59&quot; &quot;V55&quot; &quot;V25&quot; &quot;V2&quot; &quot;V6&quot; &quot;V41&quot; &quot;V58&quot; ## [51] &quot;V38&quot; &quot;V3&quot; &quot;V33&quot; &quot;V40&quot; &quot;V50&quot; &quot;V7&quot; &quot;V53&quot; &quot;V57&quot; &quot;V60&quot; &quot;V56&quot; Calling ggplot() on the RFE result provides a visual look: ggplot(rfeObj) + theme_bw() 6.4 Feature selection for correlated and low-variance predictors In some cases, there may exist a subset of features that are highly correlated with each other. While carrying these correlated variables do not necessarily impact model performance, it can (and often will!) affect model interpretation. For example, the coefficients of a linear regression model are sensitive to multicolinearity; there is a great explanation on why this is the case on Stack Exchange. Calculating variable importance can become tricky when there are correlated variables as well; if a given variable can easily be replaced by another correlated variable, it can be assigned a low importance value which may not actually be true according to our domain knowledge. Let’s see if there are any correlated variables in the Sonar dataset: library(corrplot) Sonar_corM &lt;- cor(Sonar %&gt;% select(where(is.numeric))) corrplot(Sonar_corM) Caret offers a way to filter features based on some cut-off based on correlation, which is implemented with findCorrelation(): highCorr &lt;- findCorrelation(Sonar_corM, cutoff = 0.9) highCorr ## [1] 15 18 20 The output corresponds to the indices of the columns to be removed based on our correlation coefficient cutoff of 0.9. We can then remove those 3 columns and move forward with our analysis. Another useful function is nearZeroVar(), which finds features that have very little variance. Near zero variance predictors may not be useful for prediction models and can be removed during feature selection. This function takes two main parameters: freqCut and uniqueCut - the former threshold is the ratio of the most common value to the second most common value while the latter threshold is the percentage of distinct values. Using default parameters of 95/5 and 10 respectively, we see that in the Sonar dataset we do not have any features that meet the criteria - so we’re good. nearZeroVar(Sonar %&gt;% select(-Class)) ## integer(0) 6.5 Hyperparameter tuning Previously when we trained the random forest model using train(), it automatically deduced the optimal values for the model hyperparameters. Under the hood, train() ran a grid search to find these values, but we can define our own grid as well. Hyperparameter tuning should of course be done on the training set, so I will use the Sonar dataset to arrive at the training and testing sets: idx &lt;- createDataPartition(y = Sonar$Class, p = .8, list = FALSE, times = 1) df_train &lt;- Sonar[idx,] df_test &lt;- Sonar[-idx,] preProcObj &lt;- preProcess(df_train, method = c(&#39;center&#39;, &#39;scale&#39;)) df_train &lt;- predict(preProcObj, df_train) df_test &lt;- predict(preProcObj, df_test) For the random forest model, I define the possible values for the three hyperparameters as such, and then train by providing an input for tuneGrid =. rf_grid &lt;- expand.grid(mtry = c(2, 4, 8, 10), splitrule = c(&quot;gini&quot;, &quot;extratrees&quot;), min.node.size = c(1, 3, 5)) model_rf &lt;- train(Class ~ ., data = df_train, method = &#39;ranger&#39;, importance = &#39;impurity&#39;, trControl = tr, tuneGrid = rf_grid) Calling the model then shows the model performance (with the specified resampling method) for each combination of the grid search: model_rf ## Random Forest ## ## 167 samples ## 60 predictor ## 2 classes: &#39;M&#39;, &#39;R&#39; ## ## No pre-processing ## Resampling: Cross-Validated (5 fold) ## Summary of sample sizes: 133, 133, 134, 133, 135 ## Resampling results across tuning parameters: ## ## mtry splitrule min.node.size Accuracy Kappa ## 2 gini 1 0.8151181 0.6228532 ## 2 gini 3 0.8327540 0.6590288 ## 2 gini 5 0.8090463 0.6096000 ## 2 extratrees 1 0.8207999 0.6350424 ## 2 extratrees 3 0.8211676 0.6358619 ## 2 extratrees 5 0.8022504 0.5957834 ## 4 gini 1 0.8329434 0.6587746 ## 4 gini 3 0.8210004 0.6353605 ## 4 gini 5 0.8268828 0.6467128 ## 4 extratrees 1 0.8206217 0.6332840 ## 4 extratrees 3 0.8325758 0.6576618 ## 4 extratrees 5 0.8450646 0.6844312 ## 8 gini 1 0.8208222 0.6363709 ## 8 gini 3 0.8268828 0.6495138 ## 8 gini 5 0.8208222 0.6357195 ## 8 extratrees 1 0.8445187 0.6823220 ## 8 extratrees 3 0.8388146 0.6720493 ## 8 extratrees 5 0.8272282 0.6488559 ## 10 gini 1 0.8329434 0.6599363 ## 10 gini 3 0.8388369 0.6717421 ## 10 gini 5 0.8267045 0.6480589 ## 10 extratrees 1 0.8206217 0.6339633 ## 10 extratrees 3 0.8441511 0.6816926 ## 10 extratrees 5 0.8265040 0.6457295 ## ## Accuracy was used to select the optimal model using the ## largest value. ## The final values used for the model were mtry = 4, splitrule ## = extratrees and min.node.size = 5. As before, the best set of hyperparameters can be retrieved: model_rf$bestTune ## mtry splitrule min.node.size ## 12 4 extratrees 5 Instead of a predefined grid search, we can do a randomized search instead. This can be done by setting search = 'random' within trainControl() first and then specifying tuneLength = in train(). Since we’ve only used random forest models so far, here I will do a similar grid search but using a support vector machine (SVM) with the radial basis function kernel instead: tr_svm &lt;- trainControl(method = &#39;cv&#39;, number = 5, classProbs = TRUE, search = &#39;random&#39;) model_svm &lt;- train(Class ~ ., data = df_train, method = &#39;svmRadial&#39;, trControl = tr_svm, tunelength = 8) Calling the model shows the best combination for C (cost) and sigma based on the model accuracy: model_svm ## Support Vector Machines with Radial Basis Function Kernel ## ## 167 samples ## 60 predictor ## 2 classes: &#39;M&#39;, &#39;R&#39; ## ## No pre-processing ## Resampling: Cross-Validated (5 fold) ## Summary of sample sizes: 133, 133, 133, 134, 135 ## Resampling results across tuning parameters: ## ## sigma C Accuracy Kappa ## 0.002678978 58.142065 0.7708445 0.5357187 ## 0.024781262 563.129570 0.8680481 0.7316302 ## 0.033519260 2.093291 0.8564617 0.7081256 ## ## Accuracy was used to select the optimal model using the ## largest value. ## The final values used for the model were sigma = 0.02478126 and ## C = 563.1296. 6.6 ROC and precision-recall curves A nice way to visualize model evaluation is by using ROC curves, which uses metrics that were already calculated previously - precision and recall. For this I will generate predictions for the random forest model using Sonar. Setting type = 'prob' yields probabilities for each label classification instead of the label itself: pred &lt;- predict(model_rf, df_test, type = &#39;prob&#39;) head(pred) ## M R ## 1 0.5870667 0.4129333 ## 2 0.6210333 0.3789667 ## 3 0.4531667 0.5468333 ## 4 0.2409667 0.7590333 ## 5 0.6803667 0.3196333 ## 6 0.1867333 0.8132667 The package MLeval can be used to generate ROC curves as such; here we achieve the ROC area under the curve of 0.96. library(MLeval) roc_rf &lt;- evalm(data.frame(pred, df_test$Class, Group = &#39;RF&#39;), showplots = FALSE, silent = TRUE) roc_rf$roc Similarly, a precision-recall curve can also be visualized. This curve shows the tradeoff between the two metrics for each threshold. roc_rf$proc The values can be retrieved here: roc_rf$optres ## $RF ## Score CI ## SENS 0.842 0.62-0.94 ## SPEC 0.909 0.72-0.97 ## MCC 0.755 &lt;NA&gt; ## Informedness 0.751 &lt;NA&gt; ## PREC 0.889 0.67-0.97 ## NPV 0.870 0.68-0.95 ## FPR 0.091 &lt;NA&gt; ## F1 0.865 &lt;NA&gt; ## TP 16.000 &lt;NA&gt; ## FP 2.000 &lt;NA&gt; ## TN 20.000 &lt;NA&gt; ## FN 3.000 &lt;NA&gt; ## AUC-ROC 0.930 0.84-1.02 ## AUC-PR 0.880 &lt;NA&gt; ## AUC-PRG 0.610 &lt;NA&gt; For completeness I will make these figures for the SVM model as well: pred2 &lt;- predict(model_svm, df_test, type = &#39;prob&#39;) roc_svm &lt;- evalm(data.frame(pred2, df_test$Class, Group = &#39;SVM&#39;), showplots = FALSE, silent = TRUE) roc_svm$roc roc_svm$proc 6.7 Model comparisons caret provides an elegant way to compare the performance of multiple models for model selection. We have two models trained on Sonar dataset already, so I will train two more. Here I am using a gradient boosted machine (gbm) and a k-nearest neighbors (knn). # model_rf # model_svm model_gbm &lt;- train(Class ~., data = df_train, method = &#39;gbm&#39;, trControl = tr, verbose = FALSE) model_knn &lt;- train(Class ~ ., data = df_train, method = &#39;knn&#39;, trControl = tr) model_gbm ## Stochastic Gradient Boosting ## ## 167 samples ## 60 predictor ## 2 classes: &#39;M&#39;, &#39;R&#39; ## ## No pre-processing ## Resampling: Cross-Validated (5 fold) ## Summary of sample sizes: 134, 133, 134, 133, 134 ## Resampling results across tuning parameters: ## ## interaction.depth n.trees Accuracy Kappa ## 1 50 0.8204991 0.6360105 ## 1 100 0.8557932 0.7079348 ## 1 150 0.8438503 0.6844607 ## 2 50 0.8074866 0.6089405 ## 2 100 0.8436720 0.6838739 ## 2 150 0.8495544 0.6952242 ## 3 50 0.8440285 0.6837620 ## 3 100 0.8436720 0.6844288 ## 3 150 0.8618538 0.7206228 ## ## Tuning parameter &#39;shrinkage&#39; was held constant at a value of ## 0.1 ## Tuning parameter &#39;n.minobsinnode&#39; was held constant at ## a value of 10 ## Accuracy was used to select the optimal model using the ## largest value. ## The final values used for the model were n.trees = ## 150, interaction.depth = 3, shrinkage = 0.1 and n.minobsinnode ## = 10. model_knn ## k-Nearest Neighbors ## ## 167 samples ## 60 predictor ## 2 classes: &#39;M&#39;, &#39;R&#39; ## ## No pre-processing ## Resampling: Cross-Validated (5 fold) ## Summary of sample sizes: 133, 133, 134, 134, 134 ## Resampling results across tuning parameters: ## ## k Accuracy Kappa ## 5 0.7852050 0.5633587 ## 7 0.7672014 0.5278251 ## 9 0.7433155 0.4775343 ## ## Accuracy was used to select the optimal model using the ## largest value. ## The final value used for the model was k = 5. Both accuracy and kappa are then used to compare the model performance across the four models: comps &lt;- resamples( list(RF = model_rf, SVM = model_svm, GBM = model_gbm, KNN = model_knn) ) summary(comps) ## ## Call: ## summary.resamples(object = comps) ## ## Models: RF, SVM, GBM, KNN ## Number of resamples: 5 ## ## Accuracy ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## RF 0.7941176 0.8235294 0.8484848 0.8450646 0.8529412 0.9062500 ## SVM 0.8181818 0.8235294 0.8750000 0.8680481 0.8823529 0.9411765 ## GBM 0.7575758 0.8529412 0.8787879 0.8618538 0.8787879 0.9411765 ## KNN 0.7058824 0.7272727 0.7352941 0.7852050 0.8484848 0.9090909 ## NA&#39;s ## RF 0 ## SVM 0 ## GBM 0 ## KNN 0 ## ## Kappa ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## RF 0.5853659 0.6382979 0.6857143 0.6844312 0.7017544 0.8110236 ## SVM 0.6250000 0.6382979 0.7490196 0.7316302 0.7638889 0.8819444 ## GBM 0.5056180 0.7017544 0.7555556 0.7206228 0.7582418 0.8819444 ## KNN 0.4055944 0.4469274 0.4593640 0.5633587 0.6892655 0.8156425 ## NA&#39;s ## RF 0 ## SVM 0 ## GBM 0 ## KNN 0 And finally, a quick visualization at the model performance comparisons: dotplot(comps) "],["everyday-ml-regression.html", "Chapter 7 Everyday ML: Regression 7.1 Simple linear regression 7.2 Using regression for prediction 7.3 Categorical predictors and factor encoding 7.4 Elastic net regression", " Chapter 7 Everyday ML: Regression In the previous chapter, the goal of our task was to predict each sample into one of N categorical variables. The fundamentals of such a classification task carries over to regression problems, where the goal is to predict a continuous variable instead. As such, a regression task involves taking data, fitting a model, evaluating the model, then predicting a new instance. The differences however, lies in exactly how we train the data - as in, how do we measure the ‘closeness’ of our model to the ground truth? - and how we evaluate the model in the end prior to deployment. The R package suite tidymodels handles regression tasks elegantly, however, in this chapter - to be consistent with the previous chapter - we will use caret again. As before, the explanation of ML concepts and the details into each algorithm is beyond the scope of this book, but more details are available on my blog as well as documentation for popular ML packages such as Python’s Scikit-Learn. library(tidyverse) library(caret) 7.1 Simple linear regression For this section we will use the Ozone dataset from mlbench, which involves prediction of the daily maximum one-hour-average ozone reading using various predictors such as humidity, temperature, and wind speed. data(Ozone, package = &quot;mlbench&quot;) colnames(Ozone) &lt;- c(&quot;Month&quot;, &quot;DayOfMonth&quot;, &quot;DayOfWeek&quot;, &quot;OzoneReading&quot;, &quot;PressureHeight&quot;, &quot;WindSpeed&quot;, &quot;Humidity&quot;, &quot;TemperatureSandburg&quot;, &quot;TemperatureElMonte&quot;, &quot;InversionBaseHeight&quot;, &quot;PressureGradient&quot;, &quot;InversionTemperature&quot;, &quot;Visibility&quot;) Ozone &lt;- as_tibble(Ozone) Ozone &lt;- Ozone %&gt;% select(-c(Month, DayOfMonth, DayOfWeek)) str(Ozone) ## tibble [366 × 10] (S3: tbl_df/tbl/data.frame) ## $ OzoneReading : num [1:366] 3 3 3 5 5 6 4 4 6 7 ... ## $ PressureHeight : num [1:366] 5480 5660 5710 5700 5760 5720 5790 5790 5700 5700 ... ## $ WindSpeed : num [1:366] 8 6 4 3 3 4 6 3 3 3 ... ## $ Humidity : num [1:366] 20 NA 28 37 51 69 19 25 73 59 ... ## $ TemperatureSandburg : num [1:366] NA 38 40 45 54 35 45 55 41 44 ... ## $ TemperatureElMonte : num [1:366] NA NA NA NA 45.3 ... ## $ InversionBaseHeight : num [1:366] 5000 NA 2693 590 1450 ... ## $ PressureGradient : num [1:366] -15 -14 -25 -24 25 15 -33 -28 23 -2 ... ## $ InversionTemperature: num [1:366] 30.6 NA 47.7 55 57 ... ## $ Visibility : num [1:366] 200 300 250 100 60 60 100 250 120 120 ... Identifying the number of missing values first is important: vapply(Ozone, function(x) sum(is.na(x)), FUN.VALUE = double(1)) ## OzoneReading PressureHeight WindSpeed ## 5 12 0 ## Humidity TemperatureSandburg TemperatureElMonte ## 15 2 139 ## InversionBaseHeight PressureGradient InversionTemperature ## 15 1 14 ## Visibility ## 0 For the purpose of this exercise, instead of imputation I will just discard the missing values. Ozone &lt;- na.omit(Ozone) dim(Ozone) ## [1] 203 10 For a quick look at the scales and the distribution of the variables, boxplots will do: Ozone %&gt;% pivot_longer(everything(), names_to = &#39;Var&#39;, values_to = &#39;Val&#39;) %&gt;% ggplot(aes(x = Var, y = Val)) + geom_boxplot() + facet_wrap(~ Var, scales = &#39;free&#39;) + theme_bw() As seen in the EDA chapter, a correlation plot can help visualize multicollinearity in the data: corrplot(cor(Ozone %&gt;% select(-OzoneReading)), is.corr = TRUE) Using caret’s findCorrelation() function, highly correlated predictors can be removed; the output corresponds to the indices of the highly correlated predictors to be removed. highCor &lt;- findCorrelation(cor(Ozone %&gt;% select(-OzoneReading)), cutoff = 0.9) highCorCols &lt;- Ozone %&gt;% select(-OzoneReading) %&gt;% select(any_of(highCor)) %&gt;% colnames() Ozone &lt;- Ozone %&gt;% select(-any_of(highCorCols)) highCorCols ## [1] &quot;InversionTemperature&quot; &quot;TemperatureElMonte&quot; A simple linear regression model using ordinary least squares is then built as such: lm_mod1 &lt;- lm(OzoneReading ~ ., data = Ozone) summary(lm_mod1) ## ## Call: ## lm(formula = OzoneReading ~ ., data = Ozone) ## ## Residuals: ## Min 1Q Median 3Q Max ## -12.5051 -3.3213 -0.2129 3.0129 13.8078 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -1.577e+01 3.482e+01 -0.453 0.651148 ## PressureHeight 6.846e-04 6.399e-03 0.107 0.914911 ## WindSpeed 1.217e-01 1.806e-01 0.674 0.501307 ## Humidity 9.021e-02 2.466e-02 3.658 0.000327 *** ## TemperatureSandburg 3.325e-01 5.093e-02 6.529 5.59e-10 *** ## InversionBaseHeight -7.348e-04 2.258e-04 -3.254 0.001341 ** ## PressureGradient -8.341e-03 1.370e-02 -0.609 0.543293 ## Visibility -7.132e-03 5.045e-03 -1.414 0.159029 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 4.686 on 195 degrees of freedom ## Multiple R-squared: 0.684, Adjusted R-squared: 0.6726 ## F-statistic: 60.29 on 7 and 195 DF, p-value: &lt; 2.2e-16 The ~ sign denotes that the OzoneReading variable (LHS) is predicted by . (RHS), which equates to the remaining variables in the data provided. The intercept of the model is fitted automatically. The ANOVA table of the fitted model, which contains information such as the sum of squares for each predictor, is retrieved with anova(): anova(lm_mod1) ## Analysis of Variance Table ## ## Response: OzoneReading ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## PressureHeight 1 4787.6 4787.6 218.0230 &lt; 2.2e-16 *** ## WindSpeed 1 704.5 704.5 32.0844 5.235e-08 *** ## Humidity 1 2241.2 2241.2 102.0647 &lt; 2.2e-16 *** ## TemperatureSandburg 1 1163.4 1163.4 52.9825 8.055e-12 *** ## InversionBaseHeight 1 315.0 315.0 14.3432 0.0002028 *** ## PressureGradient 1 11.9 11.9 0.5439 0.4616879 ## Visibility 1 43.9 43.9 1.9987 0.1590293 ## Residuals 195 4282.0 22.0 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 For quick interpretation, let’s print out the content of the lm model fitted: lm_mod1 ## ## Call: ## lm(formula = OzoneReading ~ ., data = Ozone) ## ## Coefficients: ## (Intercept) PressureHeight WindSpeed ## -1.577e+01 6.846e-04 1.217e-01 ## Humidity TemperatureSandburg InversionBaseHeight ## 9.021e-02 3.325e-01 -7.348e-04 ## PressureGradient Visibility ## -8.341e-03 -7.132e-03 Above result suggests that for each unit change in PressureHeight, the OzoneReading variable is increased by 6.846e-04. Of course, since this is a multiple regression task (i.e., there are multiple predictors), the interpretation of the coefficient value is not as straightforward. In ordinary least squares, the model attempts to minimize the residuals, which is the difference between the original data and the predicted data. Both of these can be retrieved as such: lm_mod1_pred &lt;- predict(lm_mod1) lm_mod1_resid &lt;- resid(lm_mod1) This means that we can plot these for a nice visualization between the residuals and the fitted values: plot(predict(lm_mod1), resid(lm_mod1), main = &#39;Residuals vs. Fitted&#39;, xlab = &#39;Fitted values&#39;, ylab = &#39;Residuals&#39;) A few observations can be made here; firstly, we see that the distribution of the points on either side of the residuals = 0 line seems random and evenly distributed. This suggests that modeling the current data with assumptions of linearity is valid. Secondly, there are no obvious outliers here, which means it is unlikely that our model is influenced by extreme values. The root-mean-squared-error (RMSE) is the standard measure to use when evaluating regression models; RMSE is the squared root of the mean squared error in the predicted values. RMSE(lm_mod1_pred, Ozone$OzoneReading) ## [1] 4.592772 7.2 Using regression for prediction With the advent of big data and ML, you are more and more likely to run into scenarios where regression is used to predict new data. In the previous example with the Ozone dataset, we fit the entire dataset using lm() and evaluated it using a within-sample method such as the RMSE. On the other hand, it’s possible to set aside a portion of the data, fit a model, use cross-validation metrics to tune the model, and evaluate the model one last time using never-before-seen data. Of course, with these concepts, we are firmly in the realm of ML - thus concepts from the previous chapter such as train/test split, model training, and model selection are carried over. idx &lt;- createDataPartition(y = Ozone$OzoneReading, p = .8, list = FALSE, times = 1) Ozone_train &lt;- Ozone[idx,] Ozone_test &lt;- Ozone[-idx,] paste0(&#39;Training data batch dimension: &#39;, nrow(Ozone_train)) ## [1] &quot;Training data batch dimension: 164&quot; paste0(&#39;Test data batch dimension: &#39;, nrow(Ozone_test)) ## [1] &quot;Test data batch dimension: 39&quot; As seen in the previous chapter, we split the original data using createDataPartition() to get the training and the test data. Then using trainControl() and train(), I can use a wrapper for a gradient boosted model for the regression task. tr &lt;- trainControl(method = &#39;cv&#39;, number = 5, search = &#39;grid&#39;) model &lt;- train(OzoneReading ~ ., data = Ozone_train, method = &#39;gbm&#39;, trControl = tr, verbose = FALSE) model ## Stochastic Gradient Boosting ## ## 164 samples ## 7 predictor ## ## No pre-processing ## Resampling: Cross-Validated (5 fold) ## Summary of sample sizes: 132, 132, 130, 131, 131 ## Resampling results across tuning parameters: ## ## interaction.depth n.trees RMSE Rsquared MAE ## 1 50 4.630898 0.6759574 3.655685 ## 1 100 4.641358 0.6784356 3.603800 ## 1 150 4.687776 0.6742934 3.632741 ## 2 50 4.714275 0.6712452 3.631883 ## 2 100 4.753206 0.6625908 3.584495 ## 2 150 4.786053 0.6596358 3.598532 ## 3 50 4.694933 0.6725721 3.549717 ## 3 100 4.803256 0.6622239 3.601409 ## 3 150 4.934647 0.6431102 3.693087 ## ## Tuning parameter &#39;shrinkage&#39; was held constant at a value of ## 0.1 ## Tuning parameter &#39;n.minobsinnode&#39; was held constant at ## a value of 10 ## RMSE was used to select the optimal model using the ## smallest value. ## The final values used for the model were n.trees = ## 50, interaction.depth = 1, shrinkage = 0.1 and n.minobsinnode ## = 10. Evidently, caret uses grid search to find the combination of hyperparameters - in this case, n.trees, interaction.depth, shrinkage, and n.minobsinnode - which minimizes the RMSE. We could have defined our own hyperparameter space using expand.grid() and using that as input for tuneGrid = within train(). Briefly, the shrinkage refers to the learning rate, which describes the size of the incremental steps in gradient descent calculations. The interaction.depth describes the number of splits per tree and the n.trees describes the total number of trees, which means that for higher values of n.trees, the model complexity increases as well as the risk of overfitting the model. Calling plot() method to the model object prints the boosting iterations versus the value we’re trying to optimize (i.e., RMSE): plot(model) Evidently, the interaction depth (i.e., tree depth) of 1 reaches the minimum RMSE at 100 boosting iterations. The best set of hyperparameters can be printed as such: model$bestTune ## n.trees interaction.depth shrinkage n.minobsinnode ## 1 50 1 0.1 10 The holdout data is then used to generate predictions and calculate the RMSE and the R squared: preds &lt;- predict(object = model, newdata = Ozone_test) rmse &lt;- RMSE(pred = preds, obs = Ozone_test$OzoneReading) r2 &lt;- R2(pred = preds, obs = Ozone_test$OzoneReading) paste0(&#39;Model RMSE: &#39;, rmse) ## [1] &quot;Model RMSE: 4.45076380391138&quot; paste0(&#39;Model R2: &#39;, r2) ## [1] &quot;Model R2: 0.750807931113192&quot; plot(x = preds, y = Ozone_test$OzoneReading) 7.3 Categorical predictors and factor encoding Regression models typically require predictors to be numerical. Therefore, categorical predictors (i.e., factor variables which take on discrete values) need to be numerically encoded. There are two mainly used methods of numerical encoding of categorical predictors: Reference coding: one class of categorical predictor is used as a reference and the rest of the classes are compared to the reference. One-hot encoding: the number of unique classes is retained. The categorical variables are encoded as numeric arrays. This method is also called ‘dummy encoding.’ Let’s load the BostonHousing2 data from mlbench and look at the columns: data(BostonHousing2, package = &#39;mlbench&#39;) BH2 &lt;- tibble(BostonHousing2) str(BH2) ## tibble [506 × 19] (S3: tbl_df/tbl/data.frame) ## $ town : Factor w/ 92 levels &quot;Arlington&quot;,&quot;Ashland&quot;,..: 54 77 77 46 46 46 69 69 69 69 ... ## $ tract : int [1:506] 2011 2021 2022 2031 2032 2033 2041 2042 2043 2044 ... ## $ lon : num [1:506] -71 -71 -70.9 -70.9 -70.9 ... ## $ lat : num [1:506] 42.3 42.3 42.3 42.3 42.3 ... ## $ medv : num [1:506] 24 21.6 34.7 33.4 36.2 28.7 22.9 27.1 16.5 18.9 ... ## $ cmedv : num [1:506] 24 21.6 34.7 33.4 36.2 28.7 22.9 22.1 16.5 18.9 ... ## $ crim : num [1:506] 0.00632 0.02731 0.02729 0.03237 0.06905 ... ## $ zn : num [1:506] 18 0 0 0 0 0 12.5 12.5 12.5 12.5 ... ## $ indus : num [1:506] 2.31 7.07 7.07 2.18 2.18 2.18 7.87 7.87 7.87 7.87 ... ## $ chas : Factor w/ 2 levels &quot;0&quot;,&quot;1&quot;: 1 1 1 1 1 1 1 1 1 1 ... ## $ nox : num [1:506] 0.538 0.469 0.469 0.458 0.458 0.458 0.524 0.524 0.524 0.524 ... ## $ rm : num [1:506] 6.58 6.42 7.18 7 7.15 ... ## $ age : num [1:506] 65.2 78.9 61.1 45.8 54.2 58.7 66.6 96.1 100 85.9 ... ## $ dis : num [1:506] 4.09 4.97 4.97 6.06 6.06 ... ## $ rad : int [1:506] 1 2 2 3 3 3 5 5 5 5 ... ## $ tax : int [1:506] 296 242 242 222 222 222 311 311 311 311 ... ## $ ptratio: num [1:506] 15.3 17.8 17.8 18.7 18.7 18.7 15.2 15.2 15.2 15.2 ... ## $ b : num [1:506] 397 397 393 395 397 ... ## $ lstat : num [1:506] 4.98 9.14 4.03 2.94 5.33 ... The target variable is cmedv - the corrected median value of homes. There are additionally two categorical predictors here: town and chas. The chas variable has already been numerically encoded so we just need to encode town. paste0(&#39;# of levels: &#39;, length(levels(BH2$town))) ## [1] &quot;# of levels: 92&quot; There are nearly 100 levels in town, so for the sake of this exercise I will truncate the dataset so that it only contains the top 5 largest towns by size: BH2 %&gt;% group_by(town) %&gt;% tally() %&gt;% arrange(desc(n)) %&gt;% head(5) ## # A tibble: 5 × 2 ## town n ## &lt;fct&gt; &lt;int&gt; ## 1 Cambridge 30 ## 2 Boston Savin Hill 23 ## 3 Lynn 22 ## 4 Boston Roxbury 19 ## 5 Newton 18 toptowns &lt;- BH2 %&gt;% group_by(town) %&gt;% tally() %&gt;% arrange(desc(n)) %&gt;% head(5) %&gt;% pull(town) BH2 &lt;- BH2 %&gt;% filter(town %in% toptowns) %&gt;% mutate(town = factor(town, levels = toptowns)) Note I needed to re-factor the town variable after I reduced the number of levels. Firstly, let’s see what happens if we fit the lm() model without encoding the town variable: BH2 &lt;- BH2 %&gt;% select(-medv) bh_mod1 &lt;- lm(cmedv ~ ., data = BH2) summary(bh_mod1) ## ## Call: ## lm(formula = cmedv ~ ., data = BH2) ## ## Residuals: ## Min 1Q Median 3Q Max ## -7.3640 -1.9905 -0.1141 1.6872 9.5167 ## ## Coefficients: (5 not defined because of singularities) ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -2.248e+02 3.390e+03 -0.066 0.947265 ## townBoston Savin Hill -3.406e+02 1.830e+02 -1.862 0.065724 . ## townLynn -1.862e+02 1.028e+02 -1.811 0.073210 . ## townBoston Roxbury -3.545e+02 1.898e+02 -1.868 0.064835 . ## townNewton 1.788e+01 1.218e+01 1.468 0.145481 ## tract -1.258e-01 6.939e-02 -1.813 0.072989 . ## lon -3.051e+01 4.140e+01 -0.737 0.462944 ## lat -3.524e+01 5.507e+01 -0.640 0.523801 ## crim 7.705e-02 4.645e-02 1.659 0.100452 ## zn NA NA NA NA ## indus NA NA NA NA ## chas1 -1.288e+00 1.114e+00 -1.157 0.250237 ## nox -2.920e+01 6.722e+00 -4.343 3.48e-05 *** ## rm 7.822e+00 6.910e-01 11.320 &lt; 2e-16 *** ## age -9.919e-02 2.899e-02 -3.422 0.000915 *** ## dis -3.442e+00 1.573e+00 -2.188 0.031091 * ## rad NA NA NA NA ## tax NA NA NA NA ## ptratio NA NA NA NA ## b 1.840e-02 3.564e-03 5.163 1.31e-06 *** ## lstat -2.028e-01 8.412e-02 -2.411 0.017803 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.286 on 96 degrees of freedom ## Multiple R-squared: 0.9167, Adjusted R-squared: 0.9037 ## F-statistic: 70.48 on 15 and 96 DF, p-value: &lt; 2.2e-16 The categorical variables have evidently been reference-encoded: in the model output, there are 4 total coefficients belonging to the town variable, which is one less than the total number of classes in town (5). In this case, Cambridge class in town was used as the reference and the remaining 4 classes are interpreted relative to it. Alternatively, a categorical variable can be one-hot encoded using model.matrix(), which outputs a sparse matrix with values of 1 and 0: town_encoded &lt;- model.matrix(~town -1, data = BH2) head(town_encoded) ## townCambridge townBoston Savin Hill townLynn townBoston Roxbury ## 1 0 0 1 0 ## 2 0 0 1 0 ## 3 0 0 1 0 ## 4 0 0 1 0 ## 5 0 0 1 0 ## 6 0 0 1 0 ## townNewton ## 1 0 ## 2 0 ## 3 0 ## 4 0 ## 5 0 ## 6 0 The predictors are named similarly to the example of reference coding, such that the predictor name is concatenated to each class name. For each observation, the corresponding categorical level is filled with 1 while the remaining levels are filled with 0. For example, as the first 22 rows correspond to homes in Lynn, the townLynn column is populated with 1s and the others 0s. Using these encoded variables in lieu of the original factors is the standard approach in certain machine learning models, such as decision trees and ensemble learners. 7.4 Elastic net regression One of the biggest challenges in predictive modeling is the balance between underfitting and overfitting to the training data. A widely used regularization strategy (i.e., constrain the model to reduce model complexity) is to add a penalty (or a cost) term to the objective function. In the case of simple linear regression, this would be like adding an extra term to the MSE function. The L1 and L2 regularization techniques refer to adding an extra term to the objective function - namely, the L1 and L2 vector norms. The L1 vector norm describes the sum of the absolute values of the vector while the L2 norm describes the squared root of the squared values of the vector. The details are beyond the scope of this book, but for further reading check out my blog post on this subject. As such, in linear models this turns out to be: \\(J(\\theta) = MSE(\\theta) + \\alpha\\frac{1}{2}\\sum_{i = 1}^{n} |\\theta_{i}|\\) \\(J(\\theta) = MSE(\\theta) + \\alpha\\frac{1}{2}\\sum_{i = 1}^{n} \\theta_{i}^{2}\\) In regression, incorporating L1 and L2 regularization is as known as Lasso and Ridge regression, respectively. Meanwhile, Elastic Net regression uses a mixture of both techniques, with the hyperparameter \\(\\alpha\\) describing the ratio of the two. In R, this can be implemented with the back-end glmnet which can be run on its own or as a wrapper in caret. data(mtcars) eln_mod &lt;- train(mpg ~ ., data = mtcars, method = &#39;glmnet&#39;, preProcess = c(&#39;center&#39;, &#39;scale&#39;), trControl = tr) eln_mod ## glmnet ## ## 32 samples ## 10 predictors ## ## Pre-processing: centered (10), scaled (10) ## Resampling: Cross-Validated (5 fold) ## Summary of sample sizes: 25, 25, 26, 25, 27 ## Resampling results across tuning parameters: ## ## alpha lambda RMSE Rsquared MAE ## 0.10 0.01029396 4.452870 0.6256119 3.654107 ## 0.10 0.10293962 3.913182 0.6894015 3.233255 ## 0.10 1.02939621 3.104474 0.8032698 2.540240 ## 0.55 0.01029396 4.423287 0.6291525 3.629092 ## 0.55 0.10293962 3.661364 0.7244250 3.012562 ## 0.55 1.02939621 3.168239 0.7940001 2.610785 ## 1.00 0.01029396 4.387221 0.6334478 3.597207 ## 1.00 0.10293962 3.551244 0.7407368 2.929359 ## 1.00 1.02939621 2.950957 0.8332474 2.364204 ## ## RMSE was used to select the optimal model using the ## smallest value. ## The final values used for the model were alpha = 1 and lambda ## = 1.029396. We’re using the trControl defined previously, with a grid search for alpha and lambda (the shrinkage parameter). As before, calling bestTune prints out the set of hyperparmeters tested with the lowest loss. eln_mod$bestTune ## alpha lambda ## 9 1 1.029396 "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
